{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Cart Pole game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000       # Maxiaml number of steps in episode\n",
    "num_episodes = 20      # Number of episodes to play\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99           # Discount factor for reward\n",
    "num_Hidden = 200       # number of nodes in the hidden layer\n",
    "\n",
    "env = gym.make('CartPole-v0')    # Choose a game and create an environment\n",
    "obs_dim = env.reset().shape      # obervation dimension\n",
    "num_actions = env.action_space.n # number of actions (this works only for descrete action space, which is the case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    '''Takes a 1D rewards (one episode) and discounts it'''\n",
    "    dis_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_sum = running_sum + gamma*r[t]\n",
    "        dis_r[t] = running_sum\n",
    "        \n",
    "    return dis_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the simplest model: Observation as an input, one hidden layer and one node in the output layer - corresponding to the action of moving right (1) or left (0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ended after 16 steps\n",
      "Episode ended after 12 steps\n",
      "Episode ended after 18 steps\n",
      "Episode ended after 13 steps\n",
      "Episode ended after 17 steps\n",
      "Episode ended after 33 steps\n",
      "Episode ended after 17 steps\n",
      "Episode ended after 15 steps\n",
      "Episode ended after 16 steps\n",
      "Episode ended after 44 steps\n",
      "Episode ended after 12 steps\n",
      "Episode ended after 18 steps\n"
     ]
    }
   ],
   "source": [
    "obs = np.zeros((num_episodes, num_steps, obs_dim[0]))     # allocate space for 1D observations\n",
    "reward = np.zeros((num_episodes, num_steps))              # allocate space for rewars\n",
    "\n",
    "''' Run episodes '''\n",
    "for ep in range(num_episodes): \n",
    "    obs[ep,0,:] = temp = env.reset() # Reset and save first observation\n",
    "    \n",
    "    ''' Run steps '''\n",
    "    for t in range(1,num_steps):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()   # Sample an action\n",
    "        obs[ep,t], reward[ep,t], done, info = env.step(action)  # Take action and save observation, reward and done boolean\n",
    "        if done: \n",
    "            print(\"Episode ended after {} steps\".format(t+1))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
