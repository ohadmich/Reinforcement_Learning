{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Cart Pole game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 300      # Number of episodes for training\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99           # Discount factor for reward\n",
    "num_Hidden = 200       # number of nodes in the hidden layer\n",
    "\n",
    "env = gym.make('CartPole-v0')    # Choose a game and create an environment\n",
    "obs_dim = env.reset().shape      # obervation dimension\n",
    "num_actions = env.action_space.n # number of actions (this works only for descrete action space, which is the case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    '''Takes a 1D rewards (one episode) and discounts it and also standardize\n",
    "    the rewards to be unit normal (helps control the gradient estimator variance)'''\n",
    "    \n",
    "    # Discounting\n",
    "    dis_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_sum = gamma*running_sum + r[t]\n",
    "        dis_r[t] = running_sum\n",
    "    \n",
    "    # Normailzing\n",
    "    dis_r = (dis_r - np.mean(dis_r))/np.std(dis_r)\n",
    "        \n",
    "    return dis_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the simplest model: Observation as an input, one hidden layer and two nodes in the output layer - corresponding to the action of moving right (1) or left (0).\n",
    "\n",
    "To keep order in our model, we use name scopes which basically group the layers of our model in a simple to follow way. Eventually, when using [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) for visualizing the model, the graph is more readable and makes it easy to understand the model and find errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create placeholders for inputs'''\n",
    "# A place holder for input observations\n",
    "input_ = tf.placeholder(tf.float32, shape = (None, obs_dim[0]), name = \"input\")\n",
    "# A place holder for actions in a full episode\n",
    "actions = tf.placeholder(tf.float32, shape = (None, num_actions), name = \"actions\")\n",
    "# A place holder for discounted rewards in a full episode\n",
    "dis_rewards = tf.placeholder(tf.float32, shape = (None, ), name = \"dis_rewards\")\n",
    "\n",
    "'''Fully connected layers'''\n",
    "with tf.name_scope(\"FC1\"):\n",
    "    fc1 = tf.layers.dense(inputs = input_, units = num_Hidden ,activation = tf.nn.relu, name = \"fc1\" )\n",
    "\n",
    "with tf.name_scope(\"FC2\"):\n",
    "    fc2 = tf.layers.dense(inputs = fc1, units = num_actions ,activation = None, name = \"fc2\" )\n",
    "\n",
    "with tf.name_scope(\"Action_PDF\"):\n",
    "    # Operate with softmax on fc2 outputs to get an action probability distribution\n",
    "    action_prob_dist = tf.nn.softmax(logits = fc2, name = \"softamx\")\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    '''Define loss'''\n",
    "    # Fist define reular softmax cross entropy loss\n",
    "    CE_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = actions, logits = fc2, name = \"CE_loss\")\n",
    "    # Modulate the loss based on our discounted reward - this is where reinforcment learning happens,\n",
    "    # we favor actions that produced high reward\n",
    "    loss = tf.reduce_mean(CE_loss * dis_rewards)\n",
    "\n",
    "with tf.name_scope(\"Training\"):\n",
    "    '''Define optimizer'''\n",
    "    training_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "'''Define saver for saving and restoring model'''\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a writer for saving summaries to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.summary.FileWriter(\"./tensorboard/1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a graph visualization:\n",
    "\n",
    "<img src=\"./img/model_graph.png\" width=\"500\">\n",
    "\n",
    "Each block in the graph is expandable and let you see the content inside, for example see an [image](./img/model_graph_loss.png) with expanded loss block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model ready, we can start training it.\n",
    "\n",
    "Our goal is to achieve a model that uses the current observation to create the best probability distribution function (PDF) for the next action to be taken. This is **not a deterministic model** - the agent has a fine probability to take any action after receiving an observation, with larger probability to take favorable actions.\n",
    "\n",
    "To achieve that, in each step, we use our model to generate a PDF of actions, draw an action out of it and take the next step.\n",
    "When we reach the end of the episode, we compute the episode loss by feeding the observation, action and discounted rewards vectors (which we kept track of). Our optimizer minimizes the loss which makes favorable (unfavorable) actions more probable (less probable) due to the multiplicative factor of the discounted reward. By repeating these steps to each episode, our agent gradually improves its PDF and becomes a better player!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 0\n",
      "Episode ended after 32 steps\n",
      "Accumulated reward in this episode 32.0\n",
      "Mean reward so far 32.00\n",
      "Maximal reward so far 32.0\n",
      "-------------------------------------------------\n",
      "Episode 1\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 14.0\n",
      "Mean reward so far 23.00\n",
      "Maximal reward so far 32.0\n",
      "-------------------------------------------------\n",
      "Episode 2\n",
      "Episode ended after 38 steps\n",
      "Accumulated reward in this episode 38.0\n",
      "Mean reward so far 28.00\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 3\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 28.0\n",
      "Mean reward so far 28.00\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 4\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 18.0\n",
      "Mean reward so far 26.00\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 5\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 14.0\n",
      "Mean reward so far 24.00\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 6\n",
      "Episode ended after 30 steps\n",
      "Accumulated reward in this episode 30.0\n",
      "Mean reward so far 24.86\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 7\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 21.0\n",
      "Mean reward so far 24.38\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 8\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 13.0\n",
      "Mean reward so far 23.11\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 9\n",
      "Episode ended after 15 steps\n",
      "Accumulated reward in this episode 15.0\n",
      "Mean reward so far 22.30\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 10\n",
      "Episode ended after 31 steps\n",
      "Accumulated reward in this episode 31.0\n",
      "Mean reward so far 23.09\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 11\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 29.0\n",
      "Mean reward so far 23.58\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 12\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 21.0\n",
      "Mean reward so far 23.38\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 13\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 28.0\n",
      "Mean reward so far 23.71\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 14\n",
      "Episode ended after 36 steps\n",
      "Accumulated reward in this episode 36.0\n",
      "Mean reward so far 24.53\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 15\n",
      "Episode ended after 41 steps\n",
      "Accumulated reward in this episode 41.0\n",
      "Mean reward so far 25.56\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 16\n",
      "Episode ended after 35 steps\n",
      "Accumulated reward in this episode 35.0\n",
      "Mean reward so far 26.12\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 17\n",
      "Episode ended after 26 steps\n",
      "Accumulated reward in this episode 26.0\n",
      "Mean reward so far 26.11\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 18\n",
      "Episode ended after 30 steps\n",
      "Accumulated reward in this episode 30.0\n",
      "Mean reward so far 26.32\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 19\n",
      "Episode ended after 16 steps\n",
      "Accumulated reward in this episode 16.0\n",
      "Mean reward so far 25.80\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 20\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 17.0\n",
      "Mean reward so far 25.38\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 21\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 28.0\n",
      "Mean reward so far 25.50\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 22\n",
      "Episode ended after 25 steps\n",
      "Accumulated reward in this episode 25.0\n",
      "Mean reward so far 25.48\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 23\n",
      "Episode ended after 32 steps\n",
      "Accumulated reward in this episode 32.0\n",
      "Mean reward so far 25.75\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 24\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 37.0\n",
      "Mean reward so far 26.20\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 25\n",
      "Episode ended after 30 steps\n",
      "Accumulated reward in this episode 30.0\n",
      "Mean reward so far 26.35\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 26\n",
      "Episode ended after 36 steps\n",
      "Accumulated reward in this episode 36.0\n",
      "Mean reward so far 26.70\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 27\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 29.0\n",
      "Mean reward so far 26.79\n",
      "Maximal reward so far 41.0\n",
      "-------------------------------------------------\n",
      "Episode 28\n",
      "Episode ended after 44 steps\n",
      "Accumulated reward in this episode 44.0\n",
      "Mean reward so far 27.38\n",
      "Maximal reward so far 44.0\n",
      "-------------------------------------------------\n",
      "Episode 29\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 24.0\n",
      "Mean reward so far 27.27\n",
      "Maximal reward so far 44.0\n",
      "-------------------------------------------------\n",
      "Episode 30\n",
      "Episode ended after 78 steps\n",
      "Accumulated reward in this episode 78.0\n",
      "Mean reward so far 28.90\n",
      "Maximal reward so far 78.0\n",
      "-------------------------------------------------\n",
      "Episode 31\n",
      "Episode ended after 78 steps\n",
      "Accumulated reward in this episode 78.0\n",
      "Mean reward so far 30.44\n",
      "Maximal reward so far 78.0\n",
      "-------------------------------------------------\n",
      "Episode 32\n",
      "Episode ended after 64 steps\n",
      "Accumulated reward in this episode 64.0\n",
      "Mean reward so far 31.45\n",
      "Maximal reward so far 78.0\n",
      "-------------------------------------------------\n",
      "Episode 33\n",
      "Episode ended after 45 steps\n",
      "Accumulated reward in this episode 45.0\n",
      "Mean reward so far 31.85\n",
      "Maximal reward so far 78.0\n",
      "-------------------------------------------------\n",
      "Episode 34\n",
      "Episode ended after 69 steps\n",
      "Accumulated reward in this episode 69.0\n",
      "Mean reward so far 32.91\n",
      "Maximal reward so far 78.0\n",
      "-------------------------------------------------\n",
      "Episode 35\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 37.56\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 36\n",
      "Episode ended after 54 steps\n",
      "Accumulated reward in this episode 54.0\n",
      "Mean reward so far 38.00\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 37\n",
      "Episode ended after 42 steps\n",
      "Accumulated reward in this episode 42.0\n",
      "Mean reward so far 38.11\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 38\n",
      "Episode ended after 36 steps\n",
      "Accumulated reward in this episode 36.0\n",
      "Mean reward so far 38.05\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 39\n",
      "Episode ended after 110 steps\n",
      "Accumulated reward in this episode 110.0\n",
      "Mean reward so far 39.85\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 40\n",
      "Episode ended after 73 steps\n",
      "Accumulated reward in this episode 73.0\n",
      "Mean reward so far 40.66\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 41\n",
      "Episode ended after 157 steps\n",
      "Accumulated reward in this episode 157.0\n",
      "Mean reward so far 43.43\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 42\n",
      "Episode ended after 55 steps\n",
      "Accumulated reward in this episode 55.0\n",
      "Mean reward so far 43.70\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 43\n",
      "Episode ended after 70 steps\n",
      "Accumulated reward in this episode 70.0\n",
      "Mean reward so far 44.30\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 44\n",
      "Episode ended after 58 steps\n",
      "Accumulated reward in this episode 58.0\n",
      "Mean reward so far 44.60\n",
      "Maximal reward so far 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 45\n",
      "Episode ended after 70 steps\n",
      "Accumulated reward in this episode 70.0\n",
      "Mean reward so far 45.15\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 46\n",
      "Episode ended after 48 steps\n",
      "Accumulated reward in this episode 48.0\n",
      "Mean reward so far 45.21\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 47\n",
      "Episode ended after 151 steps\n",
      "Accumulated reward in this episode 151.0\n",
      "Mean reward so far 47.42\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 48\n",
      "Episode ended after 85 steps\n",
      "Accumulated reward in this episode 85.0\n",
      "Mean reward so far 48.18\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 49\n",
      "Episode ended after 54 steps\n",
      "Accumulated reward in this episode 54.0\n",
      "Mean reward so far 48.30\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 50\n",
      "Episode ended after 142 steps\n",
      "Accumulated reward in this episode 142.0\n",
      "Mean reward so far 50.14\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 51\n",
      "Episode ended after 112 steps\n",
      "Accumulated reward in this episode 112.0\n",
      "Mean reward so far 51.33\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 52\n",
      "Episode ended after 64 steps\n",
      "Accumulated reward in this episode 64.0\n",
      "Mean reward so far 51.57\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 53\n",
      "Episode ended after 75 steps\n",
      "Accumulated reward in this episode 75.0\n",
      "Mean reward so far 52.00\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 54\n",
      "Episode ended after 90 steps\n",
      "Accumulated reward in this episode 90.0\n",
      "Mean reward so far 52.69\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 55\n",
      "Episode ended after 123 steps\n",
      "Accumulated reward in this episode 123.0\n",
      "Mean reward so far 53.95\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 56\n",
      "Episode ended after 104 steps\n",
      "Accumulated reward in this episode 104.0\n",
      "Mean reward so far 54.82\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 57\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 57.33\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 58\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 59.75\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 59\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 62.08\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 60\n",
      "Episode ended after 139 steps\n",
      "Accumulated reward in this episode 139.0\n",
      "Mean reward so far 63.34\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 61\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 65.55\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 62\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 67.68\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 63\n",
      "Episode ended after 92 steps\n",
      "Accumulated reward in this episode 92.0\n",
      "Mean reward so far 68.06\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 64\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 70.09\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 65\n",
      "Episode ended after 156 steps\n",
      "Accumulated reward in this episode 156.0\n",
      "Mean reward so far 71.39\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 66\n",
      "Episode ended after 91 steps\n",
      "Accumulated reward in this episode 91.0\n",
      "Mean reward so far 71.69\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 67\n",
      "Episode ended after 103 steps\n",
      "Accumulated reward in this episode 103.0\n",
      "Mean reward so far 72.15\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 68\n",
      "Episode ended after 138 steps\n",
      "Accumulated reward in this episode 138.0\n",
      "Mean reward so far 73.10\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 69\n",
      "Episode ended after 103 steps\n",
      "Accumulated reward in this episode 103.0\n",
      "Mean reward so far 73.53\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 70\n",
      "Episode ended after 145 steps\n",
      "Accumulated reward in this episode 145.0\n",
      "Mean reward so far 74.54\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 71\n",
      "Episode ended after 158 steps\n",
      "Accumulated reward in this episode 158.0\n",
      "Mean reward so far 75.69\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 72\n",
      "Episode ended after 98 steps\n",
      "Accumulated reward in this episode 98.0\n",
      "Mean reward so far 76.00\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 73\n",
      "Episode ended after 105 steps\n",
      "Accumulated reward in this episode 105.0\n",
      "Mean reward so far 76.39\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 74\n",
      "Episode ended after 114 steps\n",
      "Accumulated reward in this episode 114.0\n",
      "Mean reward so far 76.89\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 75\n",
      "Episode ended after 131 steps\n",
      "Accumulated reward in this episode 131.0\n",
      "Mean reward so far 77.61\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 76\n",
      "Episode ended after 103 steps\n",
      "Accumulated reward in this episode 103.0\n",
      "Mean reward so far 77.94\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 77\n",
      "Episode ended after 114 steps\n",
      "Accumulated reward in this episode 114.0\n",
      "Mean reward so far 78.40\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 78\n",
      "Episode ended after 128 steps\n",
      "Accumulated reward in this episode 128.0\n",
      "Mean reward so far 79.03\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 79\n",
      "Episode ended after 105 steps\n",
      "Accumulated reward in this episode 105.0\n",
      "Mean reward so far 79.35\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 80\n",
      "Episode ended after 123 steps\n",
      "Accumulated reward in this episode 123.0\n",
      "Mean reward so far 79.89\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 81\n",
      "Episode ended after 105 steps\n",
      "Accumulated reward in this episode 105.0\n",
      "Mean reward so far 80.20\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 82\n",
      "Episode ended after 117 steps\n",
      "Accumulated reward in this episode 117.0\n",
      "Mean reward so far 80.64\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 83\n",
      "Episode ended after 128 steps\n",
      "Accumulated reward in this episode 128.0\n",
      "Mean reward so far 81.20\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 84\n",
      "Episode ended after 120 steps\n",
      "Accumulated reward in this episode 120.0\n",
      "Mean reward so far 81.66\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 85\n",
      "Episode ended after 105 steps\n",
      "Accumulated reward in this episode 105.0\n",
      "Mean reward so far 81.93\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 86\n",
      "Episode ended after 108 steps\n",
      "Accumulated reward in this episode 108.0\n",
      "Mean reward so far 82.23\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 87\n",
      "Episode ended after 121 steps\n",
      "Accumulated reward in this episode 121.0\n",
      "Mean reward so far 82.67\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 88\n",
      "Episode ended after 90 steps\n",
      "Accumulated reward in this episode 90.0\n",
      "Mean reward so far 82.75\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 89\n",
      "Episode ended after 113 steps\n",
      "Accumulated reward in this episode 113.0\n",
      "Mean reward so far 83.09\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 90\n",
      "Episode ended after 90 steps\n",
      "Accumulated reward in this episode 90.0\n",
      "Mean reward so far 83.16\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 91\n",
      "Episode ended after 114 steps\n",
      "Accumulated reward in this episode 114.0\n",
      "Mean reward so far 83.50\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 92\n",
      "Episode ended after 107 steps\n",
      "Accumulated reward in this episode 107.0\n",
      "Mean reward so far 83.75\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 93\n",
      "Episode ended after 123 steps\n",
      "Accumulated reward in this episode 123.0\n",
      "Mean reward so far 84.17\n",
      "Maximal reward so far 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 94\n",
      "Episode ended after 111 steps\n",
      "Accumulated reward in this episode 111.0\n",
      "Mean reward so far 84.45\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 95\n",
      "Episode ended after 91 steps\n",
      "Accumulated reward in this episode 91.0\n",
      "Mean reward so far 84.52\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 96\n",
      "Episode ended after 111 steps\n",
      "Accumulated reward in this episode 111.0\n",
      "Mean reward so far 84.79\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 97\n",
      "Episode ended after 111 steps\n",
      "Accumulated reward in this episode 111.0\n",
      "Mean reward so far 85.06\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 98\n",
      "Episode ended after 100 steps\n",
      "Accumulated reward in this episode 100.0\n",
      "Mean reward so far 85.21\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 99\n",
      "Episode ended after 111 steps\n",
      "Accumulated reward in this episode 111.0\n",
      "Mean reward so far 85.47\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 100\n",
      "Episode ended after 106 steps\n",
      "Accumulated reward in this episode 106.0\n",
      "Mean reward so far 85.67\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 101\n",
      "Episode ended after 107 steps\n",
      "Accumulated reward in this episode 107.0\n",
      "Mean reward so far 85.88\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 102\n",
      "Episode ended after 108 steps\n",
      "Accumulated reward in this episode 108.0\n",
      "Mean reward so far 86.10\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 103\n",
      "Episode ended after 113 steps\n",
      "Accumulated reward in this episode 113.0\n",
      "Mean reward so far 86.36\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 104\n",
      "Episode ended after 97 steps\n",
      "Accumulated reward in this episode 97.0\n",
      "Mean reward so far 86.46\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 105\n",
      "Episode ended after 98 steps\n",
      "Accumulated reward in this episode 98.0\n",
      "Mean reward so far 86.57\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 106\n",
      "Episode ended after 97 steps\n",
      "Accumulated reward in this episode 97.0\n",
      "Mean reward so far 86.66\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 107\n",
      "Episode ended after 55 steps\n",
      "Accumulated reward in this episode 55.0\n",
      "Mean reward so far 86.37\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 108\n",
      "Episode ended after 44 steps\n",
      "Accumulated reward in this episode 44.0\n",
      "Mean reward so far 85.98\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 109\n",
      "Episode ended after 31 steps\n",
      "Accumulated reward in this episode 31.0\n",
      "Mean reward so far 85.48\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 110\n",
      "Episode ended after 106 steps\n",
      "Accumulated reward in this episode 106.0\n",
      "Mean reward so far 85.67\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 111\n",
      "Episode ended after 109 steps\n",
      "Accumulated reward in this episode 109.0\n",
      "Mean reward so far 85.88\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 112\n",
      "Episode ended after 38 steps\n",
      "Accumulated reward in this episode 38.0\n",
      "Mean reward so far 85.45\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 113\n",
      "Episode ended after 62 steps\n",
      "Accumulated reward in this episode 62.0\n",
      "Mean reward so far 85.25\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 114\n",
      "Episode ended after 65 steps\n",
      "Accumulated reward in this episode 65.0\n",
      "Mean reward so far 85.07\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 115\n",
      "Episode ended after 111 steps\n",
      "Accumulated reward in this episode 111.0\n",
      "Mean reward so far 85.29\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 116\n",
      "Episode ended after 119 steps\n",
      "Accumulated reward in this episode 119.0\n",
      "Mean reward so far 85.58\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 117\n",
      "Episode ended after 113 steps\n",
      "Accumulated reward in this episode 113.0\n",
      "Mean reward so far 85.81\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 118\n",
      "Episode ended after 121 steps\n",
      "Accumulated reward in this episode 121.0\n",
      "Mean reward so far 86.11\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 119\n",
      "Episode ended after 117 steps\n",
      "Accumulated reward in this episode 117.0\n",
      "Mean reward so far 86.37\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 120\n",
      "Episode ended after 150 steps\n",
      "Accumulated reward in this episode 150.0\n",
      "Mean reward so far 86.89\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 121\n",
      "Episode ended after 127 steps\n",
      "Accumulated reward in this episode 127.0\n",
      "Mean reward so far 87.22\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 122\n",
      "Episode ended after 120 steps\n",
      "Accumulated reward in this episode 120.0\n",
      "Mean reward so far 87.49\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 123\n",
      "Episode ended after 120 steps\n",
      "Accumulated reward in this episode 120.0\n",
      "Mean reward so far 87.75\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 124\n",
      "Episode ended after 131 steps\n",
      "Accumulated reward in this episode 131.0\n",
      "Mean reward so far 88.10\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 125\n",
      "Episode ended after 114 steps\n",
      "Accumulated reward in this episode 114.0\n",
      "Mean reward so far 88.30\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 126\n",
      "Episode ended after 118 steps\n",
      "Accumulated reward in this episode 118.0\n",
      "Mean reward so far 88.54\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 127\n",
      "Episode ended after 108 steps\n",
      "Accumulated reward in this episode 108.0\n",
      "Mean reward so far 88.69\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 128\n",
      "Episode ended after 119 steps\n",
      "Accumulated reward in this episode 119.0\n",
      "Mean reward so far 88.92\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 129\n",
      "Episode ended after 116 steps\n",
      "Accumulated reward in this episode 116.0\n",
      "Mean reward so far 89.13\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 130\n",
      "Episode ended after 72 steps\n",
      "Accumulated reward in this episode 72.0\n",
      "Mean reward so far 89.00\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 131\n",
      "Episode ended after 46 steps\n",
      "Accumulated reward in this episode 46.0\n",
      "Mean reward so far 88.67\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 132\n",
      "Episode ended after 92 steps\n",
      "Accumulated reward in this episode 92.0\n",
      "Mean reward so far 88.70\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 133\n",
      "Episode ended after 93 steps\n",
      "Accumulated reward in this episode 93.0\n",
      "Mean reward so far 88.73\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 134\n",
      "Episode ended after 123 steps\n",
      "Accumulated reward in this episode 123.0\n",
      "Mean reward so far 88.99\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 135\n",
      "Episode ended after 108 steps\n",
      "Accumulated reward in this episode 108.0\n",
      "Mean reward so far 89.12\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 136\n",
      "Episode ended after 124 steps\n",
      "Accumulated reward in this episode 124.0\n",
      "Mean reward so far 89.38\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 137\n",
      "Episode ended after 155 steps\n",
      "Accumulated reward in this episode 155.0\n",
      "Mean reward so far 89.86\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 138\n",
      "Episode ended after 154 steps\n",
      "Accumulated reward in this episode 154.0\n",
      "Mean reward so far 90.32\n",
      "Maximal reward so far 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 139\n",
      "Episode ended after 185 steps\n",
      "Accumulated reward in this episode 185.0\n",
      "Mean reward so far 90.99\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 140\n",
      "Episode ended after 156 steps\n",
      "Accumulated reward in this episode 156.0\n",
      "Mean reward so far 91.45\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 141\n",
      "Episode ended after 172 steps\n",
      "Accumulated reward in this episode 172.0\n",
      "Mean reward so far 92.02\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 142\n",
      "Episode ended after 175 steps\n",
      "Accumulated reward in this episode 175.0\n",
      "Mean reward so far 92.60\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 143\n",
      "Episode ended after 186 steps\n",
      "Accumulated reward in this episode 186.0\n",
      "Mean reward so far 93.25\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 144\n",
      "Episode ended after 156 steps\n",
      "Accumulated reward in this episode 156.0\n",
      "Mean reward so far 93.68\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 145\n",
      "Episode ended after 169 steps\n",
      "Accumulated reward in this episode 169.0\n",
      "Mean reward so far 94.20\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 146\n",
      "Episode ended after 163 steps\n",
      "Accumulated reward in this episode 163.0\n",
      "Mean reward so far 94.67\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 147\n",
      "Episode ended after 193 steps\n",
      "Accumulated reward in this episode 193.0\n",
      "Mean reward so far 95.33\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 148\n",
      "Episode ended after 185 steps\n",
      "Accumulated reward in this episode 185.0\n",
      "Mean reward so far 95.93\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 149\n",
      "Episode ended after 149 steps\n",
      "Accumulated reward in this episode 149.0\n",
      "Mean reward so far 96.29\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 150\n",
      "Episode ended after 150 steps\n",
      "Accumulated reward in this episode 150.0\n",
      "Mean reward so far 96.64\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 151\n",
      "Episode ended after 193 steps\n",
      "Accumulated reward in this episode 193.0\n",
      "Mean reward so far 97.28\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 152\n",
      "Episode ended after 177 steps\n",
      "Accumulated reward in this episode 177.0\n",
      "Mean reward so far 97.80\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 153\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 98.46\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 154\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 99.12\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 155\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 99.76\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 156\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 100.40\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 157\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 101.03\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 158\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 101.65\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 159\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 102.27\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 160\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 102.88\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 161\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 103.48\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 162\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 104.07\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 163\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 104.65\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 164\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 105.23\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 165\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 105.80\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 166\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 106.37\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 167\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 106.92\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 168\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 107.47\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 169\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 108.02\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 170\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 108.56\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 171\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 109.09\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 172\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 109.61\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 173\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 110.13\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 174\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 110.65\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 175\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 111.15\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 176\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 111.66\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 177\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 112.15\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 178\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 112.64\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 179\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 113.13\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 180\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 113.61\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 181\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 114.08\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 182\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 114.55\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 183\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 115.02\n",
      "Maximal reward so far 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 184\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 115.48\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 185\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 115.93\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 186\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 116.38\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 187\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 116.82\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 188\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 117.26\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 189\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 117.70\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 190\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 118.13\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 191\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 118.56\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 192\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 118.98\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 193\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 119.40\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 194\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 119.81\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 195\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 120.22\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 196\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 120.62\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 197\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 121.03\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 198\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 121.42\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 199\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 121.81\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 200\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 122.20\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 201\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 122.59\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 202\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 122.97\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 203\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 123.35\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 204\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 123.72\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 205\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 124.09\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 206\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 124.46\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 207\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 124.82\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 208\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 125.18\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 209\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 125.54\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 210\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 125.89\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 211\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 126.24\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 212\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 126.59\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 213\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 126.93\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 214\n",
      "Episode ended after 197 steps\n",
      "Accumulated reward in this episode 197.0\n",
      "Mean reward so far 127.26\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 215\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 127.59\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 216\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 127.93\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 217\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 128.26\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 218\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 128.58\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 219\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 128.91\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 220\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 129.23\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 221\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 129.55\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 222\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 129.87\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 223\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 130.18\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 224\n",
      "Episode ended after 53 steps\n",
      "Accumulated reward in this episode 53.0\n",
      "Mean reward so far 129.84\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 225\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 130.15\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 226\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 130.45\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 227\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 130.76\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 228\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 131.06\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 229\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 131.36\n",
      "Maximal reward so far 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 230\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 131.66\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 231\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 131.95\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 232\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 132.24\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 233\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 132.53\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 234\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 132.82\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 235\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 133.11\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 236\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 133.39\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 237\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 133.67\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 238\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 133.95\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 239\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 134.22\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 240\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 134.49\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 241\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 134.76\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 242\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 135.03\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 243\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 135.30\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 244\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 135.56\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 245\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 135.83\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 246\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 136.09\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 247\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 136.34\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 248\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 136.60\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 249\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 136.85\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 250\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 137.10\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 251\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 137.35\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 252\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 137.60\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 253\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 137.85\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 254\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 138.09\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 255\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 138.33\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 256\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 138.57\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 257\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 138.81\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 258\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 139.05\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 259\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 139.28\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 260\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 139.51\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 261\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 139.74\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 262\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 139.97\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 263\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 140.20\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 264\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 140.43\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 265\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 140.65\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 266\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 140.87\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 267\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 141.09\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 268\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 141.31\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 269\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 141.53\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 270\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 141.75\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 271\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 141.96\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 272\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 142.17\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 273\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 142.38\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 274\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 142.59\n",
      "Maximal reward so far 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 275\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 142.80\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 276\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 143.01\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 277\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 143.21\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 278\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 143.42\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 279\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 143.62\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 280\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 143.82\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 281\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 144.02\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 282\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 144.22\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 283\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 144.41\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 284\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 144.61\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 285\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 144.80\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 286\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 144.99\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 287\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 145.18\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 288\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 145.37\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 289\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 145.56\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 290\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 145.75\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 291\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 145.93\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 292\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 146.12\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 293\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 146.30\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 294\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 146.48\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 295\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 146.67\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 296\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 146.85\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 297\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 147.02\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 298\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 147.20\n",
      "Maximal reward so far 200.0\n",
      "-------------------------------------------------\n",
      "Episode 299\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 147.38\n",
      "Maximal reward so far 200.0\n"
     ]
    }
   ],
   "source": [
    "ep_action, ep_obs, ep_reward = [], [], []  # Allocate space for episode actions, observations and rewards\n",
    "tot_ep_reward = [] # Total episode reward\n",
    "mean_reward = []\n",
    "\n",
    "''' Run TF session '''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ''' Run episodes '''\n",
    "    for ep in range(num_episodes): \n",
    "        obs = env.reset()  # Reset and save first observation\n",
    "        ep_obs.append(obs) # append observation\n",
    "\n",
    "        ''' Run steps '''\n",
    "        while True:\n",
    "            # Propagate forward to compute action probability distribution\n",
    "            apd = np.squeeze(sess.run(action_prob_dist, feed_dict = {input_ : obs.reshape((1,4))}))\n",
    "            action = np.random.choice(np.arange(num_actions), p = apd)   # Sample an action based on the pdf\n",
    "            obs, reward, done, info = env.step(action)  # Take action and save observation, reward and done boolean\n",
    "            \n",
    "            # Convert action to one hot\n",
    "            action_oh = np.zeros((1,num_actions))\n",
    "            action_oh[0,action] = 1\n",
    "            \n",
    "            ep_action.append(action_oh)  # append action\n",
    "            ep_obs.append(obs)           # append observation\n",
    "            ep_reward.append(reward)     # append reward\n",
    "\n",
    "            if done: \n",
    "                # Stack vertically episode parameters to one np.array\n",
    "                ep_action = np.vstack(ep_action)\n",
    "                ep_obs = np.vstack(ep_obs)\n",
    "                ep_reward = np.hstack(ep_reward)\n",
    "\n",
    "                # Discount rewards\n",
    "                dis_rewards_arr = discount_rewards(ep_reward)\n",
    "                # Compute loss and optimize\n",
    "                sess.run([loss, training_opt],\n",
    "                         feed_dict = {input_ : ep_obs[:-1], actions : ep_action, dis_rewards : dis_rewards_arr})\n",
    "                \n",
    "                tot_ep_reward.append(np.sum(ep_reward))  # Compute total reward for episode\n",
    "                mean_reward.append(np.mean(tot_ep_reward))\n",
    "                \n",
    "                 # print info\n",
    "                print(\"-------------------------------------------------\")\n",
    "                print(\"Episode {}\".format(ep))\n",
    "                print(\"Episode ended after {} steps\".format(ep_action.shape[0]))\n",
    "                print(\"Accumulated reward in this episode {}\".format(tot_ep_reward[ep]))\n",
    "                print(\"Mean reward so far {:0.2f}\".format(np.mean(tot_ep_reward)))\n",
    "                print(\"Maximal reward so far {}\".format(np.max(tot_ep_reward)))\n",
    "                \n",
    "                ep_action, ep_obs, ep_reward = [], [], []  # Clear episode values for next episode\n",
    "                      \n",
    "                break\n",
    "\n",
    "    saver.save(sess, \"models/model.ckpt\") # save model for later\n",
    "#     writer.add_graph(sess.graph) # Save graph for displaying with TensorBoard\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of how good our model and to see if our agent improves in the training process, I like to plot the mean rewards gained in one episode over the episode number. This value would grow as our agent becomes more skilled and gains more rewards in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mean_reward)\n",
    "plt.xlabel(\"episode\", fontsize=14)\n",
    "plt.ylabel(\"mean reward\", fontsize = 14)\n",
    "plt.title(\"Mean episode rewards over episode number\", fontsize = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the agent play an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model.ckpt\n",
      "Game ended after 201 steps\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/model.ckpt\") # load model\n",
    "    obs = env.reset() # Reset env and save observation\n",
    "    t = 0\n",
    "    while True:\n",
    "        env.render() # Render game\n",
    "        # Use our model to create a probability distribution of actions based on observation\n",
    "        apd = np.squeeze(sess.run(action_prob_dist, feed_dict={input_ : obs.reshape((1,4))}))\n",
    "        # Choose an action out of the PDF and take action\n",
    "        action = np.random.choice(np.arange(num_actions), p = apd)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        t = t+1\n",
    "        if done:\n",
    "            print(\"Game ended after {} steps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
