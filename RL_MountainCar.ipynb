{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Mountain Car game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define all the hyperparameters for the model, this will allow as to easily iterate and find the values that give best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 200      # Number of episodes for training\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99           # Discount factor for reward\n",
    "num_Hidden = 20       # number of nodes in the hidden layer\n",
    "\n",
    "env = gym.make('MountainCar-v0')    # Choose a game and create an environment\n",
    "env = env.unwrapped\n",
    "obs_dim = env.reset().shape      # obervation dimension\n",
    "num_actions = env.action_space.n # number of actions (this works only for descrete action space, which is the case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discount reward function**: We we'll train our agent based on the reward gained for his actions. For each action we'll define the episode reward as the total reward gained in all the next steps of the current episode. Since a reward gained further away in the future has less correlation to the present action, we will give it less weight by discounting future rewards.\n",
    "\n",
    "The formula for the discounted rewards is given by:\n",
    "\n",
    "$$ R_t = \\sum_k \\gamma^k r_{t+k} $$\n",
    "\n",
    "Where $r_t$ is the reward gained in the step $t$ and $\\gamma \\in [0,1]$ is a hyperparameter called the discount factor.\n",
    "Here we define a function that takes a vector of rewards in consequent steps and returns the discounted reward vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    '''Takes a 1D rewards (one episode) and discounts it and also standardize\n",
    "    the rewards to be unit normal (helps control the gradient estimator variance)'''\n",
    "    \n",
    "    # Discounting\n",
    "    dis_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_sum = gamma*running_sum + r[t]\n",
    "        dis_r[t] = running_sum\n",
    "    \n",
    "    # Normailzing\n",
    "    dis_r = (dis_r - np.mean(dis_r))/np.std(dis_r)\n",
    "        \n",
    "    return dis_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try different models to find the one that would give us best results. below I define the network layers and connect them to form the desired architecture.\n",
    "\n",
    "To keep order in our model, we use name scopes which basically group the layers of our model in a simple to follow way. Eventually, when using [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) for visualizing the model, the graph is more readable and makes it easy to understand the model and find errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create placeholders for inputs'''\n",
    "# A place holder for input observations\n",
    "input_ = tf.placeholder(tf.float32, shape = (None, obs_dim[0]), name = \"input\")\n",
    "# A place holder for actions in a full episode\n",
    "actions = tf.placeholder(tf.float32, shape = (None, num_actions), name = \"actions\")\n",
    "# A place holder for discounted rewards in a full episode\n",
    "dis_rewards = tf.placeholder(tf.float32, shape = (None, ), name = \"dis_rewards\")\n",
    "\n",
    "'''Fully connected layers'''\n",
    "with tf.name_scope(\"FC1\"):\n",
    "    fc1 = tf.layers.dense(inputs = input_, units = num_Hidden ,activation = tf.nn.relu, name = \"fc1\" )\n",
    "\n",
    "with tf.name_scope(\"FC2\"):\n",
    "    fc2 = tf.layers.dense(inputs = fc1, units = num_Hidden ,activation = tf.nn.relu, name = \"fc2\" )\n",
    "    \n",
    "with tf.name_scope(\"FC3\"):\n",
    "    fc3 = tf.layers.dense(inputs = fc2, units = num_actions ,activation = None, name = \"fc3\" )\n",
    "\n",
    "with tf.name_scope(\"Action_PDF\"):\n",
    "    # Operate with softmax on fc2 outputs to get an action probability distribution\n",
    "    action_prob_dist = tf.nn.softmax(logits = fc3, name = \"softamx\")\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    '''Define loss'''\n",
    "    # Fist define reular softmax cross entropy loss\n",
    "    CE_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = actions, logits = fc3, name = \"CE_loss\")\n",
    "    # Modulate the loss based on our discounted reward - this is where reinforcment learning happens,\n",
    "    # we favor actions that produced high reward\n",
    "    loss = tf.reduce_mean(CE_loss * dis_rewards)\n",
    "\n",
    "with tf.name_scope(\"Training\"):\n",
    "    '''Define optimizer'''\n",
    "    training_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "'''Define saver for saving and restoring model'''\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a writer for saving summaries to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.summary.FileWriter(\"./tensorboard/1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a graph visualization:\n",
    "\n",
    "<img src=\"./img/model_graph.png\" width=\"500\">\n",
    "\n",
    "Each block in the graph is expandable and let you see the content inside, for example see an [image](./img/model_graph_loss.png) with expanded loss block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model ready, we can start training it.\n",
    "\n",
    "Our goal is to achieve a model that uses the current observation to create the best probability distribution function (PDF) for the next action to be taken. This is **not a deterministic model** - the agent has a fine probability to take any action after receiving an observation, with larger probability to take favorable actions.\n",
    "\n",
    "To achieve that, in each step, we use our model to generate a PDF of actions, draw an action out of it and take the next step.\n",
    "When we reach the end of the episode, we compute the episode loss by feeding the observation, action and discounted rewards vectors (which we kept track of). Our optimizer minimizes the loss which makes favorable (unfavorable) actions more probable (less probable) due to the multiplicative factor of the discounted reward. By repeating these steps to each episode, our agent gradually improves its PDF and becomes a better player!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 0\n",
      "Episode ended after 89275 steps\n",
      "Accumulated reward in this episode -89275.0\n",
      "Mean reward so far -89275.00\n",
      "Maximal reward so far -89275.0\n",
      "-------------------------------------------------\n",
      "Episode 1\n",
      "Episode ended after 113253 steps\n",
      "Accumulated reward in this episode -113253.0\n",
      "Mean reward so far -101264.00\n",
      "Maximal reward so far -89275.0\n",
      "-------------------------------------------------\n",
      "Episode 2\n",
      "Episode ended after 7863 steps\n",
      "Accumulated reward in this episode -7863.0\n",
      "Mean reward so far -70130.33\n",
      "Maximal reward so far -7863.0\n",
      "-------------------------------------------------\n",
      "Episode 3\n",
      "Episode ended after 10144 steps\n",
      "Accumulated reward in this episode -10144.0\n",
      "Mean reward so far -55133.75\n",
      "Maximal reward so far -7863.0\n",
      "-------------------------------------------------\n",
      "Episode 4\n",
      "Episode ended after 3551 steps\n",
      "Accumulated reward in this episode -3551.0\n",
      "Mean reward so far -44817.20\n",
      "Maximal reward so far -3551.0\n",
      "-------------------------------------------------\n",
      "Episode 5\n",
      "Episode ended after 15600 steps\n",
      "Accumulated reward in this episode -15600.0\n",
      "Mean reward so far -39947.67\n",
      "Maximal reward so far -3551.0\n",
      "-------------------------------------------------\n",
      "Episode 6\n",
      "Episode ended after 9331 steps\n",
      "Accumulated reward in this episode -9331.0\n",
      "Mean reward so far -35573.86\n",
      "Maximal reward so far -3551.0\n",
      "-------------------------------------------------\n",
      "Episode 7\n",
      "Episode ended after 1606 steps\n",
      "Accumulated reward in this episode -1606.0\n",
      "Mean reward so far -31327.88\n",
      "Maximal reward so far -1606.0\n",
      "-------------------------------------------------\n",
      "Episode 8\n",
      "Episode ended after 2598 steps\n",
      "Accumulated reward in this episode -2598.0\n",
      "Mean reward so far -28135.67\n",
      "Maximal reward so far -1606.0\n",
      "-------------------------------------------------\n",
      "Episode 9\n",
      "Episode ended after 3687 steps\n",
      "Accumulated reward in this episode -3687.0\n",
      "Mean reward so far -25690.80\n",
      "Maximal reward so far -1606.0\n",
      "-------------------------------------------------\n",
      "Episode 10\n",
      "Episode ended after 2876 steps\n",
      "Accumulated reward in this episode -2876.0\n",
      "Mean reward so far -23616.73\n",
      "Maximal reward so far -1606.0\n",
      "-------------------------------------------------\n",
      "Episode 11\n",
      "Episode ended after 1454 steps\n",
      "Accumulated reward in this episode -1454.0\n",
      "Mean reward so far -21769.83\n",
      "Maximal reward so far -1454.0\n",
      "-------------------------------------------------\n",
      "Episode 12\n",
      "Episode ended after 1503 steps\n",
      "Accumulated reward in this episode -1503.0\n",
      "Mean reward so far -20210.85\n",
      "Maximal reward so far -1454.0\n",
      "-------------------------------------------------\n",
      "Episode 13\n",
      "Episode ended after 2991 steps\n",
      "Accumulated reward in this episode -2991.0\n",
      "Mean reward so far -18980.86\n",
      "Maximal reward so far -1454.0\n",
      "-------------------------------------------------\n",
      "Episode 14\n",
      "Episode ended after 3141 steps\n",
      "Accumulated reward in this episode -3141.0\n",
      "Mean reward so far -17924.87\n",
      "Maximal reward so far -1454.0\n",
      "-------------------------------------------------\n",
      "Episode 15\n",
      "Episode ended after 2841 steps\n",
      "Accumulated reward in this episode -2841.0\n",
      "Mean reward so far -16982.12\n",
      "Maximal reward so far -1454.0\n",
      "-------------------------------------------------\n",
      "Episode 16\n",
      "Episode ended after 2951 steps\n",
      "Accumulated reward in this episode -2951.0\n",
      "Mean reward so far -16156.76\n",
      "Maximal reward so far -1454.0\n",
      "-------------------------------------------------\n",
      "Episode 17\n",
      "Episode ended after 5865 steps\n",
      "Accumulated reward in this episode -5865.0\n",
      "Mean reward so far -15585.00\n",
      "Maximal reward so far -1454.0\n",
      "-------------------------------------------------\n",
      "Episode 18\n",
      "Episode ended after 1557 steps\n",
      "Accumulated reward in this episode -1557.0\n",
      "Mean reward so far -14846.68\n",
      "Maximal reward so far -1454.0\n",
      "-------------------------------------------------\n",
      "Episode 19\n",
      "Episode ended after 963 steps\n",
      "Accumulated reward in this episode -963.0\n",
      "Mean reward so far -14152.50\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 20\n",
      "Episode ended after 2252 steps\n",
      "Accumulated reward in this episode -2252.0\n",
      "Mean reward so far -13585.81\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 21\n",
      "Episode ended after 3622 steps\n",
      "Accumulated reward in this episode -3622.0\n",
      "Mean reward so far -13132.91\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 22\n",
      "Episode ended after 4348 steps\n",
      "Accumulated reward in this episode -4348.0\n",
      "Mean reward so far -12750.96\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 23\n",
      "Episode ended after 2941 steps\n",
      "Accumulated reward in this episode -2941.0\n",
      "Mean reward so far -12342.21\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 24\n",
      "Episode ended after 5448 steps\n",
      "Accumulated reward in this episode -5448.0\n",
      "Mean reward so far -12066.44\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 25\n",
      "Episode ended after 2680 steps\n",
      "Accumulated reward in this episode -2680.0\n",
      "Mean reward so far -11705.42\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 26\n",
      "Episode ended after 1694 steps\n",
      "Accumulated reward in this episode -1694.0\n",
      "Mean reward so far -11334.63\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 27\n",
      "Episode ended after 5012 steps\n",
      "Accumulated reward in this episode -5012.0\n",
      "Mean reward so far -11108.82\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 28\n",
      "Episode ended after 1267 steps\n",
      "Accumulated reward in this episode -1267.0\n",
      "Mean reward so far -10769.45\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 29\n",
      "Episode ended after 5852 steps\n",
      "Accumulated reward in this episode -5852.0\n",
      "Mean reward so far -10605.53\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 30\n",
      "Episode ended after 2951 steps\n",
      "Accumulated reward in this episode -2951.0\n",
      "Mean reward so far -10358.61\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 31\n",
      "Episode ended after 1569 steps\n",
      "Accumulated reward in this episode -1569.0\n",
      "Mean reward so far -10083.94\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 32\n",
      "Episode ended after 2259 steps\n",
      "Accumulated reward in this episode -2259.0\n",
      "Mean reward so far -9846.82\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 33\n",
      "Episode ended after 995 steps\n",
      "Accumulated reward in this episode -995.0\n",
      "Mean reward so far -9586.47\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 34\n",
      "Episode ended after 2204 steps\n",
      "Accumulated reward in this episode -2204.0\n",
      "Mean reward so far -9375.54\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 35\n",
      "Episode ended after 1872 steps\n",
      "Accumulated reward in this episode -1872.0\n",
      "Mean reward so far -9167.11\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 36\n",
      "Episode ended after 3887 steps\n",
      "Accumulated reward in this episode -3887.0\n",
      "Mean reward so far -9024.41\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 37\n",
      "Episode ended after 2244 steps\n",
      "Accumulated reward in this episode -2244.0\n",
      "Mean reward so far -8845.97\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 38\n",
      "Episode ended after 1429 steps\n",
      "Accumulated reward in this episode -1429.0\n",
      "Mean reward so far -8655.79\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 39\n",
      "Episode ended after 1789 steps\n",
      "Accumulated reward in this episode -1789.0\n",
      "Mean reward so far -8484.12\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 40\n",
      "Episode ended after 2480 steps\n",
      "Accumulated reward in this episode -2480.0\n",
      "Mean reward so far -8337.68\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 41\n",
      "Episode ended after 1759 steps\n",
      "Accumulated reward in this episode -1759.0\n",
      "Mean reward so far -8181.05\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 42\n",
      "Episode ended after 4436 steps\n",
      "Accumulated reward in this episode -4436.0\n",
      "Mean reward so far -8093.95\n",
      "Maximal reward so far -963.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 43\n",
      "Episode ended after 3105 steps\n",
      "Accumulated reward in this episode -3105.0\n",
      "Mean reward so far -7980.57\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 44\n",
      "Episode ended after 4392 steps\n",
      "Accumulated reward in this episode -4392.0\n",
      "Mean reward so far -7900.82\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 45\n",
      "Episode ended after 6519 steps\n",
      "Accumulated reward in this episode -6519.0\n",
      "Mean reward so far -7870.78\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 46\n",
      "Episode ended after 3032 steps\n",
      "Accumulated reward in this episode -3032.0\n",
      "Mean reward so far -7767.83\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 47\n",
      "Episode ended after 2197 steps\n",
      "Accumulated reward in this episode -2197.0\n",
      "Mean reward so far -7651.77\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 48\n",
      "Episode ended after 1627 steps\n",
      "Accumulated reward in this episode -1627.0\n",
      "Mean reward so far -7528.82\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 49\n",
      "Episode ended after 2657 steps\n",
      "Accumulated reward in this episode -2657.0\n",
      "Mean reward so far -7431.38\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 50\n",
      "Episode ended after 2151 steps\n",
      "Accumulated reward in this episode -2151.0\n",
      "Mean reward so far -7327.84\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 51\n",
      "Episode ended after 3533 steps\n",
      "Accumulated reward in this episode -3533.0\n",
      "Mean reward so far -7254.87\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 52\n",
      "Episode ended after 2026 steps\n",
      "Accumulated reward in this episode -2026.0\n",
      "Mean reward so far -7156.21\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 53\n",
      "Episode ended after 3248 steps\n",
      "Accumulated reward in this episode -3248.0\n",
      "Mean reward so far -7083.83\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 54\n",
      "Episode ended after 4145 steps\n",
      "Accumulated reward in this episode -4145.0\n",
      "Mean reward so far -7030.40\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 55\n",
      "Episode ended after 4459 steps\n",
      "Accumulated reward in this episode -4459.0\n",
      "Mean reward so far -6984.48\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 56\n",
      "Episode ended after 3963 steps\n",
      "Accumulated reward in this episode -3963.0\n",
      "Mean reward so far -6931.47\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 57\n",
      "Episode ended after 3728 steps\n",
      "Accumulated reward in this episode -3728.0\n",
      "Mean reward so far -6876.24\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 58\n",
      "Episode ended after 5419 steps\n",
      "Accumulated reward in this episode -5419.0\n",
      "Mean reward so far -6851.54\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 59\n",
      "Episode ended after 8138 steps\n",
      "Accumulated reward in this episode -8138.0\n",
      "Mean reward so far -6872.98\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 60\n",
      "Episode ended after 10642 steps\n",
      "Accumulated reward in this episode -10642.0\n",
      "Mean reward so far -6934.77\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 61\n",
      "Episode ended after 15514 steps\n",
      "Accumulated reward in this episode -15514.0\n",
      "Mean reward so far -7073.15\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 62\n",
      "Episode ended after 3369 steps\n",
      "Accumulated reward in this episode -3369.0\n",
      "Mean reward so far -7014.35\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 63\n",
      "Episode ended after 7698 steps\n",
      "Accumulated reward in this episode -7698.0\n",
      "Mean reward so far -7025.03\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 64\n",
      "Episode ended after 4788 steps\n",
      "Accumulated reward in this episode -4788.0\n",
      "Mean reward so far -6990.62\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 65\n",
      "Episode ended after 7085 steps\n",
      "Accumulated reward in this episode -7085.0\n",
      "Mean reward so far -6992.05\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 66\n",
      "Episode ended after 3558 steps\n",
      "Accumulated reward in this episode -3558.0\n",
      "Mean reward so far -6940.79\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 67\n",
      "Episode ended after 7910 steps\n",
      "Accumulated reward in this episode -7910.0\n",
      "Mean reward so far -6955.04\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 68\n",
      "Episode ended after 5660 steps\n",
      "Accumulated reward in this episode -5660.0\n",
      "Mean reward so far -6936.28\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 69\n",
      "Episode ended after 3702 steps\n",
      "Accumulated reward in this episode -3702.0\n",
      "Mean reward so far -6890.07\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 70\n",
      "Episode ended after 2327 steps\n",
      "Accumulated reward in this episode -2327.0\n",
      "Mean reward so far -6825.80\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 71\n",
      "Episode ended after 6384 steps\n",
      "Accumulated reward in this episode -6384.0\n",
      "Mean reward so far -6819.67\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 72\n",
      "Episode ended after 3195 steps\n",
      "Accumulated reward in this episode -3195.0\n",
      "Mean reward so far -6770.01\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 73\n",
      "Episode ended after 8104 steps\n",
      "Accumulated reward in this episode -8104.0\n",
      "Mean reward so far -6788.04\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 74\n",
      "Episode ended after 2513 steps\n",
      "Accumulated reward in this episode -2513.0\n",
      "Mean reward so far -6731.04\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 75\n",
      "Episode ended after 3524 steps\n",
      "Accumulated reward in this episode -3524.0\n",
      "Mean reward so far -6688.84\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 76\n",
      "Episode ended after 4298 steps\n",
      "Accumulated reward in this episode -4298.0\n",
      "Mean reward so far -6657.79\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 77\n",
      "Episode ended after 6216 steps\n",
      "Accumulated reward in this episode -6216.0\n",
      "Mean reward so far -6652.13\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 78\n",
      "Episode ended after 4682 steps\n",
      "Accumulated reward in this episode -4682.0\n",
      "Mean reward so far -6627.19\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 79\n",
      "Episode ended after 2649 steps\n",
      "Accumulated reward in this episode -2649.0\n",
      "Mean reward so far -6577.46\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 80\n",
      "Episode ended after 5020 steps\n",
      "Accumulated reward in this episode -5020.0\n",
      "Mean reward so far -6558.23\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 81\n",
      "Episode ended after 2105 steps\n",
      "Accumulated reward in this episode -2105.0\n",
      "Mean reward so far -6503.93\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 82\n",
      "Episode ended after 3605 steps\n",
      "Accumulated reward in this episode -3605.0\n",
      "Mean reward so far -6469.00\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 83\n",
      "Episode ended after 1773 steps\n",
      "Accumulated reward in this episode -1773.0\n",
      "Mean reward so far -6413.10\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 84\n",
      "Episode ended after 2120 steps\n",
      "Accumulated reward in this episode -2120.0\n",
      "Mean reward so far -6362.59\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 85\n",
      "Episode ended after 3262 steps\n",
      "Accumulated reward in this episode -3262.0\n",
      "Mean reward so far -6326.53\n",
      "Maximal reward so far -963.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 86\n",
      "Episode ended after 3418 steps\n",
      "Accumulated reward in this episode -3418.0\n",
      "Mean reward so far -6293.10\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 87\n",
      "Episode ended after 1448 steps\n",
      "Accumulated reward in this episode -1448.0\n",
      "Mean reward so far -6238.05\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 88\n",
      "Episode ended after 2502 steps\n",
      "Accumulated reward in this episode -2502.0\n",
      "Mean reward so far -6196.07\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 89\n",
      "Episode ended after 2771 steps\n",
      "Accumulated reward in this episode -2771.0\n",
      "Mean reward so far -6158.01\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 90\n",
      "Episode ended after 4738 steps\n",
      "Accumulated reward in this episode -4738.0\n",
      "Mean reward so far -6142.41\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 91\n",
      "Episode ended after 2462 steps\n",
      "Accumulated reward in this episode -2462.0\n",
      "Mean reward so far -6102.40\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 92\n",
      "Episode ended after 4230 steps\n",
      "Accumulated reward in this episode -4230.0\n",
      "Mean reward so far -6082.27\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 93\n",
      "Episode ended after 3275 steps\n",
      "Accumulated reward in this episode -3275.0\n",
      "Mean reward so far -6052.40\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 94\n",
      "Episode ended after 2402 steps\n",
      "Accumulated reward in this episode -2402.0\n",
      "Mean reward so far -6013.98\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 95\n",
      "Episode ended after 2464 steps\n",
      "Accumulated reward in this episode -2464.0\n",
      "Mean reward so far -5977.00\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 96\n",
      "Episode ended after 1663 steps\n",
      "Accumulated reward in this episode -1663.0\n",
      "Mean reward so far -5932.53\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 97\n",
      "Episode ended after 4403 steps\n",
      "Accumulated reward in this episode -4403.0\n",
      "Mean reward so far -5916.92\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 98\n",
      "Episode ended after 1801 steps\n",
      "Accumulated reward in this episode -1801.0\n",
      "Mean reward so far -5875.34\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 99\n",
      "Episode ended after 2286 steps\n",
      "Accumulated reward in this episode -2286.0\n",
      "Mean reward so far -5839.45\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 100\n",
      "Episode ended after 2013 steps\n",
      "Accumulated reward in this episode -2013.0\n",
      "Mean reward so far -5801.56\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 101\n",
      "Episode ended after 3264 steps\n",
      "Accumulated reward in this episode -3264.0\n",
      "Mean reward so far -5776.69\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 102\n",
      "Episode ended after 2804 steps\n",
      "Accumulated reward in this episode -2804.0\n",
      "Mean reward so far -5747.83\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 103\n",
      "Episode ended after 1734 steps\n",
      "Accumulated reward in this episode -1734.0\n",
      "Mean reward so far -5709.23\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 104\n",
      "Episode ended after 1845 steps\n",
      "Accumulated reward in this episode -1845.0\n",
      "Mean reward so far -5672.43\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 105\n",
      "Episode ended after 2719 steps\n",
      "Accumulated reward in this episode -2719.0\n",
      "Mean reward so far -5644.57\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 106\n",
      "Episode ended after 2622 steps\n",
      "Accumulated reward in this episode -2622.0\n",
      "Mean reward so far -5616.32\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 107\n",
      "Episode ended after 1792 steps\n",
      "Accumulated reward in this episode -1792.0\n",
      "Mean reward so far -5580.91\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 108\n",
      "Episode ended after 2261 steps\n",
      "Accumulated reward in this episode -2261.0\n",
      "Mean reward so far -5550.45\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 109\n",
      "Episode ended after 2918 steps\n",
      "Accumulated reward in this episode -2918.0\n",
      "Mean reward so far -5526.52\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 110\n",
      "Episode ended after 2106 steps\n",
      "Accumulated reward in this episode -2106.0\n",
      "Mean reward so far -5495.70\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 111\n",
      "Episode ended after 2746 steps\n",
      "Accumulated reward in this episode -2746.0\n",
      "Mean reward so far -5471.15\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 112\n",
      "Episode ended after 2827 steps\n",
      "Accumulated reward in this episode -2827.0\n",
      "Mean reward so far -5447.75\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 113\n",
      "Episode ended after 3818 steps\n",
      "Accumulated reward in this episode -3818.0\n",
      "Mean reward so far -5433.46\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 114\n",
      "Episode ended after 2430 steps\n",
      "Accumulated reward in this episode -2430.0\n",
      "Mean reward so far -5407.34\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 115\n",
      "Episode ended after 3867 steps\n",
      "Accumulated reward in this episode -3867.0\n",
      "Mean reward so far -5394.06\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 116\n",
      "Episode ended after 3577 steps\n",
      "Accumulated reward in this episode -3577.0\n",
      "Mean reward so far -5378.53\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 117\n",
      "Episode ended after 2489 steps\n",
      "Accumulated reward in this episode -2489.0\n",
      "Mean reward so far -5354.04\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 118\n",
      "Episode ended after 2327 steps\n",
      "Accumulated reward in this episode -2327.0\n",
      "Mean reward so far -5328.61\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 119\n",
      "Episode ended after 2378 steps\n",
      "Accumulated reward in this episode -2378.0\n",
      "Mean reward so far -5304.02\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 120\n",
      "Episode ended after 2638 steps\n",
      "Accumulated reward in this episode -2638.0\n",
      "Mean reward so far -5281.98\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 121\n",
      "Episode ended after 1448 steps\n",
      "Accumulated reward in this episode -1448.0\n",
      "Mean reward so far -5250.56\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 122\n",
      "Episode ended after 6010 steps\n",
      "Accumulated reward in this episode -6010.0\n",
      "Mean reward so far -5256.73\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 123\n",
      "Episode ended after 3788 steps\n",
      "Accumulated reward in this episode -3788.0\n",
      "Mean reward so far -5244.89\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 124\n",
      "Episode ended after 2803 steps\n",
      "Accumulated reward in this episode -2803.0\n",
      "Mean reward so far -5225.35\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 125\n",
      "Episode ended after 2601 steps\n",
      "Accumulated reward in this episode -2601.0\n",
      "Mean reward so far -5204.52\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 126\n",
      "Episode ended after 3046 steps\n",
      "Accumulated reward in this episode -3046.0\n",
      "Mean reward so far -5187.53\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 127\n",
      "Episode ended after 3376 steps\n",
      "Accumulated reward in this episode -3376.0\n",
      "Mean reward so far -5173.38\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 128\n",
      "Episode ended after 2574 steps\n",
      "Accumulated reward in this episode -2574.0\n",
      "Mean reward so far -5153.22\n",
      "Maximal reward so far -963.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 129\n",
      "Episode ended after 2520 steps\n",
      "Accumulated reward in this episode -2520.0\n",
      "Mean reward so far -5132.97\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 130\n",
      "Episode ended after 2220 steps\n",
      "Accumulated reward in this episode -2220.0\n",
      "Mean reward so far -5110.73\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 131\n",
      "Episode ended after 3260 steps\n",
      "Accumulated reward in this episode -3260.0\n",
      "Mean reward so far -5096.71\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 132\n",
      "Episode ended after 2207 steps\n",
      "Accumulated reward in this episode -2207.0\n",
      "Mean reward so far -5074.98\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 133\n",
      "Episode ended after 2405 steps\n",
      "Accumulated reward in this episode -2405.0\n",
      "Mean reward so far -5055.06\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 134\n",
      "Episode ended after 3710 steps\n",
      "Accumulated reward in this episode -3710.0\n",
      "Mean reward so far -5045.10\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 135\n",
      "Episode ended after 2094 steps\n",
      "Accumulated reward in this episode -2094.0\n",
      "Mean reward so far -5023.40\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 136\n",
      "Episode ended after 2884 steps\n",
      "Accumulated reward in this episode -2884.0\n",
      "Mean reward so far -5007.78\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 137\n",
      "Episode ended after 1420 steps\n",
      "Accumulated reward in this episode -1420.0\n",
      "Mean reward so far -4981.78\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 138\n",
      "Episode ended after 2238 steps\n",
      "Accumulated reward in this episode -2238.0\n",
      "Mean reward so far -4962.04\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 139\n",
      "Episode ended after 2117 steps\n",
      "Accumulated reward in this episode -2117.0\n",
      "Mean reward so far -4941.72\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 140\n",
      "Episode ended after 1987 steps\n",
      "Accumulated reward in this episode -1987.0\n",
      "Mean reward so far -4920.77\n",
      "Maximal reward so far -963.0\n",
      "-------------------------------------------------\n",
      "Episode 141\n",
      "Episode ended after 788 steps\n",
      "Accumulated reward in this episode -788.0\n",
      "Mean reward so far -4891.66\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 142\n",
      "Episode ended after 1301 steps\n",
      "Accumulated reward in this episode -1301.0\n",
      "Mean reward so far -4866.55\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 143\n",
      "Episode ended after 1503 steps\n",
      "Accumulated reward in this episode -1503.0\n",
      "Mean reward so far -4843.19\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 144\n",
      "Episode ended after 2343 steps\n",
      "Accumulated reward in this episode -2343.0\n",
      "Mean reward so far -4825.95\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 145\n",
      "Episode ended after 2010 steps\n",
      "Accumulated reward in this episode -2010.0\n",
      "Mean reward so far -4806.66\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 146\n",
      "Episode ended after 2023 steps\n",
      "Accumulated reward in this episode -2023.0\n",
      "Mean reward so far -4787.73\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 147\n",
      "Episode ended after 1163 steps\n",
      "Accumulated reward in this episode -1163.0\n",
      "Mean reward so far -4763.24\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 148\n",
      "Episode ended after 1549 steps\n",
      "Accumulated reward in this episode -1549.0\n",
      "Mean reward so far -4741.66\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 149\n",
      "Episode ended after 1569 steps\n",
      "Accumulated reward in this episode -1569.0\n",
      "Mean reward so far -4720.51\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 150\n",
      "Episode ended after 1457 steps\n",
      "Accumulated reward in this episode -1457.0\n",
      "Mean reward so far -4698.90\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 151\n",
      "Episode ended after 1563 steps\n",
      "Accumulated reward in this episode -1563.0\n",
      "Mean reward so far -4678.27\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 152\n",
      "Episode ended after 1038 steps\n",
      "Accumulated reward in this episode -1038.0\n",
      "Mean reward so far -4654.48\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 153\n",
      "Episode ended after 1765 steps\n",
      "Accumulated reward in this episode -1765.0\n",
      "Mean reward so far -4635.71\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 154\n",
      "Episode ended after 1511 steps\n",
      "Accumulated reward in this episode -1511.0\n",
      "Mean reward so far -4615.55\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 155\n",
      "Episode ended after 1406 steps\n",
      "Accumulated reward in this episode -1406.0\n",
      "Mean reward so far -4594.98\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 156\n",
      "Episode ended after 1529 steps\n",
      "Accumulated reward in this episode -1529.0\n",
      "Mean reward so far -4575.45\n",
      "Maximal reward so far -788.0\n",
      "-------------------------------------------------\n",
      "Episode 157\n",
      "Episode ended after 637 steps\n",
      "Accumulated reward in this episode -637.0\n",
      "Mean reward so far -4550.53\n",
      "Maximal reward so far -637.0\n",
      "-------------------------------------------------\n",
      "Episode 158\n",
      "Episode ended after 2014 steps\n",
      "Accumulated reward in this episode -2014.0\n",
      "Mean reward so far -4534.57\n",
      "Maximal reward so far -637.0\n",
      "-------------------------------------------------\n",
      "Episode 159\n",
      "Episode ended after 825 steps\n",
      "Accumulated reward in this episode -825.0\n",
      "Mean reward so far -4511.39\n",
      "Maximal reward so far -637.0\n",
      "-------------------------------------------------\n",
      "Episode 160\n",
      "Episode ended after 1191 steps\n",
      "Accumulated reward in this episode -1191.0\n",
      "Mean reward so far -4490.76\n",
      "Maximal reward so far -637.0\n",
      "-------------------------------------------------\n",
      "Episode 161\n",
      "Episode ended after 1917 steps\n",
      "Accumulated reward in this episode -1917.0\n",
      "Mean reward so far -4474.88\n",
      "Maximal reward so far -637.0\n",
      "-------------------------------------------------\n",
      "Episode 162\n",
      "Episode ended after 1126 steps\n",
      "Accumulated reward in this episode -1126.0\n",
      "Mean reward so far -4454.33\n",
      "Maximal reward so far -637.0\n",
      "-------------------------------------------------\n",
      "Episode 163\n",
      "Episode ended after 1090 steps\n",
      "Accumulated reward in this episode -1090.0\n",
      "Mean reward so far -4433.82\n",
      "Maximal reward so far -637.0\n",
      "-------------------------------------------------\n",
      "Episode 164\n",
      "Episode ended after 1298 steps\n",
      "Accumulated reward in this episode -1298.0\n",
      "Mean reward so far -4414.81\n",
      "Maximal reward so far -637.0\n",
      "-------------------------------------------------\n",
      "Episode 165\n",
      "Episode ended after 471 steps\n",
      "Accumulated reward in this episode -471.0\n",
      "Mean reward so far -4391.05\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 166\n",
      "Episode ended after 729 steps\n",
      "Accumulated reward in this episode -729.0\n",
      "Mean reward so far -4369.13\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 167\n",
      "Episode ended after 1091 steps\n",
      "Accumulated reward in this episode -1091.0\n",
      "Mean reward so far -4349.61\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 168\n",
      "Episode ended after 1156 steps\n",
      "Accumulated reward in this episode -1156.0\n",
      "Mean reward so far -4330.72\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 169\n",
      "Episode ended after 998 steps\n",
      "Accumulated reward in this episode -998.0\n",
      "Mean reward so far -4311.11\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 170\n",
      "Episode ended after 875 steps\n",
      "Accumulated reward in this episode -875.0\n",
      "Mean reward so far -4291.02\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 171\n",
      "Episode ended after 1525 steps\n",
      "Accumulated reward in this episode -1525.0\n",
      "Mean reward so far -4274.94\n",
      "Maximal reward so far -471.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 172\n",
      "Episode ended after 648 steps\n",
      "Accumulated reward in this episode -648.0\n",
      "Mean reward so far -4253.97\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 173\n",
      "Episode ended after 1165 steps\n",
      "Accumulated reward in this episode -1165.0\n",
      "Mean reward so far -4236.22\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 174\n",
      "Episode ended after 692 steps\n",
      "Accumulated reward in this episode -692.0\n",
      "Mean reward so far -4215.97\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 175\n",
      "Episode ended after 527 steps\n",
      "Accumulated reward in this episode -527.0\n",
      "Mean reward so far -4195.01\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 176\n",
      "Episode ended after 544 steps\n",
      "Accumulated reward in this episode -544.0\n",
      "Mean reward so far -4174.38\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 177\n",
      "Episode ended after 663 steps\n",
      "Accumulated reward in this episode -663.0\n",
      "Mean reward so far -4154.65\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 178\n",
      "Episode ended after 1149 steps\n",
      "Accumulated reward in this episode -1149.0\n",
      "Mean reward so far -4137.86\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 179\n",
      "Episode ended after 863 steps\n",
      "Accumulated reward in this episode -863.0\n",
      "Mean reward so far -4119.67\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 180\n",
      "Episode ended after 868 steps\n",
      "Accumulated reward in this episode -868.0\n",
      "Mean reward so far -4101.70\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 181\n",
      "Episode ended after 963 steps\n",
      "Accumulated reward in this episode -963.0\n",
      "Mean reward so far -4084.46\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 182\n",
      "Episode ended after 1548 steps\n",
      "Accumulated reward in this episode -1548.0\n",
      "Mean reward so far -4070.60\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 183\n",
      "Episode ended after 908 steps\n",
      "Accumulated reward in this episode -908.0\n",
      "Mean reward so far -4053.41\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 184\n",
      "Episode ended after 525 steps\n",
      "Accumulated reward in this episode -525.0\n",
      "Mean reward so far -4034.34\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 185\n",
      "Episode ended after 759 steps\n",
      "Accumulated reward in this episode -759.0\n",
      "Mean reward so far -4016.73\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 186\n",
      "Episode ended after 599 steps\n",
      "Accumulated reward in this episode -599.0\n",
      "Mean reward so far -3998.45\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 187\n",
      "Episode ended after 810 steps\n",
      "Accumulated reward in this episode -810.0\n",
      "Mean reward so far -3981.49\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 188\n",
      "Episode ended after 851 steps\n",
      "Accumulated reward in this episode -851.0\n",
      "Mean reward so far -3964.93\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 189\n",
      "Episode ended after 1239 steps\n",
      "Accumulated reward in this episode -1239.0\n",
      "Mean reward so far -3950.58\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 190\n",
      "Episode ended after 590 steps\n",
      "Accumulated reward in this episode -590.0\n",
      "Mean reward so far -3932.98\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 191\n",
      "Episode ended after 794 steps\n",
      "Accumulated reward in this episode -794.0\n",
      "Mean reward so far -3916.64\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 192\n",
      "Episode ended after 484 steps\n",
      "Accumulated reward in this episode -484.0\n",
      "Mean reward so far -3898.85\n",
      "Maximal reward so far -471.0\n",
      "-------------------------------------------------\n",
      "Episode 193\n",
      "Episode ended after 468 steps\n",
      "Accumulated reward in this episode -468.0\n",
      "Mean reward so far -3881.16\n",
      "Maximal reward so far -468.0\n",
      "-------------------------------------------------\n",
      "Episode 194\n",
      "Episode ended after 468 steps\n",
      "Accumulated reward in this episode -468.0\n",
      "Mean reward so far -3863.66\n",
      "Maximal reward so far -468.0\n",
      "-------------------------------------------------\n",
      "Episode 195\n",
      "Episode ended after 532 steps\n",
      "Accumulated reward in this episode -532.0\n",
      "Mean reward so far -3846.66\n",
      "Maximal reward so far -468.0\n",
      "-------------------------------------------------\n",
      "Episode 196\n",
      "Episode ended after 595 steps\n",
      "Accumulated reward in this episode -595.0\n",
      "Mean reward so far -3830.16\n",
      "Maximal reward so far -468.0\n",
      "-------------------------------------------------\n",
      "Episode 197\n",
      "Episode ended after 695 steps\n",
      "Accumulated reward in this episode -695.0\n",
      "Mean reward so far -3814.32\n",
      "Maximal reward so far -468.0\n",
      "-------------------------------------------------\n",
      "Episode 198\n",
      "Episode ended after 514 steps\n",
      "Accumulated reward in this episode -514.0\n",
      "Mean reward so far -3797.74\n",
      "Maximal reward so far -468.0\n",
      "-------------------------------------------------\n",
      "Episode 199\n",
      "Episode ended after 507 steps\n",
      "Accumulated reward in this episode -507.0\n",
      "Mean reward so far -3781.28\n",
      "Maximal reward so far -468.0\n"
     ]
    }
   ],
   "source": [
    "ep_action, ep_obs, ep_reward = [], [], []  # Allocate space for episode actions, observations and rewards\n",
    "tot_ep_reward = [] # Total episode reward\n",
    "mean_reward = []\n",
    "\n",
    "''' Run TF session '''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ''' Run episodes '''\n",
    "    for ep in range(num_episodes): \n",
    "        obs = env.reset()  # Reset and save first observation\n",
    "        ep_obs.append(obs) # append observation\n",
    "\n",
    "        ''' Run steps '''\n",
    "        while True:\n",
    "            # Propagate forward to compute action probability distribution\n",
    "            apd = np.squeeze(sess.run(action_prob_dist, feed_dict = {input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "            action = np.random.choice(np.arange(num_actions), p = apd)   # Sample an action based on the pdf\n",
    "            obs, reward, done, info = env.step(action)  # Take action and save observation, reward and done boolean\n",
    "            \n",
    "            # Convert action to one hot\n",
    "            action_oh = np.zeros((1,num_actions))\n",
    "            action_oh[0,action] = 1\n",
    "            \n",
    "            ep_action.append(action_oh)  # append action\n",
    "            ep_obs.append(obs)           # append observation\n",
    "            ep_reward.append(reward)     # append reward\n",
    "\n",
    "            if done: \n",
    "                # Stack vertically episode parameters to one np.array\n",
    "                ep_action = np.vstack(ep_action)\n",
    "                ep_obs = np.vstack(ep_obs)\n",
    "                ep_reward = np.hstack(ep_reward)\n",
    "\n",
    "                # Discount rewards\n",
    "                dis_rewards_arr = discount_rewards(ep_reward)\n",
    "                # Compute loss and optimize\n",
    "                sess.run([loss, training_opt],\n",
    "                         feed_dict = {input_ : ep_obs[:-1], actions : ep_action, dis_rewards : dis_rewards_arr})\n",
    "                \n",
    "                tot_ep_reward.append(np.sum(ep_reward))  # Compute total reward for episode\n",
    "                mean_reward.append(np.mean(tot_ep_reward))\n",
    "                \n",
    "                 # print info\n",
    "                print(\"-------------------------------------------------\")\n",
    "                print(\"Episode {}\".format(ep))\n",
    "                print(\"Episode ended after {} steps\".format(ep_action.shape[0]))\n",
    "                print(\"Accumulated reward in this episode {}\".format(tot_ep_reward[ep]))\n",
    "                print(\"Mean reward so far {:0.2f}\".format(np.mean(tot_ep_reward)))\n",
    "                print(\"Maximal reward so far {}\".format(np.max(tot_ep_reward)))\n",
    "                \n",
    "                ep_action, ep_obs, ep_reward = [], [], []  # Clear episode values for next episode\n",
    "                      \n",
    "                break\n",
    "                \n",
    "    saver.save(sess, \"models/MountainCar/model.ckpt\") # save model for later\n",
    "#     writer.add_graph(sess.graph) # Save graph for displaying with TensorBoard\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of how good our model is, and to see if our agent improves in the training process, we can plot the mean rewards gained in one episode over the episode number. This value would grow as our agent becomes more skilled and gains more rewards in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mean_reward)\n",
    "plt.xlabel(\"episode\", fontsize=14)\n",
    "plt.ylabel(\"mean reward\", fontsize = 14)\n",
    "plt.title(\"Mean episode rewards over episode number\", fontsize = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch the agent play an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fun part!\n",
    "Now we get to see how good our agent really is by watching it play an episode. I also record the episode using the Monitor wrapper and limit the number of steps for this episode to 500, because this skilled agent can play forever..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from models/MountainCar/model.ckpt\n",
      "Game ended after 761 steps\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env._max_episode_steps = 900\n",
    "# env = gym.wrappers.Monitor(env, \"recording/MoutainCar\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/MountainCar/model.ckpt\") # load model\n",
    "    obs = env.reset() # Reset env and save observation\n",
    "    t = 0\n",
    "    while True:\n",
    "        env.render() # Render game\n",
    "        # Use our model to create a probability distribution of actions based on observation\n",
    "        apd = np.squeeze(sess.run(action_prob_dist, feed_dict={input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "        # Choose an action out of the PDF and take action\n",
    "        action = np.random.choice(np.arange(num_actions), p = apd)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        t = t+1\n",
    "        if done:\n",
    "            print(\"Game ended after {} steps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Agent preformance so far:\n",
    "\n",
    "<img src=\"./img/MountainCar_Agent.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
