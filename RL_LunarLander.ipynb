{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Lunar Lander game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define all the hyperparameters for the model, this will allow as to easily iterate and find the values that give best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 900      # Number of episodes for training\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99           # Discount factor for reward\n",
    "num_Hidden = 50       # number of nodes in the hidden layer\n",
    "\n",
    "env = gym.make('LunarLander-v2')    # Choose a game and create an environment\n",
    "# env = env.unwrapped              # The wrapper limits the number of steps in an episode to 200, let's get rid of it\n",
    "obs_dim = env.reset().shape      # obervation dimension\n",
    "num_actions = env.action_space.n # number of actions (this works only for descrete action space, which is the case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discount reward function**: We we'll train our agent based on the reward gained for his actions. For each action we'll define the episode reward as the total reward gained in all the next steps of the current episode. Since a reward gained further away in the future has less correlation to the present action, we will give it less weight by discounting future rewards.\n",
    "\n",
    "The formula for the discounted rewards is given by:\n",
    "\n",
    "$$ R_t = \\sum_k \\gamma^k r_{t+k} $$\n",
    "\n",
    "Where $r_t$ is the reward gained in the step $t$ and $\\gamma \\in [0,1]$ is a hyperparameter called the discount factor.\n",
    "Here we define a function that takes a vector of rewards in consequent steps and returns the discounted reward vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    '''Takes a 1D rewards (one episode) and discounts it and also standardize\n",
    "    the rewards to be unit normal (helps control the gradient estimator variance)'''\n",
    "    \n",
    "    # Discounting\n",
    "    dis_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_sum = gamma*running_sum + r[t]\n",
    "        dis_r[t] = running_sum\n",
    "    \n",
    "    # Normailzing\n",
    "    dis_r = (dis_r - np.mean(dis_r))/np.std(dis_r)\n",
    "        \n",
    "    return dis_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep order in our model, we use name scopes which basically group the layers of our model in a simple to follow way. Eventually, when using [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) for visualizing the model, the graph is more readable and makes it easy to understand the model and find errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create placeholders for inputs'''\n",
    "# A place holder for input observations\n",
    "input_ = tf.placeholder(tf.float32, shape = (None, obs_dim[0]), name = \"input\")\n",
    "# A place holder for actions in a full episode\n",
    "actions = tf.placeholder(tf.float32, shape = (None, num_actions), name = \"actions\")\n",
    "# A place holder for discounted rewards in a full episode\n",
    "dis_rewards = tf.placeholder(tf.float32, shape = (None, ), name = \"dis_rewards\")\n",
    "\n",
    "'''Fully connected layers'''\n",
    "with tf.name_scope(\"FC1\"):\n",
    "    fc1 = tf.layers.dense(inputs = input_, units = num_Hidden ,activation = tf.nn.relu, name = \"fc1\" )\n",
    "\n",
    "with tf.name_scope(\"FC2\"):\n",
    "    fc2 = tf.layers.dense(inputs = fc1, units = num_actions ,activation = None, name = \"fc2\" )\n",
    "\n",
    "with tf.name_scope(\"Action_PDF\"):\n",
    "    # Operate with softmax on fc2 outputs to get an action probability distribution\n",
    "    action_prob_dist = tf.nn.softmax(logits = fc2, name = \"softamx\")\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    '''Define loss'''\n",
    "    # Fist define reular softmax cross entropy loss\n",
    "    CE_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = actions, logits = fc2, name = \"CE_loss\")\n",
    "    # Modulate the loss based on our discounted reward - this is where reinforcment learning happens,\n",
    "    # we favor actions that produced high reward\n",
    "    loss = tf.reduce_mean(CE_loss * dis_rewards)\n",
    "\n",
    "with tf.name_scope(\"Training\"):\n",
    "    '''Define optimizer'''\n",
    "    training_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "'''Define saver for saving and restoring model'''\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a writer for saving summaries to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.summary.FileWriter(\"./tensorboard/1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a graph visualization:\n",
    "\n",
    "<img src=\"./img/model_graph.png\" width=\"500\">\n",
    "\n",
    "Each block in the graph is expandable and let you see the content inside, for example see an [image](./img/model_graph_loss.png) with expanded loss block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model ready, we can start training it.\n",
    "\n",
    "Our goal is to achieve a model that uses the current observation to create the best probability distribution function (PDF) for the next action to be taken. This is **not a deterministic model** - the agent has a fine probability to take any action after receiving an observation, with larger probability to take favorable actions.\n",
    "\n",
    "To achieve that, in each step, we use our model to generate a PDF of actions, draw an action out of it and take the next step.\n",
    "When we reach the end of the episode, we compute the episode loss by feeding the observation, action and discounted rewards vectors (which we kept track of). Our optimizer minimizes the loss which makes favorable (unfavorable) actions more probable (less probable) due to the multiplicative factor of the discounted reward. By repeating these steps to each episode, our agent gradually improves its PDF and becomes a better player!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 0\n",
      "Episode ended after 86 steps\n",
      "Accumulated reward in this episode -233.9644869379694\n",
      "Mean reward so far -233.96\n",
      "Maximal reward so far -233.9644869379694\n",
      "-------------------------------------------------\n",
      "Episode 1\n",
      "Episode ended after 127 steps\n",
      "Accumulated reward in this episode -262.9469750881134\n",
      "Mean reward so far -248.46\n",
      "Maximal reward so far -233.9644869379694\n",
      "-------------------------------------------------\n",
      "Episode 2\n",
      "Episode ended after 64 steps\n",
      "Accumulated reward in this episode -84.58570885603046\n",
      "Mean reward so far -193.83\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 3\n",
      "Episode ended after 78 steps\n",
      "Accumulated reward in this episode -254.96584871382157\n",
      "Mean reward so far -209.12\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 4\n",
      "Episode ended after 121 steps\n",
      "Accumulated reward in this episode -350.55513199919426\n",
      "Mean reward so far -237.40\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 5\n",
      "Episode ended after 94 steps\n",
      "Accumulated reward in this episode -369.9386696845868\n",
      "Mean reward so far -259.49\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 6\n",
      "Episode ended after 71 steps\n",
      "Accumulated reward in this episode -280.8066289373511\n",
      "Mean reward so far -262.54\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 7\n",
      "Episode ended after 74 steps\n",
      "Accumulated reward in this episode -235.59664260726112\n",
      "Mean reward so far -259.17\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 8\n",
      "Episode ended after 93 steps\n",
      "Accumulated reward in this episode -232.31495420920402\n",
      "Mean reward so far -256.19\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 9\n",
      "Episode ended after 108 steps\n",
      "Accumulated reward in this episode -266.31529465206063\n",
      "Mean reward so far -257.20\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 10\n",
      "Episode ended after 80 steps\n",
      "Accumulated reward in this episode -131.58428441705064\n",
      "Mean reward so far -245.78\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 11\n",
      "Episode ended after 83 steps\n",
      "Accumulated reward in this episode -132.096682815451\n",
      "Mean reward so far -236.31\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 12\n",
      "Episode ended after 76 steps\n",
      "Accumulated reward in this episode -105.73314067692007\n",
      "Mean reward so far -226.26\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 13\n",
      "Episode ended after 111 steps\n",
      "Accumulated reward in this episode -92.18235206221941\n",
      "Mean reward so far -216.68\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 14\n",
      "Episode ended after 59 steps\n",
      "Accumulated reward in this episode -97.81207946507756\n",
      "Mean reward so far -208.76\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 15\n",
      "Episode ended after 104 steps\n",
      "Accumulated reward in this episode -161.39652757029023\n",
      "Mean reward so far -205.80\n",
      "Maximal reward so far -84.58570885603046\n",
      "-------------------------------------------------\n",
      "Episode 16\n",
      "Episode ended after 96 steps\n",
      "Accumulated reward in this episode -49.17154169424335\n",
      "Mean reward so far -196.59\n",
      "Maximal reward so far -49.17154169424335\n",
      "-------------------------------------------------\n",
      "Episode 17\n",
      "Episode ended after 102 steps\n",
      "Accumulated reward in this episode -21.733650244954546\n",
      "Mean reward so far -186.87\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 18\n",
      "Episode ended after 60 steps\n",
      "Accumulated reward in this episode -90.06597120227528\n",
      "Mean reward so far -181.78\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 19\n",
      "Episode ended after 66 steps\n",
      "Accumulated reward in this episode -121.22964481895713\n",
      "Mean reward so far -178.75\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 20\n",
      "Episode ended after 132 steps\n",
      "Accumulated reward in this episode -214.21627125414705\n",
      "Mean reward so far -180.44\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 21\n",
      "Episode ended after 85 steps\n",
      "Accumulated reward in this episode -151.132041243811\n",
      "Mean reward so far -179.11\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 22\n",
      "Episode ended after 116 steps\n",
      "Accumulated reward in this episode -95.27412951541046\n",
      "Mean reward so far -175.46\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 23\n",
      "Episode ended after 79 steps\n",
      "Accumulated reward in this episode -125.81900935520741\n",
      "Mean reward so far -173.39\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 24\n",
      "Episode ended after 86 steps\n",
      "Accumulated reward in this episode -126.8179011409199\n",
      "Mean reward so far -171.53\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 25\n",
      "Episode ended after 92 steps\n",
      "Accumulated reward in this episode -131.29541801473266\n",
      "Mean reward so far -169.98\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 26\n",
      "Episode ended after 105 steps\n",
      "Accumulated reward in this episode -107.7703404271713\n",
      "Mean reward so far -167.68\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 27\n",
      "Episode ended after 107 steps\n",
      "Accumulated reward in this episode -89.19159339674731\n",
      "Mean reward so far -164.88\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 28\n",
      "Episode ended after 129 steps\n",
      "Accumulated reward in this episode -94.41543929388149\n",
      "Mean reward so far -162.45\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 29\n",
      "Episode ended after 107 steps\n",
      "Accumulated reward in this episode -66.03633735619374\n",
      "Mean reward so far -159.23\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 30\n",
      "Episode ended after 120 steps\n",
      "Accumulated reward in this episode -178.97267240889778\n",
      "Mean reward so far -159.87\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 31\n",
      "Episode ended after 118 steps\n",
      "Accumulated reward in this episode -76.81786550231912\n",
      "Mean reward so far -157.27\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 32\n",
      "Episode ended after 74 steps\n",
      "Accumulated reward in this episode -87.92653490155647\n",
      "Mean reward so far -155.17\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 33\n",
      "Episode ended after 125 steps\n",
      "Accumulated reward in this episode -71.86926232109828\n",
      "Mean reward so far -152.72\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 34\n",
      "Episode ended after 81 steps\n",
      "Accumulated reward in this episode -99.21698284739674\n",
      "Mean reward so far -151.19\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 35\n",
      "Episode ended after 75 steps\n",
      "Accumulated reward in this episode -107.10082377973609\n",
      "Mean reward so far -149.97\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 36\n",
      "Episode ended after 88 steps\n",
      "Accumulated reward in this episode -148.41354114220042\n",
      "Mean reward so far -149.93\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 37\n",
      "Episode ended after 98 steps\n",
      "Accumulated reward in this episode -122.99869415357104\n",
      "Mean reward so far -149.22\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 38\n",
      "Episode ended after 105 steps\n",
      "Accumulated reward in this episode -48.904437376440484\n",
      "Mean reward so far -146.65\n",
      "Maximal reward so far -21.733650244954546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 39\n",
      "Episode ended after 115 steps\n",
      "Accumulated reward in this episode -139.65046397705808\n",
      "Mean reward so far -146.47\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 40\n",
      "Episode ended after 99 steps\n",
      "Accumulated reward in this episode -97.27822840108543\n",
      "Mean reward so far -145.27\n",
      "Maximal reward so far -21.733650244954546\n",
      "-------------------------------------------------\n",
      "Episode 41\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 114.56594445990453\n",
      "Mean reward so far -139.08\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 42\n",
      "Episode ended after 92 steps\n",
      "Accumulated reward in this episode -67.89227291104109\n",
      "Mean reward so far -137.43\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 43\n",
      "Episode ended after 136 steps\n",
      "Accumulated reward in this episode -65.967744994428\n",
      "Mean reward so far -135.80\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 44\n",
      "Episode ended after 94 steps\n",
      "Accumulated reward in this episode -87.52209535545123\n",
      "Mean reward so far -134.73\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 45\n",
      "Episode ended after 90 steps\n",
      "Accumulated reward in this episode -59.671146861865495\n",
      "Mean reward so far -133.10\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 46\n",
      "Episode ended after 164 steps\n",
      "Accumulated reward in this episode -115.31956873260827\n",
      "Mean reward so far -132.72\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 47\n",
      "Episode ended after 128 steps\n",
      "Accumulated reward in this episode -84.486132787685\n",
      "Mean reward so far -131.72\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 48\n",
      "Episode ended after 78 steps\n",
      "Accumulated reward in this episode -17.455129824029385\n",
      "Mean reward so far -129.38\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 49\n",
      "Episode ended after 99 steps\n",
      "Accumulated reward in this episode 23.828547625529666\n",
      "Mean reward so far -126.32\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 50\n",
      "Episode ended after 85 steps\n",
      "Accumulated reward in this episode -74.18751151479213\n",
      "Mean reward so far -125.30\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 51\n",
      "Episode ended after 89 steps\n",
      "Accumulated reward in this episode -70.8481324046979\n",
      "Mean reward so far -124.25\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 52\n",
      "Episode ended after 120 steps\n",
      "Accumulated reward in this episode -52.664552725868475\n",
      "Mean reward so far -122.90\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 53\n",
      "Episode ended after 96 steps\n",
      "Accumulated reward in this episode -28.502874368978375\n",
      "Mean reward so far -121.15\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 54\n",
      "Episode ended after 84 steps\n",
      "Accumulated reward in this episode -122.42556914990863\n",
      "Mean reward so far -121.18\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 55\n",
      "Episode ended after 97 steps\n",
      "Accumulated reward in this episode 5.577296586436276\n",
      "Mean reward so far -118.91\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 56\n",
      "Episode ended after 72 steps\n",
      "Accumulated reward in this episode -38.974866554533605\n",
      "Mean reward so far -117.51\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 57\n",
      "Episode ended after 117 steps\n",
      "Accumulated reward in this episode -100.66494893385455\n",
      "Mean reward so far -117.22\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 58\n",
      "Episode ended after 99 steps\n",
      "Accumulated reward in this episode -77.69697400637071\n",
      "Mean reward so far -116.55\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 59\n",
      "Episode ended after 79 steps\n",
      "Accumulated reward in this episode -18.307798054495933\n",
      "Mean reward so far -114.91\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 60\n",
      "Episode ended after 88 steps\n",
      "Accumulated reward in this episode -164.06269134391948\n",
      "Mean reward so far -115.72\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 61\n",
      "Episode ended after 80 steps\n",
      "Accumulated reward in this episode -170.67422478132096\n",
      "Mean reward so far -116.60\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 62\n",
      "Episode ended after 84 steps\n",
      "Accumulated reward in this episode -111.49091332404646\n",
      "Mean reward so far -116.52\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 63\n",
      "Episode ended after 127 steps\n",
      "Accumulated reward in this episode -133.5353527816883\n",
      "Mean reward so far -116.79\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 64\n",
      "Episode ended after 85 steps\n",
      "Accumulated reward in this episode -112.28602263829669\n",
      "Mean reward so far -116.72\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 65\n",
      "Episode ended after 87 steps\n",
      "Accumulated reward in this episode -129.90205159160433\n",
      "Mean reward so far -116.92\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 66\n",
      "Episode ended after 83 steps\n",
      "Accumulated reward in this episode -56.1445692290141\n",
      "Mean reward so far -116.01\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 67\n",
      "Episode ended after 111 steps\n",
      "Accumulated reward in this episode -183.13111342856638\n",
      "Mean reward so far -117.00\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 68\n",
      "Episode ended after 116 steps\n",
      "Accumulated reward in this episode -139.65957187004346\n",
      "Mean reward so far -117.33\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 69\n",
      "Episode ended after 137 steps\n",
      "Accumulated reward in this episode -90.97346937305917\n",
      "Mean reward so far -116.95\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 70\n",
      "Episode ended after 97 steps\n",
      "Accumulated reward in this episode -124.42891866669758\n",
      "Mean reward so far -117.06\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 71\n",
      "Episode ended after 71 steps\n",
      "Accumulated reward in this episode -55.51964479795879\n",
      "Mean reward so far -116.20\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 72\n",
      "Episode ended after 83 steps\n",
      "Accumulated reward in this episode -30.309694315940163\n",
      "Mean reward so far -115.03\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 73\n",
      "Episode ended after 77 steps\n",
      "Accumulated reward in this episode -73.71912400907412\n",
      "Mean reward so far -114.47\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 74\n",
      "Episode ended after 123 steps\n",
      "Accumulated reward in this episode -161.8441925270663\n",
      "Mean reward so far -115.10\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 75\n",
      "Episode ended after 106 steps\n",
      "Accumulated reward in this episode -82.53013664139553\n",
      "Mean reward so far -114.67\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 76\n",
      "Episode ended after 134 steps\n",
      "Accumulated reward in this episode -98.8435171550232\n",
      "Mean reward so far -114.46\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 77\n",
      "Episode ended after 92 steps\n",
      "Accumulated reward in this episode -120.40663943299472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward so far -114.54\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 78\n",
      "Episode ended after 104 steps\n",
      "Accumulated reward in this episode -57.228673646092915\n",
      "Mean reward so far -113.82\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 79\n",
      "Episode ended after 97 steps\n",
      "Accumulated reward in this episode -97.62430315520224\n",
      "Mean reward so far -113.61\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 80\n",
      "Episode ended after 73 steps\n",
      "Accumulated reward in this episode -84.18029881907148\n",
      "Mean reward so far -113.25\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 81\n",
      "Episode ended after 143 steps\n",
      "Accumulated reward in this episode -105.80665001343111\n",
      "Mean reward so far -113.16\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 82\n",
      "Episode ended after 107 steps\n",
      "Accumulated reward in this episode -80.44167051463563\n",
      "Mean reward so far -112.76\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 83\n",
      "Episode ended after 120 steps\n",
      "Accumulated reward in this episode -105.9054092478621\n",
      "Mean reward so far -112.68\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 84\n",
      "Episode ended after 93 steps\n",
      "Accumulated reward in this episode -98.21779640143879\n",
      "Mean reward so far -112.51\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 85\n",
      "Episode ended after 134 steps\n",
      "Accumulated reward in this episode -27.948652687324156\n",
      "Mean reward so far -111.53\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 86\n",
      "Episode ended after 121 steps\n",
      "Accumulated reward in this episode -165.4474596490436\n",
      "Mean reward so far -112.15\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 87\n",
      "Episode ended after 80 steps\n",
      "Accumulated reward in this episode -56.6273991550471\n",
      "Mean reward so far -111.52\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 88\n",
      "Episode ended after 134 steps\n",
      "Accumulated reward in this episode -53.231705702675555\n",
      "Mean reward so far -110.86\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 89\n",
      "Episode ended after 105 steps\n",
      "Accumulated reward in this episode -147.08986465394048\n",
      "Mean reward so far -111.27\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 90\n",
      "Episode ended after 133 steps\n",
      "Accumulated reward in this episode -104.07510801637571\n",
      "Mean reward so far -111.19\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 91\n",
      "Episode ended after 125 steps\n",
      "Accumulated reward in this episode -139.72438564024247\n",
      "Mean reward so far -111.50\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 92\n",
      "Episode ended after 145 steps\n",
      "Accumulated reward in this episode -88.42508965759464\n",
      "Mean reward so far -111.25\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 93\n",
      "Episode ended after 138 steps\n",
      "Accumulated reward in this episode -164.72773747016845\n",
      "Mean reward so far -111.82\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 94\n",
      "Episode ended after 105 steps\n",
      "Accumulated reward in this episode -82.03002370686183\n",
      "Mean reward so far -111.50\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 95\n",
      "Episode ended after 84 steps\n",
      "Accumulated reward in this episode -25.452570576079395\n",
      "Mean reward so far -110.61\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 96\n",
      "Episode ended after 92 steps\n",
      "Accumulated reward in this episode -27.33025464441522\n",
      "Mean reward so far -109.75\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 97\n",
      "Episode ended after 137 steps\n",
      "Accumulated reward in this episode -68.4931545052844\n",
      "Mean reward so far -109.33\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 98\n",
      "Episode ended after 97 steps\n",
      "Accumulated reward in this episode -59.38308055485079\n",
      "Mean reward so far -108.82\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 99\n",
      "Episode ended after 138 steps\n",
      "Accumulated reward in this episode -2.3975312097182453\n",
      "Mean reward so far -107.76\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 100\n",
      "Episode ended after 143 steps\n",
      "Accumulated reward in this episode -151.90152357111407\n",
      "Mean reward so far -108.20\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 101\n",
      "Episode ended after 198 steps\n",
      "Accumulated reward in this episode -123.13857165127475\n",
      "Mean reward so far -108.34\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 102\n",
      "Episode ended after 115 steps\n",
      "Accumulated reward in this episode -164.3904162444631\n",
      "Mean reward so far -108.89\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 103\n",
      "Episode ended after 94 steps\n",
      "Accumulated reward in this episode -113.68715748550113\n",
      "Mean reward so far -108.93\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 104\n",
      "Episode ended after 117 steps\n",
      "Accumulated reward in this episode -95.21101174881446\n",
      "Mean reward so far -108.80\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 105\n",
      "Episode ended after 121 steps\n",
      "Accumulated reward in this episode -97.64422552427183\n",
      "Mean reward so far -108.70\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 106\n",
      "Episode ended after 78 steps\n",
      "Accumulated reward in this episode -22.18599038960805\n",
      "Mean reward so far -107.89\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 107\n",
      "Episode ended after 106 steps\n",
      "Accumulated reward in this episode -57.798514069719886\n",
      "Mean reward so far -107.43\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 108\n",
      "Episode ended after 120 steps\n",
      "Accumulated reward in this episode -68.68473289371792\n",
      "Mean reward so far -107.07\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 109\n",
      "Episode ended after 242 steps\n",
      "Accumulated reward in this episode -51.668168690527665\n",
      "Mean reward so far -106.57\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 110\n",
      "Episode ended after 117 steps\n",
      "Accumulated reward in this episode -96.67013732056226\n",
      "Mean reward so far -106.48\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 111\n",
      "Episode ended after 237 steps\n",
      "Accumulated reward in this episode -4.519476106717839\n",
      "Mean reward so far -105.57\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 112\n",
      "Episode ended after 146 steps\n",
      "Accumulated reward in this episode -99.77226938365575\n",
      "Mean reward so far -105.52\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 113\n",
      "Episode ended after 159 steps\n",
      "Accumulated reward in this episode -40.568125919184006\n",
      "Mean reward so far -104.95\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 114\n",
      "Episode ended after 201 steps\n",
      "Accumulated reward in this episode -154.8596632913988\n",
      "Mean reward so far -105.38\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 115\n",
      "Episode ended after 215 steps\n",
      "Accumulated reward in this episode -73.17741622503401\n",
      "Mean reward so far -105.10\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 116\n",
      "Episode ended after 96 steps\n",
      "Accumulated reward in this episode -43.919684264709986\n",
      "Mean reward so far -104.58\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 117\n",
      "Episode ended after 125 steps\n",
      "Accumulated reward in this episode -149.2781525170596\n",
      "Mean reward so far -104.96\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 118\n",
      "Episode ended after 124 steps\n",
      "Accumulated reward in this episode -116.5171626270317\n",
      "Mean reward so far -105.06\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 119\n",
      "Episode ended after 286 steps\n",
      "Accumulated reward in this episode -118.42816598867512\n",
      "Mean reward so far -105.17\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 120\n",
      "Episode ended after 175 steps\n",
      "Accumulated reward in this episode -39.24489063002778\n",
      "Mean reward so far -104.62\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 121\n",
      "Episode ended after 122 steps\n",
      "Accumulated reward in this episode -152.0668933162636\n",
      "Mean reward so far -105.01\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 122\n",
      "Episode ended after 89 steps\n",
      "Accumulated reward in this episode -21.564189870633967\n",
      "Mean reward so far -104.33\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 123\n",
      "Episode ended after 90 steps\n",
      "Accumulated reward in this episode -79.07775413655395\n",
      "Mean reward so far -104.13\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 124\n",
      "Episode ended after 198 steps\n",
      "Accumulated reward in this episode -150.02604398732421\n",
      "Mean reward so far -104.50\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 125\n",
      "Episode ended after 161 steps\n",
      "Accumulated reward in this episode -100.80513315566172\n",
      "Mean reward so far -104.47\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 126\n",
      "Episode ended after 202 steps\n",
      "Accumulated reward in this episode -87.01337519146736\n",
      "Mean reward so far -104.33\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 127\n",
      "Episode ended after 129 steps\n",
      "Accumulated reward in this episode -27.679910202888472\n",
      "Mean reward so far -103.73\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 128\n",
      "Episode ended after 227 steps\n",
      "Accumulated reward in this episode -58.87507606138988\n",
      "Mean reward so far -103.38\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 129\n",
      "Episode ended after 134 steps\n",
      "Accumulated reward in this episode -19.01389976335595\n",
      "Mean reward so far -102.73\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 130\n",
      "Episode ended after 94 steps\n",
      "Accumulated reward in this episode -43.23037725509612\n",
      "Mean reward so far -102.28\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 131\n",
      "Episode ended after 304 steps\n",
      "Accumulated reward in this episode -174.61106806676912\n",
      "Mean reward so far -102.83\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 132\n",
      "Episode ended after 147 steps\n",
      "Accumulated reward in this episode -160.3411712620772\n",
      "Mean reward so far -103.26\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 133\n",
      "Episode ended after 175 steps\n",
      "Accumulated reward in this episode -44.070853580937\n",
      "Mean reward so far -102.82\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 134\n",
      "Episode ended after 278 steps\n",
      "Accumulated reward in this episode -89.49602360797115\n",
      "Mean reward so far -102.72\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 135\n",
      "Episode ended after 364 steps\n",
      "Accumulated reward in this episode -65.15349766486916\n",
      "Mean reward so far -102.44\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 136\n",
      "Episode ended after 131 steps\n",
      "Accumulated reward in this episode -64.27232369065608\n",
      "Mean reward so far -102.16\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 137\n",
      "Episode ended after 135 steps\n",
      "Accumulated reward in this episode -41.432804037258904\n",
      "Mean reward so far -101.72\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 138\n",
      "Episode ended after 253 steps\n",
      "Accumulated reward in this episode -78.50330582955591\n",
      "Mean reward so far -101.56\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 139\n",
      "Episode ended after 355 steps\n",
      "Accumulated reward in this episode -238.8118923853602\n",
      "Mean reward so far -102.54\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 140\n",
      "Episode ended after 96 steps\n",
      "Accumulated reward in this episode -31.972669759844578\n",
      "Mean reward so far -102.04\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 141\n",
      "Episode ended after 522 steps\n",
      "Accumulated reward in this episode -175.93913549595274\n",
      "Mean reward so far -102.56\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 142\n",
      "Episode ended after 368 steps\n",
      "Accumulated reward in this episode -62.10310494442082\n",
      "Mean reward so far -102.27\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 143\n",
      "Episode ended after 292 steps\n",
      "Accumulated reward in this episode -84.73551158155422\n",
      "Mean reward so far -102.15\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 144\n",
      "Episode ended after 171 steps\n",
      "Accumulated reward in this episode 15.811912029563622\n",
      "Mean reward so far -101.34\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 145\n",
      "Episode ended after 164 steps\n",
      "Accumulated reward in this episode -49.37029770076604\n",
      "Mean reward so far -100.98\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 146\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode -26.77700448223441\n",
      "Mean reward so far -100.48\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 147\n",
      "Episode ended after 544 steps\n",
      "Accumulated reward in this episode -53.2880944256959\n",
      "Mean reward so far -100.16\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 148\n",
      "Episode ended after 161 steps\n",
      "Accumulated reward in this episode -49.28596361364693\n",
      "Mean reward so far -99.82\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 149\n",
      "Episode ended after 151 steps\n",
      "Accumulated reward in this episode -158.52034822244707\n",
      "Mean reward so far -100.21\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 150\n",
      "Episode ended after 141 steps\n",
      "Accumulated reward in this episode -29.003453896175863\n",
      "Mean reward so far -99.74\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 151\n",
      "Episode ended after 148 steps\n",
      "Accumulated reward in this episode -77.60721377779045\n",
      "Mean reward so far -99.59\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 152\n",
      "Episode ended after 291 steps\n",
      "Accumulated reward in this episode -74.76417595209644\n",
      "Mean reward so far -99.43\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 153\n",
      "Episode ended after 198 steps\n",
      "Accumulated reward in this episode -14.622612697787574\n",
      "Mean reward so far -98.88\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 154\n",
      "Episode ended after 155 steps\n",
      "Accumulated reward in this episode 44.75387203511056\n",
      "Mean reward so far -97.95\n",
      "Maximal reward so far 114.56594445990453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 155\n",
      "Episode ended after 387 steps\n",
      "Accumulated reward in this episode -33.84045530299455\n",
      "Mean reward so far -97.54\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 156\n",
      "Episode ended after 359 steps\n",
      "Accumulated reward in this episode -119.09525070613263\n",
      "Mean reward so far -97.68\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 157\n",
      "Episode ended after 220 steps\n",
      "Accumulated reward in this episode -136.0092879765558\n",
      "Mean reward so far -97.92\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 158\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode -62.52747716423342\n",
      "Mean reward so far -97.70\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 159\n",
      "Episode ended after 450 steps\n",
      "Accumulated reward in this episode -27.954542737565873\n",
      "Mean reward so far -97.26\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 160\n",
      "Episode ended after 413 steps\n",
      "Accumulated reward in this episode -55.072087727281406\n",
      "Mean reward so far -97.00\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 161\n",
      "Episode ended after 163 steps\n",
      "Accumulated reward in this episode -42.58098232698105\n",
      "Mean reward so far -96.67\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 162\n",
      "Episode ended after 328 steps\n",
      "Accumulated reward in this episode -88.63058416018316\n",
      "Mean reward so far -96.62\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 163\n",
      "Episode ended after 268 steps\n",
      "Accumulated reward in this episode -87.45247240299936\n",
      "Mean reward so far -96.56\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 164\n",
      "Episode ended after 467 steps\n",
      "Accumulated reward in this episode -241.05772166227928\n",
      "Mean reward so far -97.44\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 165\n",
      "Episode ended after 112 steps\n",
      "Accumulated reward in this episode -128.23254770891054\n",
      "Mean reward so far -97.62\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 166\n",
      "Episode ended after 553 steps\n",
      "Accumulated reward in this episode -120.37738606700245\n",
      "Mean reward so far -97.76\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 167\n",
      "Episode ended after 265 steps\n",
      "Accumulated reward in this episode -37.825690468705034\n",
      "Mean reward so far -97.40\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 168\n",
      "Episode ended after 263 steps\n",
      "Accumulated reward in this episode -210.60095165571278\n",
      "Mean reward so far -98.07\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 169\n",
      "Episode ended after 165 steps\n",
      "Accumulated reward in this episode -21.896092061970943\n",
      "Mean reward so far -97.62\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 170\n",
      "Episode ended after 165 steps\n",
      "Accumulated reward in this episode -48.67225329880479\n",
      "Mean reward so far -97.34\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 171\n",
      "Episode ended after 135 steps\n",
      "Accumulated reward in this episode -66.11249316315076\n",
      "Mean reward so far -97.15\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 172\n",
      "Episode ended after 254 steps\n",
      "Accumulated reward in this episode -100.40145437892281\n",
      "Mean reward so far -97.17\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 173\n",
      "Episode ended after 535 steps\n",
      "Accumulated reward in this episode -78.18846671085899\n",
      "Mean reward so far -97.06\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 174\n",
      "Episode ended after 97 steps\n",
      "Accumulated reward in this episode -27.7573347750876\n",
      "Mean reward so far -96.67\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 175\n",
      "Episode ended after 153 steps\n",
      "Accumulated reward in this episode -46.2246348981353\n",
      "Mean reward so far -96.38\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 176\n",
      "Episode ended after 121 steps\n",
      "Accumulated reward in this episode -2.616637926775283\n",
      "Mean reward so far -95.85\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 177\n",
      "Episode ended after 161 steps\n",
      "Accumulated reward in this episode -121.30901189679909\n",
      "Mean reward so far -96.00\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 178\n",
      "Episode ended after 120 steps\n",
      "Accumulated reward in this episode -34.793496010764585\n",
      "Mean reward so far -95.65\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 179\n",
      "Episode ended after 174 steps\n",
      "Accumulated reward in this episode -32.96175713262451\n",
      "Mean reward so far -95.31\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 180\n",
      "Episode ended after 127 steps\n",
      "Accumulated reward in this episode -22.040966890619785\n",
      "Mean reward so far -94.90\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 181\n",
      "Episode ended after 114 steps\n",
      "Accumulated reward in this episode -125.73367233393009\n",
      "Mean reward so far -95.07\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 182\n",
      "Episode ended after 247 steps\n",
      "Accumulated reward in this episode -37.23330321921786\n",
      "Mean reward so far -94.75\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 183\n",
      "Episode ended after 118 steps\n",
      "Accumulated reward in this episode -35.61056576106252\n",
      "Mean reward so far -94.43\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 184\n",
      "Episode ended after 149 steps\n",
      "Accumulated reward in this episode -77.99628675904313\n",
      "Mean reward so far -94.34\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 185\n",
      "Episode ended after 112 steps\n",
      "Accumulated reward in this episode -121.11529156404325\n",
      "Mean reward so far -94.49\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 186\n",
      "Episode ended after 115 steps\n",
      "Accumulated reward in this episode -87.81522674857355\n",
      "Mean reward so far -94.45\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 187\n",
      "Episode ended after 132 steps\n",
      "Accumulated reward in this episode -128.88269734010873\n",
      "Mean reward so far -94.63\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 188\n",
      "Episode ended after 170 steps\n",
      "Accumulated reward in this episode -23.221111983115787\n",
      "Mean reward so far -94.26\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 189\n",
      "Episode ended after 133 steps\n",
      "Accumulated reward in this episode -40.67844731356288\n",
      "Mean reward so far -93.97\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 190\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 57.03123116333981\n",
      "Mean reward so far -93.18\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 191\n",
      "Episode ended after 160 steps\n",
      "Accumulated reward in this episode -134.73817263231342\n",
      "Mean reward so far -93.40\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 192\n",
      "Episode ended after 120 steps\n",
      "Accumulated reward in this episode -77.102381293798\n",
      "Mean reward so far -93.32\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 193\n",
      "Episode ended after 144 steps\n",
      "Accumulated reward in this episode -167.25419531486517\n",
      "Mean reward so far -93.70\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 194\n",
      "Episode ended after 144 steps\n",
      "Accumulated reward in this episode -77.94136234101751\n",
      "Mean reward so far -93.62\n",
      "Maximal reward so far 114.56594445990453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 195\n",
      "Episode ended after 115 steps\n",
      "Accumulated reward in this episode -140.0884347711034\n",
      "Mean reward so far -93.85\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 196\n",
      "Episode ended after 178 steps\n",
      "Accumulated reward in this episode 26.18780391930035\n",
      "Mean reward so far -93.24\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 197\n",
      "Episode ended after 125 steps\n",
      "Accumulated reward in this episode -179.94788556785744\n",
      "Mean reward so far -93.68\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 198\n",
      "Episode ended after 102 steps\n",
      "Accumulated reward in this episode -28.748231386967234\n",
      "Mean reward so far -93.36\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 199\n",
      "Episode ended after 112 steps\n",
      "Accumulated reward in this episode -64.6498997200598\n",
      "Mean reward so far -93.21\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 200\n",
      "Episode ended after 143 steps\n",
      "Accumulated reward in this episode -10.898800102579301\n",
      "Mean reward so far -92.80\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 201\n",
      "Episode ended after 140 steps\n",
      "Accumulated reward in this episode -78.3679837329242\n",
      "Mean reward so far -92.73\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 202\n",
      "Episode ended after 140 steps\n",
      "Accumulated reward in this episode -84.63308739120146\n",
      "Mean reward so far -92.69\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 203\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 98.64796168376226\n",
      "Mean reward so far -91.75\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 204\n",
      "Episode ended after 172 steps\n",
      "Accumulated reward in this episode 20.236415263048627\n",
      "Mean reward so far -91.21\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 205\n",
      "Episode ended after 157 steps\n",
      "Accumulated reward in this episode 33.275697775691\n",
      "Mean reward so far -90.60\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 206\n",
      "Episode ended after 97 steps\n",
      "Accumulated reward in this episode 5.843405295619647\n",
      "Mean reward so far -90.14\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 207\n",
      "Episode ended after 179 steps\n",
      "Accumulated reward in this episode -1.8454853184339406\n",
      "Mean reward so far -89.71\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 208\n",
      "Episode ended after 116 steps\n",
      "Accumulated reward in this episode -84.26600787255288\n",
      "Mean reward so far -89.69\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 209\n",
      "Episode ended after 117 steps\n",
      "Accumulated reward in this episode 24.334618882342752\n",
      "Mean reward so far -89.14\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 210\n",
      "Episode ended after 174 steps\n",
      "Accumulated reward in this episode -13.934264107520686\n",
      "Mean reward so far -88.79\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 211\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode -110.42338378574475\n",
      "Mean reward so far -88.89\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 212\n",
      "Episode ended after 175 steps\n",
      "Accumulated reward in this episode -76.25080588567855\n",
      "Mean reward so far -88.83\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 213\n",
      "Episode ended after 149 steps\n",
      "Accumulated reward in this episode -41.14920740265994\n",
      "Mean reward so far -88.61\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 214\n",
      "Episode ended after 164 steps\n",
      "Accumulated reward in this episode -47.73247298775116\n",
      "Mean reward so far -88.42\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 215\n",
      "Episode ended after 116 steps\n",
      "Accumulated reward in this episode -141.44502819190268\n",
      "Mean reward so far -88.66\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 216\n",
      "Episode ended after 191 steps\n",
      "Accumulated reward in this episode -0.13154314683165325\n",
      "Mean reward so far -88.25\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 217\n",
      "Episode ended after 203 steps\n",
      "Accumulated reward in this episode -224.51482748888682\n",
      "Mean reward so far -88.88\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 218\n",
      "Episode ended after 165 steps\n",
      "Accumulated reward in this episode -190.21243877970193\n",
      "Mean reward so far -89.34\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 219\n",
      "Episode ended after 510 steps\n",
      "Accumulated reward in this episode -152.4199651091843\n",
      "Mean reward so far -89.63\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 220\n",
      "Episode ended after 156 steps\n",
      "Accumulated reward in this episode -183.38729731489778\n",
      "Mean reward so far -90.05\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 221\n",
      "Episode ended after 274 steps\n",
      "Accumulated reward in this episode -105.10208103462142\n",
      "Mean reward so far -90.12\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 222\n",
      "Episode ended after 223 steps\n",
      "Accumulated reward in this episode -247.73201153909187\n",
      "Mean reward so far -90.83\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 223\n",
      "Episode ended after 274 steps\n",
      "Accumulated reward in this episode -191.8710033948881\n",
      "Mean reward so far -91.28\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 224\n",
      "Episode ended after 403 steps\n",
      "Accumulated reward in this episode -237.26621539573358\n",
      "Mean reward so far -91.93\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 225\n",
      "Episode ended after 357 steps\n",
      "Accumulated reward in this episode -193.0275862531645\n",
      "Mean reward so far -92.38\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 226\n",
      "Episode ended after 321 steps\n",
      "Accumulated reward in this episode -307.9001431641192\n",
      "Mean reward so far -93.32\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 227\n",
      "Episode ended after 212 steps\n",
      "Accumulated reward in this episode -243.1711586281755\n",
      "Mean reward so far -93.98\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 228\n",
      "Episode ended after 795 steps\n",
      "Accumulated reward in this episode -259.2194974670686\n",
      "Mean reward so far -94.70\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 229\n",
      "Episode ended after 400 steps\n",
      "Accumulated reward in this episode -170.97933228603648\n",
      "Mean reward so far -95.03\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 230\n",
      "Episode ended after 474 steps\n",
      "Accumulated reward in this episode -278.6783888358518\n",
      "Mean reward so far -95.83\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 231\n",
      "Episode ended after 310 steps\n",
      "Accumulated reward in this episode -171.88127111854186\n",
      "Mean reward so far -96.16\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 232\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -10.715061923582965\n",
      "Mean reward so far -95.79\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 233\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -201.5744633916168\n",
      "Mean reward so far -96.24\n",
      "Maximal reward so far 114.56594445990453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 234\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -38.467869282875775\n",
      "Mean reward so far -96.00\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 235\n",
      "Episode ended after 322 steps\n",
      "Accumulated reward in this episode -60.47363980725886\n",
      "Mean reward so far -95.85\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 236\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 37.55848176522571\n",
      "Mean reward so far -95.28\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 237\n",
      "Episode ended after 432 steps\n",
      "Accumulated reward in this episode -14.89825981091252\n",
      "Mean reward so far -94.95\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 238\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 63.84451338943076\n",
      "Mean reward so far -94.28\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 239\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -9.080386295903693\n",
      "Mean reward so far -93.93\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 240\n",
      "Episode ended after 121 steps\n",
      "Accumulated reward in this episode 28.673653629176584\n",
      "Mean reward so far -93.42\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 241\n",
      "Episode ended after 108 steps\n",
      "Accumulated reward in this episode -13.628967918200814\n",
      "Mean reward so far -93.09\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 242\n",
      "Episode ended after 175 steps\n",
      "Accumulated reward in this episode -53.90913925305\n",
      "Mean reward so far -92.93\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 243\n",
      "Episode ended after 366 steps\n",
      "Accumulated reward in this episode -140.08802128214722\n",
      "Mean reward so far -93.12\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 244\n",
      "Episode ended after 217 steps\n",
      "Accumulated reward in this episode -16.50588442653971\n",
      "Mean reward so far -92.81\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 245\n",
      "Episode ended after 126 steps\n",
      "Accumulated reward in this episode -25.08059938307052\n",
      "Mean reward so far -92.53\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 246\n",
      "Episode ended after 136 steps\n",
      "Accumulated reward in this episode -53.709910012989084\n",
      "Mean reward so far -92.38\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 247\n",
      "Episode ended after 123 steps\n",
      "Accumulated reward in this episode 13.068576265061523\n",
      "Mean reward so far -91.95\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 248\n",
      "Episode ended after 133 steps\n",
      "Accumulated reward in this episode 26.024657587591854\n",
      "Mean reward so far -91.48\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 249\n",
      "Episode ended after 128 steps\n",
      "Accumulated reward in this episode 31.7106219320862\n",
      "Mean reward so far -90.98\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 250\n",
      "Episode ended after 110 steps\n",
      "Accumulated reward in this episode 0.9980314002433772\n",
      "Mean reward so far -90.62\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 251\n",
      "Episode ended after 116 steps\n",
      "Accumulated reward in this episode -17.48195851568309\n",
      "Mean reward so far -90.33\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 252\n",
      "Episode ended after 103 steps\n",
      "Accumulated reward in this episode 28.245810582832235\n",
      "Mean reward so far -89.86\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 253\n",
      "Episode ended after 143 steps\n",
      "Accumulated reward in this episode -36.25045386905652\n",
      "Mean reward so far -89.65\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 254\n",
      "Episode ended after 193 steps\n",
      "Accumulated reward in this episode -53.71031930697734\n",
      "Mean reward so far -89.51\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 255\n",
      "Episode ended after 124 steps\n",
      "Accumulated reward in this episode 9.975174699576357\n",
      "Mean reward so far -89.12\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 256\n",
      "Episode ended after 164 steps\n",
      "Accumulated reward in this episode -11.916816765563793\n",
      "Mean reward so far -88.82\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 257\n",
      "Episode ended after 132 steps\n",
      "Accumulated reward in this episode -16.74066615024213\n",
      "Mean reward so far -88.54\n",
      "Maximal reward so far 114.56594445990453\n",
      "-------------------------------------------------\n",
      "Episode 258\n",
      "Episode ended after 304 steps\n",
      "Accumulated reward in this episode 191.33173593617803\n",
      "Mean reward so far -87.46\n",
      "Maximal reward so far 191.33173593617803\n",
      "-------------------------------------------------\n",
      "Episode 259\n",
      "Episode ended after 167 steps\n",
      "Accumulated reward in this episode -25.582914570795857\n",
      "Mean reward so far -87.22\n",
      "Maximal reward so far 191.33173593617803\n",
      "-------------------------------------------------\n",
      "Episode 260\n",
      "Episode ended after 134 steps\n",
      "Accumulated reward in this episode 2.1075131002530227\n",
      "Mean reward so far -86.88\n",
      "Maximal reward so far 191.33173593617803\n",
      "-------------------------------------------------\n",
      "Episode 261\n",
      "Episode ended after 955 steps\n",
      "Accumulated reward in this episode 181.22376577488254\n",
      "Mean reward so far -85.85\n",
      "Maximal reward so far 191.33173593617803\n",
      "-------------------------------------------------\n",
      "Episode 262\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 96.16646865221956\n",
      "Mean reward so far -85.16\n",
      "Maximal reward so far 191.33173593617803\n",
      "-------------------------------------------------\n",
      "Episode 263\n",
      "Episode ended after 154 steps\n",
      "Accumulated reward in this episode -12.40263266284941\n",
      "Mean reward so far -84.89\n",
      "Maximal reward so far 191.33173593617803\n",
      "-------------------------------------------------\n",
      "Episode 264\n",
      "Episode ended after 147 steps\n",
      "Accumulated reward in this episode -35.82241332638532\n",
      "Mean reward so far -84.70\n",
      "Maximal reward so far 191.33173593617803\n",
      "-------------------------------------------------\n",
      "Episode 265\n",
      "Episode ended after 161 steps\n",
      "Accumulated reward in this episode 55.14710774685635\n",
      "Mean reward so far -84.18\n",
      "Maximal reward so far 191.33173593617803\n",
      "-------------------------------------------------\n",
      "Episode 266\n",
      "Episode ended after 141 steps\n",
      "Accumulated reward in this episode -12.426087439903691\n",
      "Mean reward so far -83.91\n",
      "Maximal reward so far 191.33173593617803\n",
      "-------------------------------------------------\n",
      "Episode 267\n",
      "Episode ended after 381 steps\n",
      "Accumulated reward in this episode 226.52818069013912\n",
      "Mean reward so far -82.75\n",
      "Maximal reward so far 226.52818069013912\n",
      "-------------------------------------------------\n",
      "Episode 268\n",
      "Episode ended after 113 steps\n",
      "Accumulated reward in this episode -10.277623861314439\n",
      "Mean reward so far -82.48\n",
      "Maximal reward so far 226.52818069013912\n",
      "-------------------------------------------------\n",
      "Episode 269\n",
      "Episode ended after 297 steps\n",
      "Accumulated reward in this episode 273.5999933458608\n",
      "Mean reward so far -81.16\n",
      "Maximal reward so far 273.5999933458608\n",
      "-------------------------------------------------\n",
      "Episode 270\n",
      "Episode ended after 475 steps\n",
      "Accumulated reward in this episode 285.7643496087661\n",
      "Mean reward so far -79.81\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 271\n",
      "Episode ended after 333 steps\n",
      "Accumulated reward in this episode 200.42594019335576\n",
      "Mean reward so far -78.78\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 272\n",
      "Episode ended after 166 steps\n",
      "Accumulated reward in this episode -52.13971395431817\n",
      "Mean reward so far -78.68\n",
      "Maximal reward so far 285.7643496087661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 273\n",
      "Episode ended after 309 steps\n",
      "Accumulated reward in this episode 251.16367036014768\n",
      "Mean reward so far -77.47\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 274\n",
      "Episode ended after 244 steps\n",
      "Accumulated reward in this episode 1.66064661183934\n",
      "Mean reward so far -77.19\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 275\n",
      "Episode ended after 144 steps\n",
      "Accumulated reward in this episode 2.634853924915557\n",
      "Mean reward so far -76.90\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 276\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 22.32424326708969\n",
      "Mean reward so far -76.54\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 277\n",
      "Episode ended after 343 steps\n",
      "Accumulated reward in this episode 179.55952437396968\n",
      "Mean reward so far -75.62\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 278\n",
      "Episode ended after 184 steps\n",
      "Accumulated reward in this episode 37.837883978897466\n",
      "Mean reward so far -75.21\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 279\n",
      "Episode ended after 156 steps\n",
      "Accumulated reward in this episode 3.375633017415055\n",
      "Mean reward so far -74.93\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 280\n",
      "Episode ended after 157 steps\n",
      "Accumulated reward in this episode 14.73249907362892\n",
      "Mean reward so far -74.61\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 281\n",
      "Episode ended after 168 steps\n",
      "Accumulated reward in this episode -15.714864999323915\n",
      "Mean reward so far -74.40\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 282\n",
      "Episode ended after 186 steps\n",
      "Accumulated reward in this episode -23.66848817160855\n",
      "Mean reward so far -74.22\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 283\n",
      "Episode ended after 188 steps\n",
      "Accumulated reward in this episode -38.93576085512734\n",
      "Mean reward so far -74.10\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 284\n",
      "Episode ended after 306 steps\n",
      "Accumulated reward in this episode 239.76913642832233\n",
      "Mean reward so far -73.00\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 285\n",
      "Episode ended after 462 steps\n",
      "Accumulated reward in this episode 236.2620152940935\n",
      "Mean reward so far -71.92\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 286\n",
      "Episode ended after 185 steps\n",
      "Accumulated reward in this episode 48.972497344937935\n",
      "Mean reward so far -71.50\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 287\n",
      "Episode ended after 111 steps\n",
      "Accumulated reward in this episode -23.713855256750037\n",
      "Mean reward so far -71.33\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 288\n",
      "Episode ended after 214 steps\n",
      "Accumulated reward in this episode -38.4923243459168\n",
      "Mean reward so far -71.22\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 289\n",
      "Episode ended after 212 steps\n",
      "Accumulated reward in this episode -145.3056249056868\n",
      "Mean reward so far -71.47\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 290\n",
      "Episode ended after 185 steps\n",
      "Accumulated reward in this episode -75.26058716232937\n",
      "Mean reward so far -71.48\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 291\n",
      "Episode ended after 222 steps\n",
      "Accumulated reward in this episode -64.17184170820133\n",
      "Mean reward so far -71.46\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 292\n",
      "Episode ended after 392 steps\n",
      "Accumulated reward in this episode 139.68240804923747\n",
      "Mean reward so far -70.74\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 293\n",
      "Episode ended after 166 steps\n",
      "Accumulated reward in this episode -47.189395017134466\n",
      "Mean reward so far -70.66\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 294\n",
      "Episode ended after 243 steps\n",
      "Accumulated reward in this episode -102.30809480909424\n",
      "Mean reward so far -70.77\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 295\n",
      "Episode ended after 312 steps\n",
      "Accumulated reward in this episode -137.6700404393613\n",
      "Mean reward so far -70.99\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 296\n",
      "Episode ended after 201 steps\n",
      "Accumulated reward in this episode -235.62661409409128\n",
      "Mean reward so far -71.55\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 297\n",
      "Episode ended after 206 steps\n",
      "Accumulated reward in this episode -61.488349643870244\n",
      "Mean reward so far -71.51\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 298\n",
      "Episode ended after 375 steps\n",
      "Accumulated reward in this episode -204.78468814614314\n",
      "Mean reward so far -71.96\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 299\n",
      "Episode ended after 284 steps\n",
      "Accumulated reward in this episode 37.929468417453876\n",
      "Mean reward so far -71.59\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 300\n",
      "Episode ended after 218 steps\n",
      "Accumulated reward in this episode -268.50863960672785\n",
      "Mean reward so far -72.25\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 301\n",
      "Episode ended after 300 steps\n",
      "Accumulated reward in this episode -135.90190359507034\n",
      "Mean reward so far -72.46\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 302\n",
      "Episode ended after 259 steps\n",
      "Accumulated reward in this episode -149.99269812263296\n",
      "Mean reward so far -72.71\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 303\n",
      "Episode ended after 332 steps\n",
      "Accumulated reward in this episode -173.1610154924864\n",
      "Mean reward so far -73.04\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 304\n",
      "Episode ended after 287 steps\n",
      "Accumulated reward in this episode -137.7504877701618\n",
      "Mean reward so far -73.26\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 305\n",
      "Episode ended after 327 steps\n",
      "Accumulated reward in this episode -144.62458440727175\n",
      "Mean reward so far -73.49\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 306\n",
      "Episode ended after 229 steps\n",
      "Accumulated reward in this episode -158.53738172754385\n",
      "Mean reward so far -73.77\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 307\n",
      "Episode ended after 163 steps\n",
      "Accumulated reward in this episode -78.0259772049159\n",
      "Mean reward so far -73.78\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 308\n",
      "Episode ended after 271 steps\n",
      "Accumulated reward in this episode -133.25663409844765\n",
      "Mean reward so far -73.97\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 309\n",
      "Episode ended after 425 steps\n",
      "Accumulated reward in this episode -131.94158314426397\n",
      "Mean reward so far -74.16\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 310\n",
      "Episode ended after 359 steps\n",
      "Accumulated reward in this episode -136.243504386437\n",
      "Mean reward so far -74.36\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 311\n",
      "Episode ended after 313 steps\n",
      "Accumulated reward in this episode -143.04046223692512\n",
      "Mean reward so far -74.58\n",
      "Maximal reward so far 285.7643496087661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 312\n",
      "Episode ended after 235 steps\n",
      "Accumulated reward in this episode -89.86156035884885\n",
      "Mean reward so far -74.63\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 313\n",
      "Episode ended after 413 steps\n",
      "Accumulated reward in this episode 192.6648251029997\n",
      "Mean reward so far -73.78\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 314\n",
      "Episode ended after 213 steps\n",
      "Accumulated reward in this episode 9.066204360342908\n",
      "Mean reward so far -73.51\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 315\n",
      "Episode ended after 272 steps\n",
      "Accumulated reward in this episode -60.64171220706727\n",
      "Mean reward so far -73.47\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 316\n",
      "Episode ended after 349 steps\n",
      "Accumulated reward in this episode -141.36963681368292\n",
      "Mean reward so far -73.69\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 317\n",
      "Episode ended after 305 steps\n",
      "Accumulated reward in this episode -109.10018721174649\n",
      "Mean reward so far -73.80\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 318\n",
      "Episode ended after 418 steps\n",
      "Accumulated reward in this episode -168.15226970549782\n",
      "Mean reward so far -74.09\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 319\n",
      "Episode ended after 238 steps\n",
      "Accumulated reward in this episode -151.87749797364586\n",
      "Mean reward so far -74.34\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 320\n",
      "Episode ended after 279 steps\n",
      "Accumulated reward in this episode -190.78366593357913\n",
      "Mean reward so far -74.70\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 321\n",
      "Episode ended after 438 steps\n",
      "Accumulated reward in this episode -120.84172805978656\n",
      "Mean reward so far -74.84\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 322\n",
      "Episode ended after 312 steps\n",
      "Accumulated reward in this episode -78.66015466664784\n",
      "Mean reward so far -74.85\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 323\n",
      "Episode ended after 288 steps\n",
      "Accumulated reward in this episode -45.29909245616971\n",
      "Mean reward so far -74.76\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 324\n",
      "Episode ended after 627 steps\n",
      "Accumulated reward in this episode 206.59102823391808\n",
      "Mean reward so far -73.90\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 325\n",
      "Episode ended after 308 steps\n",
      "Accumulated reward in this episode -5.335264152588197\n",
      "Mean reward so far -73.69\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 326\n",
      "Episode ended after 449 steps\n",
      "Accumulated reward in this episode -253.15468090387714\n",
      "Mean reward so far -74.24\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 327\n",
      "Episode ended after 318 steps\n",
      "Accumulated reward in this episode -133.1877063270505\n",
      "Mean reward so far -74.42\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 328\n",
      "Episode ended after 338 steps\n",
      "Accumulated reward in this episode 192.52894593811163\n",
      "Mean reward so far -73.60\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 329\n",
      "Episode ended after 741 steps\n",
      "Accumulated reward in this episode 137.53554973278466\n",
      "Mean reward so far -72.96\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 330\n",
      "Episode ended after 337 steps\n",
      "Accumulated reward in this episode -22.328250335262243\n",
      "Mean reward so far -72.81\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 331\n",
      "Episode ended after 427 steps\n",
      "Accumulated reward in this episode -204.4589228165096\n",
      "Mean reward so far -73.21\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 332\n",
      "Episode ended after 348 steps\n",
      "Accumulated reward in this episode 218.9329427482463\n",
      "Mean reward so far -72.33\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 333\n",
      "Episode ended after 410 steps\n",
      "Accumulated reward in this episode -51.80140447394763\n",
      "Mean reward so far -72.27\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 334\n",
      "Episode ended after 506 steps\n",
      "Accumulated reward in this episode 203.27092526025245\n",
      "Mean reward so far -71.45\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 335\n",
      "Episode ended after 335 steps\n",
      "Accumulated reward in this episode -48.52983256625404\n",
      "Mean reward so far -71.38\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 336\n",
      "Episode ended after 420 steps\n",
      "Accumulated reward in this episode -183.0451943700446\n",
      "Mean reward so far -71.71\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 337\n",
      "Episode ended after 419 steps\n",
      "Accumulated reward in this episode -23.30732990338972\n",
      "Mean reward so far -71.57\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 338\n",
      "Episode ended after 724 steps\n",
      "Accumulated reward in this episode 202.14033347259843\n",
      "Mean reward so far -70.76\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 339\n",
      "Episode ended after 443 steps\n",
      "Accumulated reward in this episode -0.279149707146118\n",
      "Mean reward so far -70.55\n",
      "Maximal reward so far 285.7643496087661\n",
      "-------------------------------------------------\n",
      "Episode 340\n",
      "Episode ended after 403 steps\n",
      "Accumulated reward in this episode 293.2518340927227\n",
      "Mean reward so far -69.49\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 341\n",
      "Episode ended after 713 steps\n",
      "Accumulated reward in this episode 144.03770301825\n",
      "Mean reward so far -68.86\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 342\n",
      "Episode ended after 755 steps\n",
      "Accumulated reward in this episode 176.2705935497618\n",
      "Mean reward so far -68.15\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 343\n",
      "Episode ended after 199 steps\n",
      "Accumulated reward in this episode 11.52271001960596\n",
      "Mean reward so far -67.91\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 344\n",
      "Episode ended after 211 steps\n",
      "Accumulated reward in this episode -38.9853876063897\n",
      "Mean reward so far -67.83\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 345\n",
      "Episode ended after 769 steps\n",
      "Accumulated reward in this episode -93.34176706710356\n",
      "Mean reward so far -67.90\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 346\n",
      "Episode ended after 238 steps\n",
      "Accumulated reward in this episode -24.701986306323512\n",
      "Mean reward so far -67.78\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 347\n",
      "Episode ended after 560 steps\n",
      "Accumulated reward in this episode -40.08316330608156\n",
      "Mean reward so far -67.70\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 348\n",
      "Episode ended after 576 steps\n",
      "Accumulated reward in this episode 187.63901087039153\n",
      "Mean reward so far -66.97\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 349\n",
      "Episode ended after 589 steps\n",
      "Accumulated reward in this episode -70.16643380002364\n",
      "Mean reward so far -66.98\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 350\n",
      "Episode ended after 199 steps\n",
      "Accumulated reward in this episode 55.056337787852115\n",
      "Mean reward so far -66.63\n",
      "Maximal reward so far 293.2518340927227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 351\n",
      "Episode ended after 757 steps\n",
      "Accumulated reward in this episode 205.484085538246\n",
      "Mean reward so far -65.86\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 352\n",
      "Episode ended after 469 steps\n",
      "Accumulated reward in this episode -55.776750729653855\n",
      "Mean reward so far -65.83\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 353\n",
      "Episode ended after 535 steps\n",
      "Accumulated reward in this episode -209.675088495618\n",
      "Mean reward so far -66.24\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 354\n",
      "Episode ended after 859 steps\n",
      "Accumulated reward in this episode -103.02596775552834\n",
      "Mean reward so far -66.34\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 355\n",
      "Episode ended after 302 steps\n",
      "Accumulated reward in this episode -16.42976466867706\n",
      "Mean reward so far -66.20\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 356\n",
      "Episode ended after 395 steps\n",
      "Accumulated reward in this episode 175.53444024484963\n",
      "Mean reward so far -65.52\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 357\n",
      "Episode ended after 256 steps\n",
      "Accumulated reward in this episode -70.89425298818244\n",
      "Mean reward so far -65.54\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 358\n",
      "Episode ended after 365 steps\n",
      "Accumulated reward in this episode -17.153511496435442\n",
      "Mean reward so far -65.40\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 359\n",
      "Episode ended after 269 steps\n",
      "Accumulated reward in this episode -70.43199746961285\n",
      "Mean reward so far -65.42\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 360\n",
      "Episode ended after 387 steps\n",
      "Accumulated reward in this episode -17.171797835140467\n",
      "Mean reward so far -65.28\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 361\n",
      "Episode ended after 399 steps\n",
      "Accumulated reward in this episode 214.02836017783704\n",
      "Mean reward so far -64.51\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 362\n",
      "Episode ended after 685 steps\n",
      "Accumulated reward in this episode 202.12309436217737\n",
      "Mean reward so far -63.78\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 363\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -26.372399127570233\n",
      "Mean reward so far -63.67\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 364\n",
      "Episode ended after 600 steps\n",
      "Accumulated reward in this episode 160.9601919737812\n",
      "Mean reward so far -63.06\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 365\n",
      "Episode ended after 657 steps\n",
      "Accumulated reward in this episode -33.170244326566205\n",
      "Mean reward so far -62.98\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 366\n",
      "Episode ended after 297 steps\n",
      "Accumulated reward in this episode 275.25921221952376\n",
      "Mean reward so far -62.05\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 367\n",
      "Episode ended after 637 steps\n",
      "Accumulated reward in this episode -46.9273305988436\n",
      "Mean reward so far -62.01\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 368\n",
      "Episode ended after 689 steps\n",
      "Accumulated reward in this episode 151.50640918496669\n",
      "Mean reward so far -61.43\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 369\n",
      "Episode ended after 546 steps\n",
      "Accumulated reward in this episode 226.86382939761054\n",
      "Mean reward so far -60.66\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 370\n",
      "Episode ended after 269 steps\n",
      "Accumulated reward in this episode -31.497312514561287\n",
      "Mean reward so far -60.58\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 371\n",
      "Episode ended after 290 steps\n",
      "Accumulated reward in this episode 15.156871779814452\n",
      "Mean reward so far -60.37\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 372\n",
      "Episode ended after 478 steps\n",
      "Accumulated reward in this episode 229.69108823413683\n",
      "Mean reward so far -59.60\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 373\n",
      "Episode ended after 501 steps\n",
      "Accumulated reward in this episode 199.730720249392\n",
      "Mean reward so far -58.90\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 374\n",
      "Episode ended after 216 steps\n",
      "Accumulated reward in this episode -16.771006673083164\n",
      "Mean reward so far -58.79\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 375\n",
      "Episode ended after 564 steps\n",
      "Accumulated reward in this episode 206.29607579525617\n",
      "Mean reward so far -58.08\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 376\n",
      "Episode ended after 432 steps\n",
      "Accumulated reward in this episode -6.684456650967178\n",
      "Mean reward so far -57.95\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 377\n",
      "Episode ended after 235 steps\n",
      "Accumulated reward in this episode 29.40853242929701\n",
      "Mean reward so far -57.72\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 378\n",
      "Episode ended after 552 steps\n",
      "Accumulated reward in this episode 226.63154106060168\n",
      "Mean reward so far -56.97\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 379\n",
      "Episode ended after 301 steps\n",
      "Accumulated reward in this episode 233.8164631141837\n",
      "Mean reward so far -56.20\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 380\n",
      "Episode ended after 406 steps\n",
      "Accumulated reward in this episode -23.501349805337455\n",
      "Mean reward so far -56.12\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 381\n",
      "Episode ended after 357 steps\n",
      "Accumulated reward in this episode -49.43879575851906\n",
      "Mean reward so far -56.10\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 382\n",
      "Episode ended after 544 steps\n",
      "Accumulated reward in this episode 230.15324381546688\n",
      "Mean reward so far -55.35\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 383\n",
      "Episode ended after 494 steps\n",
      "Accumulated reward in this episode 227.578777166032\n",
      "Mean reward so far -54.61\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 384\n",
      "Episode ended after 245 steps\n",
      "Accumulated reward in this episode -15.552696547108958\n",
      "Mean reward so far -54.51\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 385\n",
      "Episode ended after 454 steps\n",
      "Accumulated reward in this episode 215.99578127730842\n",
      "Mean reward so far -53.81\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 386\n",
      "Episode ended after 141 steps\n",
      "Accumulated reward in this episode -19.42169528069134\n",
      "Mean reward so far -53.72\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 387\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 26.71185076694114\n",
      "Mean reward so far -53.52\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 388\n",
      "Episode ended after 218 steps\n",
      "Accumulated reward in this episode -9.06921629661285\n",
      "Mean reward so far -53.40\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 389\n",
      "Episode ended after 206 steps\n",
      "Accumulated reward in this episode -63.30638882463427\n",
      "Mean reward so far -53.43\n",
      "Maximal reward so far 293.2518340927227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 390\n",
      "Episode ended after 213 steps\n",
      "Accumulated reward in this episode 281.12675878202015\n",
      "Mean reward so far -52.57\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 391\n",
      "Episode ended after 412 steps\n",
      "Accumulated reward in this episode 243.19785851541354\n",
      "Mean reward so far -51.82\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 392\n",
      "Episode ended after 187 steps\n",
      "Accumulated reward in this episode -20.960770936760973\n",
      "Mean reward so far -51.74\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 393\n",
      "Episode ended after 196 steps\n",
      "Accumulated reward in this episode -35.26331362235719\n",
      "Mean reward so far -51.70\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 394\n",
      "Episode ended after 329 steps\n",
      "Accumulated reward in this episode 275.7491696029209\n",
      "Mean reward so far -50.87\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 395\n",
      "Episode ended after 250 steps\n",
      "Accumulated reward in this episode -79.27985684514177\n",
      "Mean reward so far -50.94\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 396\n",
      "Episode ended after 146 steps\n",
      "Accumulated reward in this episode -31.368002941754263\n",
      "Mean reward so far -50.89\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 397\n",
      "Episode ended after 170 steps\n",
      "Accumulated reward in this episode -3.8540365920113153\n",
      "Mean reward so far -50.77\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 398\n",
      "Episode ended after 132 steps\n",
      "Accumulated reward in this episode -0.1722191315204924\n",
      "Mean reward so far -50.65\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 399\n",
      "Episode ended after 138 steps\n",
      "Accumulated reward in this episode 2.8466632930612157\n",
      "Mean reward so far -50.51\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 400\n",
      "Episode ended after 249 steps\n",
      "Accumulated reward in this episode 247.55291770083505\n",
      "Mean reward so far -49.77\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 401\n",
      "Episode ended after 126 steps\n",
      "Accumulated reward in this episode 28.56599090795035\n",
      "Mean reward so far -49.57\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 402\n",
      "Episode ended after 126 steps\n",
      "Accumulated reward in this episode -0.8633012957323132\n",
      "Mean reward so far -49.45\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 403\n",
      "Episode ended after 128 steps\n",
      "Accumulated reward in this episode 8.37724875965435\n",
      "Mean reward so far -49.31\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 404\n",
      "Episode ended after 127 steps\n",
      "Accumulated reward in this episode 25.618480166671446\n",
      "Mean reward so far -49.12\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 405\n",
      "Episode ended after 176 steps\n",
      "Accumulated reward in this episode 5.8961921158851\n",
      "Mean reward so far -48.99\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 406\n",
      "Episode ended after 308 steps\n",
      "Accumulated reward in this episode 254.92553991598658\n",
      "Mean reward so far -48.24\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 407\n",
      "Episode ended after 435 steps\n",
      "Accumulated reward in this episode 181.7629683840078\n",
      "Mean reward so far -47.68\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 408\n",
      "Episode ended after 131 steps\n",
      "Accumulated reward in this episode 0.7352671447758539\n",
      "Mean reward so far -47.56\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 409\n",
      "Episode ended after 316 steps\n",
      "Accumulated reward in this episode 257.3951466296607\n",
      "Mean reward so far -46.82\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 410\n",
      "Episode ended after 120 steps\n",
      "Accumulated reward in this episode -12.801210212184088\n",
      "Mean reward so far -46.73\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 411\n",
      "Episode ended after 519 steps\n",
      "Accumulated reward in this episode 280.41064789078155\n",
      "Mean reward so far -45.94\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 412\n",
      "Episode ended after 185 steps\n",
      "Accumulated reward in this episode -36.289809426123725\n",
      "Mean reward so far -45.92\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 413\n",
      "Episode ended after 552 steps\n",
      "Accumulated reward in this episode 265.1559497056152\n",
      "Mean reward so far -45.16\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 414\n",
      "Episode ended after 145 steps\n",
      "Accumulated reward in this episode 74.09084730499369\n",
      "Mean reward so far -44.88\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 415\n",
      "Episode ended after 152 steps\n",
      "Accumulated reward in this episode -83.49925220519516\n",
      "Mean reward so far -44.97\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 416\n",
      "Episode ended after 412 steps\n",
      "Accumulated reward in this episode 120.93147255989636\n",
      "Mean reward so far -44.57\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 417\n",
      "Episode ended after 289 steps\n",
      "Accumulated reward in this episode 159.00734334210705\n",
      "Mean reward so far -44.09\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 418\n",
      "Episode ended after 129 steps\n",
      "Accumulated reward in this episode 15.628079332884903\n",
      "Mean reward so far -43.94\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 419\n",
      "Episode ended after 471 steps\n",
      "Accumulated reward in this episode 236.89517487558345\n",
      "Mean reward so far -43.27\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 420\n",
      "Episode ended after 364 steps\n",
      "Accumulated reward in this episode -94.32185616144164\n",
      "Mean reward so far -43.40\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 421\n",
      "Episode ended after 253 steps\n",
      "Accumulated reward in this episode -82.58966488559646\n",
      "Mean reward so far -43.49\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 422\n",
      "Episode ended after 272 steps\n",
      "Accumulated reward in this episode -146.87909757207723\n",
      "Mean reward so far -43.73\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 423\n",
      "Episode ended after 382 steps\n",
      "Accumulated reward in this episode -176.74335064082072\n",
      "Mean reward so far -44.05\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 424\n",
      "Episode ended after 221 steps\n",
      "Accumulated reward in this episode -79.08168949938613\n",
      "Mean reward so far -44.13\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 425\n",
      "Episode ended after 253 steps\n",
      "Accumulated reward in this episode -60.73369685906522\n",
      "Mean reward so far -44.17\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 426\n",
      "Episode ended after 173 steps\n",
      "Accumulated reward in this episode -79.42148667852908\n",
      "Mean reward so far -44.25\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 427\n",
      "Episode ended after 337 steps\n",
      "Accumulated reward in this episode -58.45522371499014\n",
      "Mean reward so far -44.28\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 428\n",
      "Episode ended after 775 steps\n",
      "Accumulated reward in this episode -182.7845786077749\n",
      "Mean reward so far -44.61\n",
      "Maximal reward so far 293.2518340927227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 429\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -143.6815909912593\n",
      "Mean reward so far -44.84\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 430\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -28.78895702026591\n",
      "Mean reward so far -44.80\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 431\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -95.25781103688422\n",
      "Mean reward so far -44.92\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 432\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -97.55435971483575\n",
      "Mean reward so far -45.04\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 433\n",
      "Episode ended after 618 steps\n",
      "Accumulated reward in this episode 210.70421065434329\n",
      "Mean reward so far -44.45\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 434\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -112.5015638574224\n",
      "Mean reward so far -44.61\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 435\n",
      "Episode ended after 517 steps\n",
      "Accumulated reward in this episode 202.2632761218581\n",
      "Mean reward so far -44.04\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 436\n",
      "Episode ended after 938 steps\n",
      "Accumulated reward in this episode 133.19584043894696\n",
      "Mean reward so far -43.63\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 437\n",
      "Episode ended after 493 steps\n",
      "Accumulated reward in this episode 213.04469408436955\n",
      "Mean reward so far -43.05\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 438\n",
      "Episode ended after 456 steps\n",
      "Accumulated reward in this episode 222.20161401563027\n",
      "Mean reward so far -42.44\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 439\n",
      "Episode ended after 673 steps\n",
      "Accumulated reward in this episode 167.22299467890463\n",
      "Mean reward so far -41.97\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 440\n",
      "Episode ended after 335 steps\n",
      "Accumulated reward in this episode -86.24123604968719\n",
      "Mean reward so far -42.07\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 441\n",
      "Episode ended after 604 steps\n",
      "Accumulated reward in this episode 190.97056540422403\n",
      "Mean reward so far -41.54\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 442\n",
      "Episode ended after 773 steps\n",
      "Accumulated reward in this episode 132.09112256199387\n",
      "Mean reward so far -41.15\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 443\n",
      "Episode ended after 678 steps\n",
      "Accumulated reward in this episode 202.64765016850515\n",
      "Mean reward so far -40.60\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 444\n",
      "Episode ended after 835 steps\n",
      "Accumulated reward in this episode 138.62865926226763\n",
      "Mean reward so far -40.20\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 445\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 7.924579217521547\n",
      "Mean reward so far -40.09\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 446\n",
      "Episode ended after 473 steps\n",
      "Accumulated reward in this episode -88.2981909131528\n",
      "Mean reward so far -40.20\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 447\n",
      "Episode ended after 704 steps\n",
      "Accumulated reward in this episode 102.68489871171506\n",
      "Mean reward so far -39.88\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 448\n",
      "Episode ended after 671 steps\n",
      "Accumulated reward in this episode 162.01658139639778\n",
      "Mean reward so far -39.43\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 449\n",
      "Episode ended after 662 steps\n",
      "Accumulated reward in this episode -95.28369125137075\n",
      "Mean reward so far -39.55\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 450\n",
      "Episode ended after 456 steps\n",
      "Accumulated reward in this episode -47.79306911872882\n",
      "Mean reward so far -39.57\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 451\n",
      "Episode ended after 530 steps\n",
      "Accumulated reward in this episode 178.24142959700464\n",
      "Mean reward so far -39.09\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 452\n",
      "Episode ended after 612 steps\n",
      "Accumulated reward in this episode 168.6066251382668\n",
      "Mean reward so far -38.63\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 453\n",
      "Episode ended after 337 steps\n",
      "Accumulated reward in this episode -36.39295084957163\n",
      "Mean reward so far -38.62\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 454\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 31.306215598581467\n",
      "Mean reward so far -38.47\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 455\n",
      "Episode ended after 880 steps\n",
      "Accumulated reward in this episode 74.89351889065706\n",
      "Mean reward so far -38.22\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 456\n",
      "Episode ended after 645 steps\n",
      "Accumulated reward in this episode -50.920446917148965\n",
      "Mean reward so far -38.25\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 457\n",
      "Episode ended after 693 steps\n",
      "Accumulated reward in this episode 120.48051606415336\n",
      "Mean reward so far -37.90\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 458\n",
      "Episode ended after 468 steps\n",
      "Accumulated reward in this episode -28.505249017243116\n",
      "Mean reward so far -37.88\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 459\n",
      "Episode ended after 489 steps\n",
      "Accumulated reward in this episode -166.87318141161046\n",
      "Mean reward so far -38.16\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 460\n",
      "Episode ended after 639 steps\n",
      "Accumulated reward in this episode 225.94916938186424\n",
      "Mean reward so far -37.59\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 461\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -39.927383013649106\n",
      "Mean reward so far -37.60\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 462\n",
      "Episode ended after 899 steps\n",
      "Accumulated reward in this episode 167.26296751958336\n",
      "Mean reward so far -37.15\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 463\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -27.95311237236482\n",
      "Mean reward so far -37.13\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 464\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -82.9412406079562\n",
      "Mean reward so far -37.23\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 465\n",
      "Episode ended after 496 steps\n",
      "Accumulated reward in this episode 274.07411277921994\n",
      "Mean reward so far -36.56\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 466\n",
      "Episode ended after 787 steps\n",
      "Accumulated reward in this episode -136.05137823477162\n",
      "Mean reward so far -36.78\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 467\n",
      "Episode ended after 492 steps\n",
      "Accumulated reward in this episode 224.49235901235875\n",
      "Mean reward so far -36.22\n",
      "Maximal reward so far 293.2518340927227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 468\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -51.374973524132024\n",
      "Mean reward so far -36.25\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 469\n",
      "Episode ended after 822 steps\n",
      "Accumulated reward in this episode 146.53310498318905\n",
      "Mean reward so far -35.86\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 470\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 0.958519089652448\n",
      "Mean reward so far -35.78\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 471\n",
      "Episode ended after 155 steps\n",
      "Accumulated reward in this episode 63.207242212102166\n",
      "Mean reward so far -35.57\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 472\n",
      "Episode ended after 614 steps\n",
      "Accumulated reward in this episode 196.88756599419165\n",
      "Mean reward so far -35.08\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 473\n",
      "Episode ended after 786 steps\n",
      "Accumulated reward in this episode 158.69204383857243\n",
      "Mean reward so far -34.67\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 474\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 16.420574261260512\n",
      "Mean reward so far -34.57\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 475\n",
      "Episode ended after 621 steps\n",
      "Accumulated reward in this episode 230.61495009953904\n",
      "Mean reward so far -34.01\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 476\n",
      "Episode ended after 416 steps\n",
      "Accumulated reward in this episode 256.4710395064141\n",
      "Mean reward so far -33.40\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 477\n",
      "Episode ended after 305 steps\n",
      "Accumulated reward in this episode 271.73699107032536\n",
      "Mean reward so far -32.76\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 478\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode 20.289377203864383\n",
      "Mean reward so far -32.65\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 479\n",
      "Episode ended after 369 steps\n",
      "Accumulated reward in this episode 182.00153642834175\n",
      "Mean reward so far -32.20\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 480\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -22.337323672941594\n",
      "Mean reward so far -32.18\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 481\n",
      "Episode ended after 928 steps\n",
      "Accumulated reward in this episode 120.63632587948425\n",
      "Mean reward so far -31.87\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 482\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -46.14972016600271\n",
      "Mean reward so far -31.90\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 483\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -35.1785476140934\n",
      "Mean reward so far -31.90\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 484\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -44.990945407180945\n",
      "Mean reward so far -31.93\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 485\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -35.93976057098113\n",
      "Mean reward so far -31.94\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 486\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -76.56013235258328\n",
      "Mean reward so far -32.03\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 487\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -26.82656809656818\n",
      "Mean reward so far -32.02\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 488\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -33.07976260053276\n",
      "Mean reward so far -32.02\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 489\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -60.090170208454424\n",
      "Mean reward so far -32.08\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 490\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -29.690036445992632\n",
      "Mean reward so far -32.07\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 491\n",
      "Episode ended after 973 steps\n",
      "Accumulated reward in this episode 155.20702930860296\n",
      "Mean reward so far -31.69\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 492\n",
      "Episode ended after 965 steps\n",
      "Accumulated reward in this episode 127.17784770188624\n",
      "Mean reward so far -31.37\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 493\n",
      "Episode ended after 673 steps\n",
      "Accumulated reward in this episode 174.32198109962275\n",
      "Mean reward so far -30.95\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 494\n",
      "Episode ended after 699 steps\n",
      "Accumulated reward in this episode 196.0575913066504\n",
      "Mean reward so far -30.50\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 495\n",
      "Episode ended after 459 steps\n",
      "Accumulated reward in this episode 233.14273406165626\n",
      "Mean reward so far -29.96\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 496\n",
      "Episode ended after 394 steps\n",
      "Accumulated reward in this episode 210.46469654845555\n",
      "Mean reward so far -29.48\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 497\n",
      "Episode ended after 230 steps\n",
      "Accumulated reward in this episode 50.85770910308315\n",
      "Mean reward so far -29.32\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 498\n",
      "Episode ended after 571 steps\n",
      "Accumulated reward in this episode 156.84254903025573\n",
      "Mean reward so far -28.95\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 499\n",
      "Episode ended after 374 steps\n",
      "Accumulated reward in this episode 281.91197417391646\n",
      "Mean reward so far -28.32\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 500\n",
      "Episode ended after 223 steps\n",
      "Accumulated reward in this episode 50.841415050660906\n",
      "Mean reward so far -28.17\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 501\n",
      "Episode ended after 338 steps\n",
      "Accumulated reward in this episode 257.80779933028947\n",
      "Mean reward so far -27.60\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 502\n",
      "Episode ended after 311 steps\n",
      "Accumulated reward in this episode 246.6862719195663\n",
      "Mean reward so far -27.05\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 503\n",
      "Episode ended after 307 steps\n",
      "Accumulated reward in this episode 270.3522221234905\n",
      "Mean reward so far -26.46\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 504\n",
      "Episode ended after 272 steps\n",
      "Accumulated reward in this episode 272.4414109963725\n",
      "Mean reward so far -25.87\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 505\n",
      "Episode ended after 179 steps\n",
      "Accumulated reward in this episode 39.55061243180797\n",
      "Mean reward so far -25.74\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 506\n",
      "Episode ended after 248 steps\n",
      "Accumulated reward in this episode 254.82675570435222\n",
      "Mean reward so far -25.19\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 507\n",
      "Episode ended after 166 steps\n",
      "Accumulated reward in this episode -5.66101358193707\n",
      "Mean reward so far -25.15\n",
      "Maximal reward so far 293.2518340927227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 508\n",
      "Episode ended after 227 steps\n",
      "Accumulated reward in this episode 33.83630655388024\n",
      "Mean reward so far -25.03\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 509\n",
      "Episode ended after 302 steps\n",
      "Accumulated reward in this episode 280.4526757960632\n",
      "Mean reward so far -24.43\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 510\n",
      "Episode ended after 329 steps\n",
      "Accumulated reward in this episode 238.68042884575829\n",
      "Mean reward so far -23.92\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 511\n",
      "Episode ended after 377 steps\n",
      "Accumulated reward in this episode 282.6763948147477\n",
      "Mean reward so far -23.32\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 512\n",
      "Episode ended after 359 steps\n",
      "Accumulated reward in this episode 268.44337308830825\n",
      "Mean reward so far -22.75\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 513\n",
      "Episode ended after 459 steps\n",
      "Accumulated reward in this episode 273.02845669525175\n",
      "Mean reward so far -22.18\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 514\n",
      "Episode ended after 391 steps\n",
      "Accumulated reward in this episode 244.2299917698207\n",
      "Mean reward so far -21.66\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 515\n",
      "Episode ended after 381 steps\n",
      "Accumulated reward in this episode 291.11443410629926\n",
      "Mean reward so far -21.05\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 516\n",
      "Episode ended after 329 steps\n",
      "Accumulated reward in this episode 265.3433676570103\n",
      "Mean reward so far -20.50\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 517\n",
      "Episode ended after 319 steps\n",
      "Accumulated reward in this episode -19.767298487713205\n",
      "Mean reward so far -20.50\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 518\n",
      "Episode ended after 484 steps\n",
      "Accumulated reward in this episode 196.51204880391222\n",
      "Mean reward so far -20.08\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 519\n",
      "Episode ended after 288 steps\n",
      "Accumulated reward in this episode 251.77290344595872\n",
      "Mean reward so far -19.56\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 520\n",
      "Episode ended after 557 steps\n",
      "Accumulated reward in this episode 220.17433172630052\n",
      "Mean reward so far -19.10\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 521\n",
      "Episode ended after 596 steps\n",
      "Accumulated reward in this episode 181.59959421975026\n",
      "Mean reward so far -18.71\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 522\n",
      "Episode ended after 247 steps\n",
      "Accumulated reward in this episode -18.06944214647872\n",
      "Mean reward so far -18.71\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 523\n",
      "Episode ended after 403 steps\n",
      "Accumulated reward in this episode 219.08958972105597\n",
      "Mean reward so far -18.26\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 524\n",
      "Episode ended after 382 steps\n",
      "Accumulated reward in this episode 227.64293124735434\n",
      "Mean reward so far -17.79\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 525\n",
      "Episode ended after 424 steps\n",
      "Accumulated reward in this episode 220.49287657591327\n",
      "Mean reward so far -17.33\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 526\n",
      "Episode ended after 513 steps\n",
      "Accumulated reward in this episode -141.89019524368967\n",
      "Mean reward so far -17.57\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 527\n",
      "Episode ended after 594 steps\n",
      "Accumulated reward in this episode 192.43566913963792\n",
      "Mean reward so far -17.17\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 528\n",
      "Episode ended after 384 steps\n",
      "Accumulated reward in this episode 256.06266572688213\n",
      "Mean reward so far -16.66\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 529\n",
      "Episode ended after 635 steps\n",
      "Accumulated reward in this episode 223.7567475071416\n",
      "Mean reward so far -16.20\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 530\n",
      "Episode ended after 433 steps\n",
      "Accumulated reward in this episode 226.58443636004478\n",
      "Mean reward so far -15.75\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 531\n",
      "Episode ended after 367 steps\n",
      "Accumulated reward in this episode 268.8453776369348\n",
      "Mean reward so far -15.21\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 532\n",
      "Episode ended after 371 steps\n",
      "Accumulated reward in this episode 285.6082982715034\n",
      "Mean reward so far -14.65\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 533\n",
      "Episode ended after 303 steps\n",
      "Accumulated reward in this episode 255.94270681387903\n",
      "Mean reward so far -14.14\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 534\n",
      "Episode ended after 446 steps\n",
      "Accumulated reward in this episode 223.64476602069738\n",
      "Mean reward so far -13.70\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 535\n",
      "Episode ended after 244 steps\n",
      "Accumulated reward in this episode 10.055345909789828\n",
      "Mean reward so far -13.65\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 536\n",
      "Episode ended after 317 steps\n",
      "Accumulated reward in this episode 244.3308524165004\n",
      "Mean reward so far -13.17\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 537\n",
      "Episode ended after 364 steps\n",
      "Accumulated reward in this episode 227.2667382816237\n",
      "Mean reward so far -12.72\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 538\n",
      "Episode ended after 406 steps\n",
      "Accumulated reward in this episode 250.54332378300325\n",
      "Mean reward so far -12.24\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 539\n",
      "Episode ended after 324 steps\n",
      "Accumulated reward in this episode 220.85999755683406\n",
      "Mean reward so far -11.80\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 540\n",
      "Episode ended after 267 steps\n",
      "Accumulated reward in this episode -86.47008952713342\n",
      "Mean reward so far -11.94\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 541\n",
      "Episode ended after 265 steps\n",
      "Accumulated reward in this episode 253.73587076380448\n",
      "Mean reward so far -11.45\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 542\n",
      "Episode ended after 613 steps\n",
      "Accumulated reward in this episode 160.06346970705977\n",
      "Mean reward so far -11.14\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 543\n",
      "Episode ended after 322 steps\n",
      "Accumulated reward in this episode 238.74774467904794\n",
      "Mean reward so far -10.68\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 544\n",
      "Episode ended after 349 steps\n",
      "Accumulated reward in this episode 281.9514790130037\n",
      "Mean reward so far -10.14\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 545\n",
      "Episode ended after 281 steps\n",
      "Accumulated reward in this episode 260.5416831874378\n",
      "Mean reward so far -9.64\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 546\n",
      "Episode ended after 305 steps\n",
      "Accumulated reward in this episode 37.89733247800616\n",
      "Mean reward so far -9.56\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 547\n",
      "Episode ended after 153 steps\n",
      "Accumulated reward in this episode 5.123041024411291\n",
      "Mean reward so far -9.53\n",
      "Maximal reward so far 293.2518340927227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 548\n",
      "Episode ended after 134 steps\n",
      "Accumulated reward in this episode 14.950533581675842\n",
      "Mean reward so far -9.49\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 549\n",
      "Episode ended after 290 steps\n",
      "Accumulated reward in this episode 268.0936673907747\n",
      "Mean reward so far -8.98\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 550\n",
      "Episode ended after 151 steps\n",
      "Accumulated reward in this episode 66.93219019304662\n",
      "Mean reward so far -8.84\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 551\n",
      "Episode ended after 460 steps\n",
      "Accumulated reward in this episode 219.42835197190868\n",
      "Mean reward so far -8.43\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 552\n",
      "Episode ended after 321 steps\n",
      "Accumulated reward in this episode 256.01295108270114\n",
      "Mean reward so far -7.95\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 553\n",
      "Episode ended after 402 steps\n",
      "Accumulated reward in this episode 264.0699682444131\n",
      "Mean reward so far -7.46\n",
      "Maximal reward so far 293.2518340927227\n",
      "-------------------------------------------------\n",
      "Episode 554\n",
      "Episode ended after 302 steps\n",
      "Accumulated reward in this episode 293.4370585481139\n",
      "Mean reward so far -6.92\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 555\n",
      "Episode ended after 206 steps\n",
      "Accumulated reward in this episode 52.8580778248769\n",
      "Mean reward so far -6.81\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 556\n",
      "Episode ended after 335 steps\n",
      "Accumulated reward in this episode 271.204240019932\n",
      "Mean reward so far -6.31\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 557\n",
      "Episode ended after 277 steps\n",
      "Accumulated reward in this episode 12.437868855561938\n",
      "Mean reward so far -6.28\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 558\n",
      "Episode ended after 410 steps\n",
      "Accumulated reward in this episode 241.61591039983858\n",
      "Mean reward so far -5.83\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 559\n",
      "Episode ended after 371 steps\n",
      "Accumulated reward in this episode 251.24641411850433\n",
      "Mean reward so far -5.37\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 560\n",
      "Episode ended after 370 steps\n",
      "Accumulated reward in this episode 275.5774471089839\n",
      "Mean reward so far -4.87\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 561\n",
      "Episode ended after 448 steps\n",
      "Accumulated reward in this episode 280.2574618995053\n",
      "Mean reward so far -4.37\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 562\n",
      "Episode ended after 558 steps\n",
      "Accumulated reward in this episode 207.51941927554464\n",
      "Mean reward so far -3.99\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 563\n",
      "Episode ended after 209 steps\n",
      "Accumulated reward in this episode 48.68090865180592\n",
      "Mean reward so far -3.90\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 564\n",
      "Episode ended after 467 steps\n",
      "Accumulated reward in this episode 201.3235201899261\n",
      "Mean reward so far -3.53\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 565\n",
      "Episode ended after 398 steps\n",
      "Accumulated reward in this episode 245.0782180982834\n",
      "Mean reward so far -3.09\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 566\n",
      "Episode ended after 296 steps\n",
      "Accumulated reward in this episode 11.92283073809429\n",
      "Mean reward so far -3.07\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 567\n",
      "Episode ended after 443 steps\n",
      "Accumulated reward in this episode 143.65834083044922\n",
      "Mean reward so far -2.81\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 568\n",
      "Episode ended after 502 steps\n",
      "Accumulated reward in this episode 195.3574714761402\n",
      "Mean reward so far -2.46\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 569\n",
      "Episode ended after 370 steps\n",
      "Accumulated reward in this episode -157.67693493075564\n",
      "Mean reward so far -2.73\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 570\n",
      "Episode ended after 263 steps\n",
      "Accumulated reward in this episode -4.310232499336934\n",
      "Mean reward so far -2.74\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 571\n",
      "Episode ended after 415 steps\n",
      "Accumulated reward in this episode 234.93517933477642\n",
      "Mean reward so far -2.32\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 572\n",
      "Episode ended after 349 steps\n",
      "Accumulated reward in this episode 181.15605694106435\n",
      "Mean reward so far -2.00\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 573\n",
      "Episode ended after 412 steps\n",
      "Accumulated reward in this episode 248.44792279535562\n",
      "Mean reward so far -1.56\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 574\n",
      "Episode ended after 280 steps\n",
      "Accumulated reward in this episode 257.02684927706514\n",
      "Mean reward so far -1.11\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 575\n",
      "Episode ended after 275 steps\n",
      "Accumulated reward in this episode -10.435504022647457\n",
      "Mean reward so far -1.13\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 576\n",
      "Episode ended after 270 steps\n",
      "Accumulated reward in this episode -39.66900068462864\n",
      "Mean reward so far -1.20\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 577\n",
      "Episode ended after 175 steps\n",
      "Accumulated reward in this episode -33.649545463834514\n",
      "Mean reward so far -1.25\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 578\n",
      "Episode ended after 387 steps\n",
      "Accumulated reward in this episode 240.26424736174155\n",
      "Mean reward so far -0.84\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 579\n",
      "Episode ended after 489 steps\n",
      "Accumulated reward in this episode -64.70054351970361\n",
      "Mean reward so far -0.95\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 580\n",
      "Episode ended after 196 steps\n",
      "Accumulated reward in this episode -25.660017551855802\n",
      "Mean reward so far -0.99\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 581\n",
      "Episode ended after 429 steps\n",
      "Accumulated reward in this episode 217.02968124970045\n",
      "Mean reward so far -0.61\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 582\n",
      "Episode ended after 387 steps\n",
      "Accumulated reward in this episode 197.40373310308348\n",
      "Mean reward so far -0.28\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 583\n",
      "Episode ended after 611 steps\n",
      "Accumulated reward in this episode 118.43966021747396\n",
      "Mean reward so far -0.07\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 584\n",
      "Episode ended after 397 steps\n",
      "Accumulated reward in this episode -26.693075288363126\n",
      "Mean reward so far -0.12\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 585\n",
      "Episode ended after 157 steps\n",
      "Accumulated reward in this episode -14.879004212231209\n",
      "Mean reward so far -0.14\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 586\n",
      "Episode ended after 454 steps\n",
      "Accumulated reward in this episode 2.442460745182771\n",
      "Mean reward so far -0.14\n",
      "Maximal reward so far 293.4370585481139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 587\n",
      "Episode ended after 407 steps\n",
      "Accumulated reward in this episode 205.71684327209707\n",
      "Mean reward so far 0.21\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 588\n",
      "Episode ended after 294 steps\n",
      "Accumulated reward in this episode -39.70708557055713\n",
      "Mean reward so far 0.14\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 589\n",
      "Episode ended after 203 steps\n",
      "Accumulated reward in this episode -22.40184286407873\n",
      "Mean reward so far 0.11\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 590\n",
      "Episode ended after 278 steps\n",
      "Accumulated reward in this episode -7.871384966040978\n",
      "Mean reward so far 0.09\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 591\n",
      "Episode ended after 266 steps\n",
      "Accumulated reward in this episode 239.0041707006378\n",
      "Mean reward so far 0.50\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 592\n",
      "Episode ended after 215 steps\n",
      "Accumulated reward in this episode -54.29189491959153\n",
      "Mean reward so far 0.40\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 593\n",
      "Episode ended after 245 steps\n",
      "Accumulated reward in this episode -76.21824371372412\n",
      "Mean reward so far 0.27\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 594\n",
      "Episode ended after 196 steps\n",
      "Accumulated reward in this episode -24.624140526065645\n",
      "Mean reward so far 0.23\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 595\n",
      "Episode ended after 341 steps\n",
      "Accumulated reward in this episode 169.35134926843014\n",
      "Mean reward so far 0.52\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 596\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode -61.65303181292539\n",
      "Mean reward so far 0.41\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 597\n",
      "Episode ended after 350 steps\n",
      "Accumulated reward in this episode 188.83974768373707\n",
      "Mean reward so far 0.73\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 598\n",
      "Episode ended after 211 steps\n",
      "Accumulated reward in this episode -45.267087980616644\n",
      "Mean reward so far 0.65\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 599\n",
      "Episode ended after 252 steps\n",
      "Accumulated reward in this episode -130.36766399187673\n",
      "Mean reward so far 0.43\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 600\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode -56.31525078752843\n",
      "Mean reward so far 0.34\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 601\n",
      "Episode ended after 166 steps\n",
      "Accumulated reward in this episode -34.5377610022212\n",
      "Mean reward so far 0.28\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 602\n",
      "Episode ended after 151 steps\n",
      "Accumulated reward in this episode -57.332694126883695\n",
      "Mean reward so far 0.18\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 603\n",
      "Episode ended after 313 steps\n",
      "Accumulated reward in this episode 201.03018527122782\n",
      "Mean reward so far 0.52\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 604\n",
      "Episode ended after 196 steps\n",
      "Accumulated reward in this episode -77.86109548954953\n",
      "Mean reward so far 0.39\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 605\n",
      "Episode ended after 334 steps\n",
      "Accumulated reward in this episode -117.86985773911695\n",
      "Mean reward so far 0.19\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 606\n",
      "Episode ended after 192 steps\n",
      "Accumulated reward in this episode -94.03298415783614\n",
      "Mean reward so far 0.04\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 607\n",
      "Episode ended after 228 steps\n",
      "Accumulated reward in this episode -55.32796893867718\n",
      "Mean reward so far -0.05\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 608\n",
      "Episode ended after 184 steps\n",
      "Accumulated reward in this episode -81.44915494388249\n",
      "Mean reward so far -0.19\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 609\n",
      "Episode ended after 114 steps\n",
      "Accumulated reward in this episode -84.86199612174262\n",
      "Mean reward so far -0.33\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 610\n",
      "Episode ended after 167 steps\n",
      "Accumulated reward in this episode -70.31880684038512\n",
      "Mean reward so far -0.44\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 611\n",
      "Episode ended after 152 steps\n",
      "Accumulated reward in this episode -133.70336030054472\n",
      "Mean reward so far -0.66\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 612\n",
      "Episode ended after 163 steps\n",
      "Accumulated reward in this episode -25.15643183091467\n",
      "Mean reward so far -0.70\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 613\n",
      "Episode ended after 125 steps\n",
      "Accumulated reward in this episode -27.25416983031532\n",
      "Mean reward so far -0.74\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 614\n",
      "Episode ended after 232 steps\n",
      "Accumulated reward in this episode -15.63917088787813\n",
      "Mean reward so far -0.77\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 615\n",
      "Episode ended after 151 steps\n",
      "Accumulated reward in this episode -59.676942604993755\n",
      "Mean reward so far -0.86\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 616\n",
      "Episode ended after 137 steps\n",
      "Accumulated reward in this episode -25.076446160842465\n",
      "Mean reward so far -0.90\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 617\n",
      "Episode ended after 283 steps\n",
      "Accumulated reward in this episode -185.6686071428715\n",
      "Mean reward so far -1.20\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 618\n",
      "Episode ended after 223 steps\n",
      "Accumulated reward in this episode -32.877871966881074\n",
      "Mean reward so far -1.25\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 619\n",
      "Episode ended after 492 steps\n",
      "Accumulated reward in this episode 190.62892552094843\n",
      "Mean reward so far -0.94\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 620\n",
      "Episode ended after 732 steps\n",
      "Accumulated reward in this episode 165.25328852770107\n",
      "Mean reward so far -0.67\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 621\n",
      "Episode ended after 175 steps\n",
      "Accumulated reward in this episode 6.988748993494973\n",
      "Mean reward so far -0.66\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 622\n",
      "Episode ended after 591 steps\n",
      "Accumulated reward in this episode 147.1671157950507\n",
      "Mean reward so far -0.42\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 623\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -3.335475782147803\n",
      "Mean reward so far -0.43\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 624\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -15.087044923910497\n",
      "Mean reward so far -0.45\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 625\n",
      "Episode ended after 711 steps\n",
      "Accumulated reward in this episode 166.7032215093505\n",
      "Mean reward so far -0.19\n",
      "Maximal reward so far 293.4370585481139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 626\n",
      "Episode ended after 222 steps\n",
      "Accumulated reward in this episode -37.33528533734754\n",
      "Mean reward so far -0.24\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 627\n",
      "Episode ended after 428 steps\n",
      "Accumulated reward in this episode -60.45075998275932\n",
      "Mean reward so far -0.34\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 628\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -40.64708381043543\n",
      "Mean reward so far -0.40\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 629\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -6.112057782225975\n",
      "Mean reward so far -0.41\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 630\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -14.233511012989524\n",
      "Mean reward so far -0.44\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 631\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -80.01118550042699\n",
      "Mean reward so far -0.56\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 632\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -45.75663560575641\n",
      "Mean reward so far -0.63\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 633\n",
      "Episode ended after 675 steps\n",
      "Accumulated reward in this episode -85.04517892346232\n",
      "Mean reward so far -0.77\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 634\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -65.63509371626772\n",
      "Mean reward so far -0.87\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 635\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -157.88507060784931\n",
      "Mean reward so far -1.12\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 636\n",
      "Episode ended after 888 steps\n",
      "Accumulated reward in this episode 146.50548078570938\n",
      "Mean reward so far -0.88\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 637\n",
      "Episode ended after 964 steps\n",
      "Accumulated reward in this episode -156.81297140656918\n",
      "Mean reward so far -1.13\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 638\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -13.783647014437012\n",
      "Mean reward so far -1.15\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 639\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -10.386919693413972\n",
      "Mean reward so far -1.16\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 640\n",
      "Episode ended after 164 steps\n",
      "Accumulated reward in this episode 57.68593060543644\n",
      "Mean reward so far -1.07\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 641\n",
      "Episode ended after 733 steps\n",
      "Accumulated reward in this episode 99.43901872193902\n",
      "Mean reward so far -0.91\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 642\n",
      "Episode ended after 627 steps\n",
      "Accumulated reward in this episode 130.68042513151653\n",
      "Mean reward so far -0.71\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 643\n",
      "Episode ended after 587 steps\n",
      "Accumulated reward in this episode 173.48090026503323\n",
      "Mean reward so far -0.44\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 644\n",
      "Episode ended after 390 steps\n",
      "Accumulated reward in this episode -57.1044887153038\n",
      "Mean reward so far -0.53\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 645\n",
      "Episode ended after 316 steps\n",
      "Accumulated reward in this episode 34.8537821788446\n",
      "Mean reward so far -0.47\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 646\n",
      "Episode ended after 656 steps\n",
      "Accumulated reward in this episode 170.22102412024066\n",
      "Mean reward so far -0.21\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 647\n",
      "Episode ended after 523 steps\n",
      "Accumulated reward in this episode 219.2968780829188\n",
      "Mean reward so far 0.13\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 648\n",
      "Episode ended after 521 steps\n",
      "Accumulated reward in this episode -260.3164679318198\n",
      "Mean reward so far -0.27\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 649\n",
      "Episode ended after 341 steps\n",
      "Accumulated reward in this episode -45.070085602853936\n",
      "Mean reward so far -0.34\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 650\n",
      "Episode ended after 260 steps\n",
      "Accumulated reward in this episode -24.895777885113915\n",
      "Mean reward so far -0.38\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 651\n",
      "Episode ended after 910 steps\n",
      "Accumulated reward in this episode -206.74053474677928\n",
      "Mean reward so far -0.69\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 652\n",
      "Episode ended after 495 steps\n",
      "Accumulated reward in this episode 185.73035661777772\n",
      "Mean reward so far -0.41\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 653\n",
      "Episode ended after 495 steps\n",
      "Accumulated reward in this episode 158.32581747749623\n",
      "Mean reward so far -0.17\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 654\n",
      "Episode ended after 475 steps\n",
      "Accumulated reward in this episode 247.51149016065384\n",
      "Mean reward so far 0.21\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 655\n",
      "Episode ended after 323 steps\n",
      "Accumulated reward in this episode -67.14616573379652\n",
      "Mean reward so far 0.11\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 656\n",
      "Episode ended after 501 steps\n",
      "Accumulated reward in this episode -92.7170270785353\n",
      "Mean reward so far -0.03\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 657\n",
      "Episode ended after 380 steps\n",
      "Accumulated reward in this episode -39.49030650441901\n",
      "Mean reward so far -0.09\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 658\n",
      "Episode ended after 709 steps\n",
      "Accumulated reward in this episode 173.68629554823423\n",
      "Mean reward so far 0.17\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 659\n",
      "Episode ended after 380 steps\n",
      "Accumulated reward in this episode 245.61396909084743\n",
      "Mean reward so far 0.54\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 660\n",
      "Episode ended after 326 steps\n",
      "Accumulated reward in this episode 259.42181745071565\n",
      "Mean reward so far 0.94\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 661\n",
      "Episode ended after 346 steps\n",
      "Accumulated reward in this episode -15.748038948550992\n",
      "Mean reward so far 0.91\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 662\n",
      "Episode ended after 335 steps\n",
      "Accumulated reward in this episode -47.26926519302645\n",
      "Mean reward so far 0.84\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 663\n",
      "Episode ended after 393 steps\n",
      "Accumulated reward in this episode -66.19386253409445\n",
      "Mean reward so far 0.74\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 664\n",
      "Episode ended after 523 steps\n",
      "Accumulated reward in this episode 211.59105973581484\n",
      "Mean reward so far 1.05\n",
      "Maximal reward so far 293.4370585481139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 665\n",
      "Episode ended after 333 steps\n",
      "Accumulated reward in this episode 278.4535200678369\n",
      "Mean reward so far 1.47\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 666\n",
      "Episode ended after 399 steps\n",
      "Accumulated reward in this episode 214.61446819731947\n",
      "Mean reward so far 1.79\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 667\n",
      "Episode ended after 706 steps\n",
      "Accumulated reward in this episode 155.07713528466937\n",
      "Mean reward so far 2.02\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 668\n",
      "Episode ended after 702 steps\n",
      "Accumulated reward in this episode 160.37640498346772\n",
      "Mean reward so far 2.26\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 669\n",
      "Episode ended after 632 steps\n",
      "Accumulated reward in this episode 206.88189153931788\n",
      "Mean reward so far 2.56\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 670\n",
      "Episode ended after 844 steps\n",
      "Accumulated reward in this episode 200.8392108323717\n",
      "Mean reward so far 2.86\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 671\n",
      "Episode ended after 585 steps\n",
      "Accumulated reward in this episode 213.12495034184005\n",
      "Mean reward so far 3.17\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 672\n",
      "Episode ended after 547 steps\n",
      "Accumulated reward in this episode 189.20858786457939\n",
      "Mean reward so far 3.45\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 673\n",
      "Episode ended after 619 steps\n",
      "Accumulated reward in this episode 221.19828919052094\n",
      "Mean reward so far 3.77\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 674\n",
      "Episode ended after 402 steps\n",
      "Accumulated reward in this episode -23.803840694940718\n",
      "Mean reward so far 3.73\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 675\n",
      "Episode ended after 345 steps\n",
      "Accumulated reward in this episode 273.33572670129195\n",
      "Mean reward so far 4.13\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 676\n",
      "Episode ended after 301 steps\n",
      "Accumulated reward in this episode 291.4006999334771\n",
      "Mean reward so far 4.55\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 677\n",
      "Episode ended after 619 steps\n",
      "Accumulated reward in this episode 196.30998784224394\n",
      "Mean reward so far 4.83\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 678\n",
      "Episode ended after 389 steps\n",
      "Accumulated reward in this episode 259.5949951720541\n",
      "Mean reward so far 5.21\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 679\n",
      "Episode ended after 374 steps\n",
      "Accumulated reward in this episode 249.40744037634636\n",
      "Mean reward so far 5.57\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 680\n",
      "Episode ended after 382 steps\n",
      "Accumulated reward in this episode 290.0250089460076\n",
      "Mean reward so far 5.99\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 681\n",
      "Episode ended after 433 steps\n",
      "Accumulated reward in this episode 237.86529331789944\n",
      "Mean reward so far 6.33\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 682\n",
      "Episode ended after 356 steps\n",
      "Accumulated reward in this episode 271.4745387267567\n",
      "Mean reward so far 6.72\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 683\n",
      "Episode ended after 412 steps\n",
      "Accumulated reward in this episode 248.82519384482893\n",
      "Mean reward so far 7.07\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 684\n",
      "Episode ended after 352 steps\n",
      "Accumulated reward in this episode 240.41469893326644\n",
      "Mean reward so far 7.41\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 685\n",
      "Episode ended after 305 steps\n",
      "Accumulated reward in this episode 224.0701399639995\n",
      "Mean reward so far 7.73\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 686\n",
      "Episode ended after 352 steps\n",
      "Accumulated reward in this episode 270.81457499404706\n",
      "Mean reward so far 8.11\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 687\n",
      "Episode ended after 152 steps\n",
      "Accumulated reward in this episode 0.6520717770217708\n",
      "Mean reward so far 8.10\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 688\n",
      "Episode ended after 204 steps\n",
      "Accumulated reward in this episode 26.940763884854398\n",
      "Mean reward so far 8.13\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 689\n",
      "Episode ended after 245 steps\n",
      "Accumulated reward in this episode 256.94161078718673\n",
      "Mean reward so far 8.49\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 690\n",
      "Episode ended after 260 steps\n",
      "Accumulated reward in this episode 259.8238520883252\n",
      "Mean reward so far 8.85\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 691\n",
      "Episode ended after 193 steps\n",
      "Accumulated reward in this episode 21.453316004127345\n",
      "Mean reward so far 8.87\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 692\n",
      "Episode ended after 184 steps\n",
      "Accumulated reward in this episode 44.58885428115127\n",
      "Mean reward so far 8.92\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 693\n",
      "Episode ended after 171 steps\n",
      "Accumulated reward in this episode 49.136200265701405\n",
      "Mean reward so far 8.98\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 694\n",
      "Episode ended after 277 steps\n",
      "Accumulated reward in this episode 228.1818477560899\n",
      "Mean reward so far 9.29\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 695\n",
      "Episode ended after 204 steps\n",
      "Accumulated reward in this episode 22.248061393211465\n",
      "Mean reward so far 9.31\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 696\n",
      "Episode ended after 276 steps\n",
      "Accumulated reward in this episode 257.97506713686664\n",
      "Mean reward so far 9.67\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 697\n",
      "Episode ended after 175 steps\n",
      "Accumulated reward in this episode -3.0543212779606463\n",
      "Mean reward so far 9.65\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 698\n",
      "Episode ended after 270 steps\n",
      "Accumulated reward in this episode 241.22193893977317\n",
      "Mean reward so far 9.98\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 699\n",
      "Episode ended after 282 steps\n",
      "Accumulated reward in this episode 233.13555873495832\n",
      "Mean reward so far 10.30\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 700\n",
      "Episode ended after 277 steps\n",
      "Accumulated reward in this episode 239.68254453059888\n",
      "Mean reward so far 10.63\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 701\n",
      "Episode ended after 359 steps\n",
      "Accumulated reward in this episode 255.14739620301924\n",
      "Mean reward so far 10.98\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 702\n",
      "Episode ended after 301 steps\n",
      "Accumulated reward in this episode 275.41183800909937\n",
      "Mean reward so far 11.35\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 703\n",
      "Episode ended after 295 steps\n",
      "Accumulated reward in this episode 287.52334221185356\n",
      "Mean reward so far 11.74\n",
      "Maximal reward so far 293.4370585481139\n",
      "-------------------------------------------------\n",
      "Episode 704\n",
      "Episode ended after 222 steps\n",
      "Accumulated reward in this episode 18.72643714946638\n",
      "Mean reward so far 11.75\n",
      "Maximal reward so far 293.4370585481139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 705\n",
      "Episode ended after 271 steps\n",
      "Accumulated reward in this episode 302.92659921315874\n",
      "Mean reward so far 12.17\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 706\n",
      "Episode ended after 311 steps\n",
      "Accumulated reward in this episode 271.86956420104536\n",
      "Mean reward so far 12.53\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 707\n",
      "Episode ended after 238 steps\n",
      "Accumulated reward in this episode 299.44548985262026\n",
      "Mean reward so far 12.94\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 708\n",
      "Episode ended after 241 steps\n",
      "Accumulated reward in this episode 25.42403805251083\n",
      "Mean reward so far 12.96\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 709\n",
      "Episode ended after 291 steps\n",
      "Accumulated reward in this episode 260.6214131412931\n",
      "Mean reward so far 13.31\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 710\n",
      "Episode ended after 192 steps\n",
      "Accumulated reward in this episode 47.90669351256588\n",
      "Mean reward so far 13.35\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 711\n",
      "Episode ended after 346 steps\n",
      "Accumulated reward in this episode 252.62903049263718\n",
      "Mean reward so far 13.69\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 712\n",
      "Episode ended after 427 steps\n",
      "Accumulated reward in this episode 246.26397945976788\n",
      "Mean reward so far 14.02\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 713\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 57.174087004263676\n",
      "Mean reward so far 14.08\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 714\n",
      "Episode ended after 353 steps\n",
      "Accumulated reward in this episode 231.51231835017055\n",
      "Mean reward so far 14.38\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 715\n",
      "Episode ended after 471 steps\n",
      "Accumulated reward in this episode 228.85511654382816\n",
      "Mean reward so far 14.68\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 716\n",
      "Episode ended after 285 steps\n",
      "Accumulated reward in this episode 259.64160177382024\n",
      "Mean reward so far 15.02\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 717\n",
      "Episode ended after 366 steps\n",
      "Accumulated reward in this episode 207.29964744233527\n",
      "Mean reward so far 15.29\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 718\n",
      "Episode ended after 293 steps\n",
      "Accumulated reward in this episode 275.7726711238613\n",
      "Mean reward so far 15.65\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 719\n",
      "Episode ended after 291 steps\n",
      "Accumulated reward in this episode 257.56295067046267\n",
      "Mean reward so far 15.99\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 720\n",
      "Episode ended after 310 steps\n",
      "Accumulated reward in this episode 254.74318681983107\n",
      "Mean reward so far 16.32\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 721\n",
      "Episode ended after 279 steps\n",
      "Accumulated reward in this episode 247.67714350905555\n",
      "Mean reward so far 16.64\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 722\n",
      "Episode ended after 278 steps\n",
      "Accumulated reward in this episode 278.7261315976785\n",
      "Mean reward so far 17.00\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 723\n",
      "Episode ended after 402 steps\n",
      "Accumulated reward in this episode 272.44514756378646\n",
      "Mean reward so far 17.35\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 724\n",
      "Episode ended after 306 steps\n",
      "Accumulated reward in this episode 244.26301480110678\n",
      "Mean reward so far 17.67\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 725\n",
      "Episode ended after 354 steps\n",
      "Accumulated reward in this episode 294.22201593649896\n",
      "Mean reward so far 18.05\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 726\n",
      "Episode ended after 372 steps\n",
      "Accumulated reward in this episode 240.27734554521396\n",
      "Mean reward so far 18.35\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 727\n",
      "Episode ended after 455 steps\n",
      "Accumulated reward in this episode 209.15705795468955\n",
      "Mean reward so far 18.62\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 728\n",
      "Episode ended after 234 steps\n",
      "Accumulated reward in this episode -18.42578506105292\n",
      "Mean reward so far 18.57\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 729\n",
      "Episode ended after 208 steps\n",
      "Accumulated reward in this episode 256.61044090265693\n",
      "Mean reward so far 18.89\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 730\n",
      "Episode ended after 213 steps\n",
      "Accumulated reward in this episode 65.53173576595738\n",
      "Mean reward so far 18.96\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 731\n",
      "Episode ended after 309 steps\n",
      "Accumulated reward in this episode 236.0798900206065\n",
      "Mean reward so far 19.25\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 732\n",
      "Episode ended after 323 steps\n",
      "Accumulated reward in this episode 208.03153087148388\n",
      "Mean reward so far 19.51\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 733\n",
      "Episode ended after 621 steps\n",
      "Accumulated reward in this episode 164.71570636899443\n",
      "Mean reward so far 19.71\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 734\n",
      "Episode ended after 521 steps\n",
      "Accumulated reward in this episode 233.50310148049647\n",
      "Mean reward so far 20.00\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 735\n",
      "Episode ended after 280 steps\n",
      "Accumulated reward in this episode -45.12748444743039\n",
      "Mean reward so far 19.91\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 736\n",
      "Episode ended after 201 steps\n",
      "Accumulated reward in this episode -57.96522216630789\n",
      "Mean reward so far 19.80\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 737\n",
      "Episode ended after 255 steps\n",
      "Accumulated reward in this episode -78.66908851303046\n",
      "Mean reward so far 19.67\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 738\n",
      "Episode ended after 331 steps\n",
      "Accumulated reward in this episode -45.70468595617612\n",
      "Mean reward so far 19.58\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 739\n",
      "Episode ended after 360 steps\n",
      "Accumulated reward in this episode 228.2184476207196\n",
      "Mean reward so far 19.86\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 740\n",
      "Episode ended after 418 steps\n",
      "Accumulated reward in this episode 212.89807872111174\n",
      "Mean reward so far 20.12\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 741\n",
      "Episode ended after 354 steps\n",
      "Accumulated reward in this episode 213.87921414939723\n",
      "Mean reward so far 20.39\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 742\n",
      "Episode ended after 318 steps\n",
      "Accumulated reward in this episode 242.96285093813867\n",
      "Mean reward so far 20.69\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 743\n",
      "Episode ended after 711 steps\n",
      "Accumulated reward in this episode 182.99141586978686\n",
      "Mean reward so far 20.90\n",
      "Maximal reward so far 302.92659921315874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 744\n",
      "Episode ended after 600 steps\n",
      "Accumulated reward in this episode 204.79911599686608\n",
      "Mean reward so far 21.15\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 745\n",
      "Episode ended after 554 steps\n",
      "Accumulated reward in this episode 194.03805462082562\n",
      "Mean reward so far 21.38\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 746\n",
      "Episode ended after 461 steps\n",
      "Accumulated reward in this episode 176.65784112185685\n",
      "Mean reward so far 21.59\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 747\n",
      "Episode ended after 263 steps\n",
      "Accumulated reward in this episode -79.75186236478885\n",
      "Mean reward so far 21.45\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 748\n",
      "Episode ended after 335 steps\n",
      "Accumulated reward in this episode 246.3391676905486\n",
      "Mean reward so far 21.75\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 749\n",
      "Episode ended after 419 steps\n",
      "Accumulated reward in this episode 197.17420389233865\n",
      "Mean reward so far 21.99\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 750\n",
      "Episode ended after 300 steps\n",
      "Accumulated reward in this episode 227.64104737128878\n",
      "Mean reward so far 22.26\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 751\n",
      "Episode ended after 513 steps\n",
      "Accumulated reward in this episode 246.95780810381746\n",
      "Mean reward so far 22.56\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 752\n",
      "Episode ended after 392 steps\n",
      "Accumulated reward in this episode 196.50932747941346\n",
      "Mean reward so far 22.79\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 753\n",
      "Episode ended after 187 steps\n",
      "Accumulated reward in this episode 2.5019192455381045\n",
      "Mean reward so far 22.77\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 754\n",
      "Episode ended after 347 steps\n",
      "Accumulated reward in this episode -22.451780718792378\n",
      "Mean reward so far 22.71\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 755\n",
      "Episode ended after 150 steps\n",
      "Accumulated reward in this episode 65.78356405947251\n",
      "Mean reward so far 22.76\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 756\n",
      "Episode ended after 475 steps\n",
      "Accumulated reward in this episode 223.50600077155775\n",
      "Mean reward so far 23.03\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 757\n",
      "Episode ended after 356 steps\n",
      "Accumulated reward in this episode 215.16052286420313\n",
      "Mean reward so far 23.28\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 758\n",
      "Episode ended after 160 steps\n",
      "Accumulated reward in this episode 5.827004559208216\n",
      "Mean reward so far 23.26\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 759\n",
      "Episode ended after 218 steps\n",
      "Accumulated reward in this episode 43.64328717313471\n",
      "Mean reward so far 23.28\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 760\n",
      "Episode ended after 740 steps\n",
      "Accumulated reward in this episode -153.51809263623454\n",
      "Mean reward so far 23.05\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 761\n",
      "Episode ended after 472 steps\n",
      "Accumulated reward in this episode 227.74057790245772\n",
      "Mean reward so far 23.32\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 762\n",
      "Episode ended after 377 steps\n",
      "Accumulated reward in this episode 225.7198242370235\n",
      "Mean reward so far 23.59\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 763\n",
      "Episode ended after 465 steps\n",
      "Accumulated reward in this episode 241.7460672695958\n",
      "Mean reward so far 23.87\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 764\n",
      "Episode ended after 378 steps\n",
      "Accumulated reward in this episode 3.254486424883126\n",
      "Mean reward so far 23.85\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 765\n",
      "Episode ended after 422 steps\n",
      "Accumulated reward in this episode 209.68355567582446\n",
      "Mean reward so far 24.09\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 766\n",
      "Episode ended after 521 steps\n",
      "Accumulated reward in this episode 217.63370472699265\n",
      "Mean reward so far 24.34\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 767\n",
      "Episode ended after 502 steps\n",
      "Accumulated reward in this episode 228.22135569159352\n",
      "Mean reward so far 24.61\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 768\n",
      "Episode ended after 549 steps\n",
      "Accumulated reward in this episode 219.15523978191246\n",
      "Mean reward so far 24.86\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 769\n",
      "Episode ended after 449 steps\n",
      "Accumulated reward in this episode 208.96497365714265\n",
      "Mean reward so far 25.10\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 770\n",
      "Episode ended after 408 steps\n",
      "Accumulated reward in this episode 209.2521645153148\n",
      "Mean reward so far 25.34\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 771\n",
      "Episode ended after 310 steps\n",
      "Accumulated reward in this episode -30.77256009768267\n",
      "Mean reward so far 25.26\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 772\n",
      "Episode ended after 163 steps\n",
      "Accumulated reward in this episode 16.246497320003332\n",
      "Mean reward so far 25.25\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 773\n",
      "Episode ended after 293 steps\n",
      "Accumulated reward in this episode -40.89213564893886\n",
      "Mean reward so far 25.17\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 774\n",
      "Episode ended after 678 steps\n",
      "Accumulated reward in this episode 140.63714702918216\n",
      "Mean reward so far 25.32\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 775\n",
      "Episode ended after 454 steps\n",
      "Accumulated reward in this episode 214.9642375167554\n",
      "Mean reward so far 25.56\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 776\n",
      "Episode ended after 328 steps\n",
      "Accumulated reward in this episode -98.49066110319191\n",
      "Mean reward so far 25.40\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 777\n",
      "Episode ended after 538 steps\n",
      "Accumulated reward in this episode 221.24727075705155\n",
      "Mean reward so far 25.65\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 778\n",
      "Episode ended after 674 steps\n",
      "Accumulated reward in this episode 192.4303652359495\n",
      "Mean reward so far 25.87\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 779\n",
      "Episode ended after 494 steps\n",
      "Accumulated reward in this episode 199.05462861973868\n",
      "Mean reward so far 26.09\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 780\n",
      "Episode ended after 537 steps\n",
      "Accumulated reward in this episode 220.52233524720282\n",
      "Mean reward so far 26.34\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 781\n",
      "Episode ended after 330 steps\n",
      "Accumulated reward in this episode 201.16091483307775\n",
      "Mean reward so far 26.56\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 782\n",
      "Episode ended after 684 steps\n",
      "Accumulated reward in this episode 146.198237913064\n",
      "Mean reward so far 26.71\n",
      "Maximal reward so far 302.92659921315874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 783\n",
      "Episode ended after 345 steps\n",
      "Accumulated reward in this episode -56.72762067403919\n",
      "Mean reward so far 26.61\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 784\n",
      "Episode ended after 389 steps\n",
      "Accumulated reward in this episode 203.34606276732148\n",
      "Mean reward so far 26.83\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 785\n",
      "Episode ended after 394 steps\n",
      "Accumulated reward in this episode 197.2906043185751\n",
      "Mean reward so far 27.05\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 786\n",
      "Episode ended after 320 steps\n",
      "Accumulated reward in this episode 234.67627603055547\n",
      "Mean reward so far 27.31\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 787\n",
      "Episode ended after 152 steps\n",
      "Accumulated reward in this episode 33.52087106907605\n",
      "Mean reward so far 27.32\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 788\n",
      "Episode ended after 378 steps\n",
      "Accumulated reward in this episode 207.28279959477035\n",
      "Mean reward so far 27.55\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 789\n",
      "Episode ended after 266 steps\n",
      "Accumulated reward in this episode -46.0530767666195\n",
      "Mean reward so far 27.46\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 790\n",
      "Episode ended after 245 steps\n",
      "Accumulated reward in this episode -31.817681383406708\n",
      "Mean reward so far 27.38\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 791\n",
      "Episode ended after 300 steps\n",
      "Accumulated reward in this episode -0.09317689277857966\n",
      "Mean reward so far 27.35\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 792\n",
      "Episode ended after 372 steps\n",
      "Accumulated reward in this episode 243.5876607725898\n",
      "Mean reward so far 27.62\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 793\n",
      "Episode ended after 391 steps\n",
      "Accumulated reward in this episode 224.31500804480095\n",
      "Mean reward so far 27.87\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 794\n",
      "Episode ended after 332 steps\n",
      "Accumulated reward in this episode 215.71126817304867\n",
      "Mean reward so far 28.10\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 795\n",
      "Episode ended after 399 steps\n",
      "Accumulated reward in this episode 279.6892155042471\n",
      "Mean reward so far 28.42\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 796\n",
      "Episode ended after 151 steps\n",
      "Accumulated reward in this episode 39.17029332683827\n",
      "Mean reward so far 28.43\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 797\n",
      "Episode ended after 382 steps\n",
      "Accumulated reward in this episode 246.0982393398077\n",
      "Mean reward so far 28.71\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 798\n",
      "Episode ended after 356 steps\n",
      "Accumulated reward in this episode 279.1875301364721\n",
      "Mean reward so far 29.02\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 799\n",
      "Episode ended after 394 steps\n",
      "Accumulated reward in this episode 203.04506233983852\n",
      "Mean reward so far 29.24\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 800\n",
      "Episode ended after 341 steps\n",
      "Accumulated reward in this episode 210.19275627907734\n",
      "Mean reward so far 29.46\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 801\n",
      "Episode ended after 361 steps\n",
      "Accumulated reward in this episode 168.14258298391476\n",
      "Mean reward so far 29.64\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 802\n",
      "Episode ended after 435 steps\n",
      "Accumulated reward in this episode 214.65778393948835\n",
      "Mean reward so far 29.87\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 803\n",
      "Episode ended after 148 steps\n",
      "Accumulated reward in this episode 59.25947759039467\n",
      "Mean reward so far 29.90\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 804\n",
      "Episode ended after 518 steps\n",
      "Accumulated reward in this episode 252.11498055669665\n",
      "Mean reward so far 30.18\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 805\n",
      "Episode ended after 402 steps\n",
      "Accumulated reward in this episode 230.6942235802382\n",
      "Mean reward so far 30.43\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 806\n",
      "Episode ended after 377 steps\n",
      "Accumulated reward in this episode 213.6212351328531\n",
      "Mean reward so far 30.65\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 807\n",
      "Episode ended after 519 steps\n",
      "Accumulated reward in this episode 157.75311368822747\n",
      "Mean reward so far 30.81\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 808\n",
      "Episode ended after 794 steps\n",
      "Accumulated reward in this episode 112.37718156224695\n",
      "Mean reward so far 30.91\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 809\n",
      "Episode ended after 387 steps\n",
      "Accumulated reward in this episode 227.18787541354632\n",
      "Mean reward so far 31.15\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 810\n",
      "Episode ended after 311 steps\n",
      "Accumulated reward in this episode 248.51336563019416\n",
      "Mean reward so far 31.42\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 811\n",
      "Episode ended after 382 steps\n",
      "Accumulated reward in this episode 256.5535661620225\n",
      "Mean reward so far 31.70\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 812\n",
      "Episode ended after 358 steps\n",
      "Accumulated reward in this episode 259.53243187564635\n",
      "Mean reward so far 31.98\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 813\n",
      "Episode ended after 365 steps\n",
      "Accumulated reward in this episode 283.42782030600983\n",
      "Mean reward so far 32.29\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 814\n",
      "Episode ended after 434 steps\n",
      "Accumulated reward in this episode -7.120705717446242\n",
      "Mean reward so far 32.24\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 815\n",
      "Episode ended after 601 steps\n",
      "Accumulated reward in this episode 214.61139945437395\n",
      "Mean reward so far 32.46\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 816\n",
      "Episode ended after 576 steps\n",
      "Accumulated reward in this episode 219.49780156079004\n",
      "Mean reward so far 32.69\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 817\n",
      "Episode ended after 517 steps\n",
      "Accumulated reward in this episode 252.99097729970856\n",
      "Mean reward so far 32.96\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 818\n",
      "Episode ended after 457 steps\n",
      "Accumulated reward in this episode -34.729521284154174\n",
      "Mean reward so far 32.88\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 819\n",
      "Episode ended after 619 steps\n",
      "Accumulated reward in this episode -39.841644655022876\n",
      "Mean reward so far 32.79\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 820\n",
      "Episode ended after 542 steps\n",
      "Accumulated reward in this episode 214.59816791974905\n",
      "Mean reward so far 33.01\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 821\n",
      "Episode ended after 1000 steps\n",
      "Accumulated reward in this episode -14.47254937054096\n",
      "Mean reward so far 32.95\n",
      "Maximal reward so far 302.92659921315874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 822\n",
      "Episode ended after 658 steps\n",
      "Accumulated reward in this episode 149.80286462387562\n",
      "Mean reward so far 33.10\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 823\n",
      "Episode ended after 757 steps\n",
      "Accumulated reward in this episode 190.75746030191232\n",
      "Mean reward so far 33.29\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 824\n",
      "Episode ended after 419 steps\n",
      "Accumulated reward in this episode 273.0251565702463\n",
      "Mean reward so far 33.58\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 825\n",
      "Episode ended after 659 steps\n",
      "Accumulated reward in this episode 216.59856815010113\n",
      "Mean reward so far 33.80\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 826\n",
      "Episode ended after 391 steps\n",
      "Accumulated reward in this episode 273.10972532775\n",
      "Mean reward so far 34.09\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 827\n",
      "Episode ended after 750 steps\n",
      "Accumulated reward in this episode 199.34834141781013\n",
      "Mean reward so far 34.29\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 828\n",
      "Episode ended after 545 steps\n",
      "Accumulated reward in this episode 190.50867190637936\n",
      "Mean reward so far 34.48\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 829\n",
      "Episode ended after 496 steps\n",
      "Accumulated reward in this episode 195.50135343688933\n",
      "Mean reward so far 34.67\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 830\n",
      "Episode ended after 490 steps\n",
      "Accumulated reward in this episode 228.19018396062117\n",
      "Mean reward so far 34.90\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 831\n",
      "Episode ended after 358 steps\n",
      "Accumulated reward in this episode 275.3860581581247\n",
      "Mean reward so far 35.19\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 832\n",
      "Episode ended after 413 steps\n",
      "Accumulated reward in this episode 219.4710152051098\n",
      "Mean reward so far 35.41\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 833\n",
      "Episode ended after 338 steps\n",
      "Accumulated reward in this episode 227.51995733994332\n",
      "Mean reward so far 35.64\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 834\n",
      "Episode ended after 585 steps\n",
      "Accumulated reward in this episode 215.71316437513926\n",
      "Mean reward so far 35.86\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 835\n",
      "Episode ended after 382 steps\n",
      "Accumulated reward in this episode 225.7909426471098\n",
      "Mean reward so far 36.09\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 836\n",
      "Episode ended after 391 steps\n",
      "Accumulated reward in this episode 252.96400257165018\n",
      "Mean reward so far 36.35\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 837\n",
      "Episode ended after 715 steps\n",
      "Accumulated reward in this episode 175.6045843376975\n",
      "Mean reward so far 36.51\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 838\n",
      "Episode ended after 307 steps\n",
      "Accumulated reward in this episode 254.30289622074503\n",
      "Mean reward so far 36.77\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 839\n",
      "Episode ended after 406 steps\n",
      "Accumulated reward in this episode 236.05790058020986\n",
      "Mean reward so far 37.01\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 840\n",
      "Episode ended after 366 steps\n",
      "Accumulated reward in this episode 277.4481616993286\n",
      "Mean reward so far 37.30\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 841\n",
      "Episode ended after 323 steps\n",
      "Accumulated reward in this episode 269.5217307330369\n",
      "Mean reward so far 37.57\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 842\n",
      "Episode ended after 403 steps\n",
      "Accumulated reward in this episode 241.43254103286517\n",
      "Mean reward so far 37.81\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 843\n",
      "Episode ended after 326 steps\n",
      "Accumulated reward in this episode 272.1433322584352\n",
      "Mean reward so far 38.09\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 844\n",
      "Episode ended after 220 steps\n",
      "Accumulated reward in this episode 55.07300893768486\n",
      "Mean reward so far 38.11\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 845\n",
      "Episode ended after 307 steps\n",
      "Accumulated reward in this episode 282.2903437275452\n",
      "Mean reward so far 38.40\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 846\n",
      "Episode ended after 327 steps\n",
      "Accumulated reward in this episode 264.23799847215673\n",
      "Mean reward so far 38.67\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 847\n",
      "Episode ended after 373 steps\n",
      "Accumulated reward in this episode 254.60126482229066\n",
      "Mean reward so far 38.92\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 848\n",
      "Episode ended after 276 steps\n",
      "Accumulated reward in this episode 44.715710875648014\n",
      "Mean reward so far 38.93\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 849\n",
      "Episode ended after 187 steps\n",
      "Accumulated reward in this episode 34.90857257836733\n",
      "Mean reward so far 38.92\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 850\n",
      "Episode ended after 343 steps\n",
      "Accumulated reward in this episode 266.632635375708\n",
      "Mean reward so far 39.19\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 851\n",
      "Episode ended after 291 steps\n",
      "Accumulated reward in this episode 259.87376518253336\n",
      "Mean reward so far 39.45\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 852\n",
      "Episode ended after 236 steps\n",
      "Accumulated reward in this episode 14.353436495987921\n",
      "Mean reward so far 39.42\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 853\n",
      "Episode ended after 289 steps\n",
      "Accumulated reward in this episode 254.37997798164184\n",
      "Mean reward so far 39.67\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 854\n",
      "Episode ended after 325 steps\n",
      "Accumulated reward in this episode 233.6222583745585\n",
      "Mean reward so far 39.90\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 855\n",
      "Episode ended after 347 steps\n",
      "Accumulated reward in this episode 231.842903864839\n",
      "Mean reward so far 40.12\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 856\n",
      "Episode ended after 359 steps\n",
      "Accumulated reward in this episode 229.1443405064419\n",
      "Mean reward so far 40.34\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 857\n",
      "Episode ended after 307 steps\n",
      "Accumulated reward in this episode 287.79671115983274\n",
      "Mean reward so far 40.63\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 858\n",
      "Episode ended after 344 steps\n",
      "Accumulated reward in this episode 261.17640409836304\n",
      "Mean reward so far 40.89\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 859\n",
      "Episode ended after 258 steps\n",
      "Accumulated reward in this episode 300.7665964100045\n",
      "Mean reward so far 41.19\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 860\n",
      "Episode ended after 312 steps\n",
      "Accumulated reward in this episode 287.1443568973488\n",
      "Mean reward so far 41.48\n",
      "Maximal reward so far 302.92659921315874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 861\n",
      "Episode ended after 376 steps\n",
      "Accumulated reward in this episode 281.3630191338599\n",
      "Mean reward so far 41.75\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 862\n",
      "Episode ended after 352 steps\n",
      "Accumulated reward in this episode 230.14678948778618\n",
      "Mean reward so far 41.97\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 863\n",
      "Episode ended after 368 steps\n",
      "Accumulated reward in this episode 237.90331871998296\n",
      "Mean reward so far 42.20\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 864\n",
      "Episode ended after 631 steps\n",
      "Accumulated reward in this episode 196.8762088566104\n",
      "Mean reward so far 42.38\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 865\n",
      "Episode ended after 303 steps\n",
      "Accumulated reward in this episode 249.58826902375245\n",
      "Mean reward so far 42.62\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 866\n",
      "Episode ended after 629 steps\n",
      "Accumulated reward in this episode 193.53944888473512\n",
      "Mean reward so far 42.79\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 867\n",
      "Episode ended after 306 steps\n",
      "Accumulated reward in this episode 254.33916055939747\n",
      "Mean reward so far 43.04\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 868\n",
      "Episode ended after 293 steps\n",
      "Accumulated reward in this episode 271.74992710647723\n",
      "Mean reward so far 43.30\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 869\n",
      "Episode ended after 253 steps\n",
      "Accumulated reward in this episode -15.195823706969044\n",
      "Mean reward so far 43.23\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 870\n",
      "Episode ended after 275 steps\n",
      "Accumulated reward in this episode 276.5495908252472\n",
      "Mean reward so far 43.50\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 871\n",
      "Episode ended after 193 steps\n",
      "Accumulated reward in this episode 5.352066810827665\n",
      "Mean reward so far 43.46\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 872\n",
      "Episode ended after 320 steps\n",
      "Accumulated reward in this episode 267.0351748710039\n",
      "Mean reward so far 43.71\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 873\n",
      "Episode ended after 319 steps\n",
      "Accumulated reward in this episode 242.42636893347702\n",
      "Mean reward so far 43.94\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 874\n",
      "Episode ended after 294 steps\n",
      "Accumulated reward in this episode 290.0127571640545\n",
      "Mean reward so far 44.22\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 875\n",
      "Episode ended after 352 steps\n",
      "Accumulated reward in this episode 265.1349886525274\n",
      "Mean reward so far 44.47\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 876\n",
      "Episode ended after 240 steps\n",
      "Accumulated reward in this episode 45.755265047289726\n",
      "Mean reward so far 44.47\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 877\n",
      "Episode ended after 315 steps\n",
      "Accumulated reward in this episode 268.5775007423267\n",
      "Mean reward so far 44.73\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 878\n",
      "Episode ended after 175 steps\n",
      "Accumulated reward in this episode 51.529035801754105\n",
      "Mean reward so far 44.74\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 879\n",
      "Episode ended after 267 steps\n",
      "Accumulated reward in this episode 281.8460303430791\n",
      "Mean reward so far 45.01\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 880\n",
      "Episode ended after 189 steps\n",
      "Accumulated reward in this episode 55.24592034979551\n",
      "Mean reward so far 45.02\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 881\n",
      "Episode ended after 227 steps\n",
      "Accumulated reward in this episode 54.43051903633747\n",
      "Mean reward so far 45.03\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 882\n",
      "Episode ended after 124 steps\n",
      "Accumulated reward in this episode 65.15919734044579\n",
      "Mean reward so far 45.05\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 883\n",
      "Episode ended after 269 steps\n",
      "Accumulated reward in this episode 272.8923273371394\n",
      "Mean reward so far 45.31\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 884\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 55.79521524596511\n",
      "Mean reward so far 45.32\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 885\n",
      "Episode ended after 166 steps\n",
      "Accumulated reward in this episode 28.07689375911888\n",
      "Mean reward so far 45.30\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 886\n",
      "Episode ended after 257 steps\n",
      "Accumulated reward in this episode 265.02976939125034\n",
      "Mean reward so far 45.55\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 887\n",
      "Episode ended after 164 steps\n",
      "Accumulated reward in this episode 6.249000367111037\n",
      "Mean reward so far 45.51\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 888\n",
      "Episode ended after 333 steps\n",
      "Accumulated reward in this episode 294.70459594320687\n",
      "Mean reward so far 45.79\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 889\n",
      "Episode ended after 190 steps\n",
      "Accumulated reward in this episode -16.337650198879828\n",
      "Mean reward so far 45.72\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 890\n",
      "Episode ended after 307 steps\n",
      "Accumulated reward in this episode 260.3633536286992\n",
      "Mean reward so far 45.96\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 891\n",
      "Episode ended after 314 steps\n",
      "Accumulated reward in this episode 293.3577673551456\n",
      "Mean reward so far 46.23\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 892\n",
      "Episode ended after 959 steps\n",
      "Accumulated reward in this episode 160.21125838125982\n",
      "Mean reward so far 46.36\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 893\n",
      "Episode ended after 253 steps\n",
      "Accumulated reward in this episode 293.59022130013034\n",
      "Mean reward so far 46.64\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 894\n",
      "Episode ended after 194 steps\n",
      "Accumulated reward in this episode 54.193728114786055\n",
      "Mean reward so far 46.65\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 895\n",
      "Episode ended after 203 steps\n",
      "Accumulated reward in this episode 29.831218277268952\n",
      "Mean reward so far 46.63\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 896\n",
      "Episode ended after 269 steps\n",
      "Accumulated reward in this episode 240.81636499590536\n",
      "Mean reward so far 46.84\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 897\n",
      "Episode ended after 237 steps\n",
      "Accumulated reward in this episode 301.4940494189591\n",
      "Mean reward so far 47.13\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 898\n",
      "Episode ended after 320 steps\n",
      "Accumulated reward in this episode 244.97512864127046\n",
      "Mean reward so far 47.35\n",
      "Maximal reward so far 302.92659921315874\n",
      "-------------------------------------------------\n",
      "Episode 899\n",
      "Episode ended after 234 steps\n",
      "Accumulated reward in this episode 259.3336391546377\n",
      "Mean reward so far 47.58\n",
      "Maximal reward so far 302.92659921315874\n"
     ]
    }
   ],
   "source": [
    "ep_action, ep_obs, ep_reward = [], [], []  # Allocate space for episode actions, observations and rewards\n",
    "tot_ep_reward = [] # Total episode reward\n",
    "mean_reward = []\n",
    "\n",
    "''' Run TF session '''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ''' Run episodes '''\n",
    "    for ep in range(num_episodes): \n",
    "        obs = env.reset()  # Reset and save first observation\n",
    "        ep_obs.append(obs) # append observation\n",
    "\n",
    "        ''' Run steps '''\n",
    "        while True:\n",
    "            # Propagate forward to compute action probability distribution\n",
    "            apd = np.squeeze(sess.run(action_prob_dist, feed_dict = {input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "            action = np.random.choice(np.arange(num_actions), p = apd)   # Sample an action based on the pdf\n",
    "            obs, reward, done, info = env.step(action)  # Take action and save observation, reward and done boolean\n",
    "            \n",
    "            # Convert action to one hot\n",
    "            action_oh = np.zeros((1,num_actions))\n",
    "            action_oh[0,action] = 1\n",
    "            \n",
    "            ep_action.append(action_oh)  # append action\n",
    "            ep_obs.append(obs)           # append observation\n",
    "            ep_reward.append(reward)     # append reward\n",
    "\n",
    "            if done: \n",
    "                # Stack vertically episode parameters to one np.array\n",
    "                ep_action = np.vstack(ep_action)\n",
    "                ep_obs = np.vstack(ep_obs)\n",
    "                ep_reward = np.hstack(ep_reward)\n",
    "\n",
    "                # Discount rewards\n",
    "                dis_rewards_arr = discount_rewards(ep_reward)\n",
    "                # Compute loss and optimize\n",
    "                sess.run([loss, training_opt],\n",
    "                         feed_dict = {input_ : ep_obs[:-1], actions : ep_action, dis_rewards : dis_rewards_arr})\n",
    "                \n",
    "                tot_ep_reward.append(np.sum(ep_reward))  # Compute total reward for episode\n",
    "                mean_reward.append(np.mean(tot_ep_reward))\n",
    "                \n",
    "                 # print info\n",
    "                print(\"-------------------------------------------------\")\n",
    "                print(\"Episode {}\".format(ep))\n",
    "                print(\"Episode ended after {} steps\".format(ep_action.shape[0]))\n",
    "                print(\"Accumulated reward in this episode {}\".format(tot_ep_reward[ep]))\n",
    "                print(\"Mean reward so far {:0.2f}\".format(np.mean(tot_ep_reward)))\n",
    "                print(\"Maximal reward so far {}\".format(np.max(tot_ep_reward)))\n",
    "                \n",
    "                ep_action, ep_obs, ep_reward = [], [], []  # Clear episode values for next episode\n",
    "                      \n",
    "                break\n",
    "                \n",
    "    saver.save(sess, \"models/LunarLander/model.ckpt\") # save model for later\n",
    "#     writer.add_graph(sess.graph) # Save graph for displaying with TensorBoard\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of how good our model is, and to see if our agent improves in the training process, we can plot the mean rewards gained in one episode over the episode number. This value would grow as our agent becomes more skilled and gains more rewards in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mean_reward)\n",
    "plt.xlabel(\"episode\", fontsize=14)\n",
    "plt.ylabel(\"mean reward\", fontsize = 14)\n",
    "plt.title(\"Mean episode rewards over episode number\", fontsize = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch the agent play an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fun part!\n",
    "Now we get to see how good our agent really is by watching it play an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/LunarLander/model.ckpt\n",
      "Game ended after 320 steps\n",
      "Total reward earned in this episode: 248.79\n"
     ]
    }
   ],
   "source": [
    "ep_reward = []\n",
    "env = gym.make('LunarLander-v2')\n",
    "env._max_episode_steps = 1000\n",
    "# env = gym.wrappers.Monitor(env, \"recording/LunarLander\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/LunarLander/model.ckpt\") # load model\n",
    "    obs = env.reset() # Reset env and save observation\n",
    "    t = 0\n",
    "    while True:\n",
    "        env.render() # Render game\n",
    "        # Use our model to create a probability distribution of actions based on observation\n",
    "        apd = np.squeeze(sess.run(action_prob_dist, feed_dict={input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "        # Choose an action out of the PDF and take action\n",
    "        action = np.random.choice(np.arange(num_actions), p = apd)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        ep_reward.append(reward)\n",
    "        t = t+1\n",
    "        if done:\n",
    "            print(\"Game ended after {} steps\".format(t+1))\n",
    "            print(\"Total reward earned in this episode: {:0.2f}\".format(sum(ep_reward)))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/LunarLander/model.ckpt\n",
      "-------------------------------------------\n",
      "Game ended after 374 steps\n",
      "Total reward earned in this episode: 256.32\n",
      "-------------------------------------------\n",
      "Game ended after 289 steps\n",
      "Total reward earned in this episode: 289.95\n",
      "-------------------------------------------\n",
      "Game ended after 265 steps\n",
      "Total reward earned in this episode: 251.29\n",
      "-------------------------------------------\n",
      "Game ended after 231 steps\n",
      "Total reward earned in this episode: 257.90\n",
      "-------------------------------------------\n",
      "Game ended after 262 steps\n",
      "Total reward earned in this episode: 266.45\n",
      "-------------------------------------------\n",
      "Game ended after 284 steps\n",
      "Total reward earned in this episode: 263.15\n",
      "-------------------------------------------\n",
      "Game ended after 258 steps\n",
      "Total reward earned in this episode: 276.48\n",
      "-------------------------------------------\n",
      "Game ended after 360 steps\n",
      "Total reward earned in this episode: 280.44\n",
      "-------------------------------------------\n",
      "Game ended after 227 steps\n",
      "Total reward earned in this episode: 20.21\n",
      "-------------------------------------------\n",
      "Game ended after 286 steps\n",
      "Total reward earned in this episode: 295.05\n",
      "===================================================\n",
      "Average reward in batch is 245.72\n"
     ]
    }
   ],
   "source": [
    "num_ep = 10\n",
    "ep_reward = []\n",
    "batch_reward = []\n",
    "env = gym.make('LunarLander-v2')\n",
    "env._max_episode_steps = 1000\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/LunarLander/model.ckpt\") # load model\n",
    "    for ep in range(num_ep):\n",
    "        obs = env.reset() # Reset env and save observation\n",
    "        t = 0\n",
    "        while True:\n",
    "            # Use our model to create a probability distribution of actions based on observation\n",
    "            apd = np.squeeze(sess.run(action_prob_dist, feed_dict={input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "            # Choose an action out of the PDF and take action\n",
    "            action = np.random.choice(np.arange(num_actions), p = apd)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            ep_reward.append(reward)\n",
    "            t = t+1\n",
    "            if done:\n",
    "                print(\"-------------------------------------------\")\n",
    "                print(\"Game ended after {} steps\".format(t+1))\n",
    "                print(\"Total reward earned in this episode: {:0.2f}\".format(sum(ep_reward)))\n",
    "                batch_reward.append(sum(ep_reward))\n",
    "                ep_reward = []\n",
    "                break\n",
    "print(\"===================================================\")\n",
    "print(\"Average reward in batch is {:0.2f}\".format(np.mean(batch_reward)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of an episode played by this agent:\n",
    "\n",
    "<img src=\"./img/LunarLander_Agent.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
