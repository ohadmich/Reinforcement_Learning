{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Cart Pole game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100      # Number of episodes for training\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99           # Discount factor for reward\n",
    "num_Hidden = 200       # number of nodes in the hidden layer\n",
    "\n",
    "env = gym.make('CartPole-v0')    # Choose a game and create an environment\n",
    "env = env.unwrapped              # The wrapper limits the number of steps in an episode to 200, let's get rid of it\n",
    "obs_dim = env.reset().shape      # obervation dimension\n",
    "num_actions = env.action_space.n # number of actions (this works only for descrete action space, which is the case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    '''Takes a 1D rewards (one episode) and discounts it and also standardize\n",
    "    the rewards to be unit normal (helps control the gradient estimator variance)'''\n",
    "    \n",
    "    # Discounting\n",
    "    dis_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_sum = gamma*running_sum + r[t]\n",
    "        dis_r[t] = running_sum\n",
    "    \n",
    "    # Normailzing\n",
    "    dis_r = (dis_r - np.mean(dis_r))/np.std(dis_r)\n",
    "        \n",
    "    return dis_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the simplest model: Observation as an input, one hidden layer and two nodes in the output layer - corresponding to the action of moving right (1) or left (0).\n",
    "\n",
    "To keep order in our model, we use name scopes which basically group the layers of our model in a simple to follow way. Eventually, when using [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) for visualizing the model, the graph is more readable and makes it easy to understand the model and find errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create placeholders for inputs'''\n",
    "# A place holder for input observations\n",
    "input_ = tf.placeholder(tf.float32, shape = (None, obs_dim[0]), name = \"input\")\n",
    "# A place holder for actions in a full episode\n",
    "actions = tf.placeholder(tf.float32, shape = (None, num_actions), name = \"actions\")\n",
    "# A place holder for discounted rewards in a full episode\n",
    "dis_rewards = tf.placeholder(tf.float32, shape = (None, ), name = \"dis_rewards\")\n",
    "\n",
    "'''Fully connected layers'''\n",
    "with tf.name_scope(\"FC1\"):\n",
    "    fc1 = tf.layers.dense(inputs = input_, units = num_Hidden ,activation = tf.nn.relu, name = \"fc1\" )\n",
    "\n",
    "with tf.name_scope(\"FC2\"):\n",
    "    fc2 = tf.layers.dense(inputs = fc1, units = num_actions ,activation = None, name = \"fc2\" )\n",
    "\n",
    "with tf.name_scope(\"Action_PDF\"):\n",
    "    # Operate with softmax on fc2 outputs to get an action probability distribution\n",
    "    action_prob_dist = tf.nn.softmax(logits = fc2, name = \"softamx\")\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    '''Define loss'''\n",
    "    # Fist define reular softmax cross entropy loss\n",
    "    CE_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = actions, logits = fc2, name = \"CE_loss\")\n",
    "    # Modulate the loss based on our discounted reward - this is where reinforcment learning happens,\n",
    "    # we favor actions that produced high reward\n",
    "    loss = tf.reduce_mean(CE_loss * dis_rewards)\n",
    "\n",
    "with tf.name_scope(\"Training\"):\n",
    "    '''Define optimizer'''\n",
    "    training_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "'''Define saver for saving and restoring model'''\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a writer for saving summaries to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.summary.FileWriter(\"./tensorboard/1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a graph visualization:\n",
    "\n",
    "<img src=\"./img/model_graph.png\" width=\"500\">\n",
    "\n",
    "Each block in the graph is expandable and let you see the content inside, for example see an [image](./img/model_graph_loss.png) with expanded loss block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model ready, we can start training it.\n",
    "\n",
    "Our goal is to achieve a model that uses the current observation to create the best probability distribution function (PDF) for the next action to be taken. This is **not a deterministic model** - the agent has a fine probability to take any action after receiving an observation, with larger probability to take favorable actions.\n",
    "\n",
    "To achieve that, in each step, we use our model to generate a PDF of actions, draw an action out of it and take the next step.\n",
    "When we reach the end of the episode, we compute the episode loss by feeding the observation, action and discounted rewards vectors (which we kept track of). Our optimizer minimizes the loss which makes favorable (unfavorable) actions more probable (less probable) due to the multiplicative factor of the discounted reward. By repeating these steps to each episode, our agent gradually improves its PDF and becomes a better player!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 0\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 18.0\n",
      "Mean reward so far 18.00\n",
      "Maximal reward so far 18.0\n",
      "-------------------------------------------------\n",
      "Episode 1\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 20.0\n",
      "Mean reward so far 19.00\n",
      "Maximal reward so far 20.0\n",
      "-------------------------------------------------\n",
      "Episode 2\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 19.0\n",
      "Mean reward so far 19.00\n",
      "Maximal reward so far 20.0\n",
      "-------------------------------------------------\n",
      "Episode 3\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 14.0\n",
      "Mean reward so far 17.75\n",
      "Maximal reward so far 20.0\n",
      "-------------------------------------------------\n",
      "Episode 4\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 13.0\n",
      "Mean reward so far 16.80\n",
      "Maximal reward so far 20.0\n",
      "-------------------------------------------------\n",
      "Episode 5\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 24.0\n",
      "Mean reward so far 18.00\n",
      "Maximal reward so far 24.0\n",
      "-------------------------------------------------\n",
      "Episode 6\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 17.0\n",
      "Mean reward so far 17.86\n",
      "Maximal reward so far 24.0\n",
      "-------------------------------------------------\n",
      "Episode 7\n",
      "Episode ended after 51 steps\n",
      "Accumulated reward in this episode 51.0\n",
      "Mean reward so far 22.00\n",
      "Maximal reward so far 51.0\n",
      "-------------------------------------------------\n",
      "Episode 8\n",
      "Episode ended after 42 steps\n",
      "Accumulated reward in this episode 42.0\n",
      "Mean reward so far 24.22\n",
      "Maximal reward so far 51.0\n",
      "-------------------------------------------------\n",
      "Episode 9\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 19.0\n",
      "Mean reward so far 23.70\n",
      "Maximal reward so far 51.0\n",
      "-------------------------------------------------\n",
      "Episode 10\n",
      "Episode ended after 27 steps\n",
      "Accumulated reward in this episode 27.0\n",
      "Mean reward so far 24.00\n",
      "Maximal reward so far 51.0\n",
      "-------------------------------------------------\n",
      "Episode 11\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 12.0\n",
      "Mean reward so far 23.00\n",
      "Maximal reward so far 51.0\n",
      "-------------------------------------------------\n",
      "Episode 12\n",
      "Episode ended after 23 steps\n",
      "Accumulated reward in this episode 23.0\n",
      "Mean reward so far 23.00\n",
      "Maximal reward so far 51.0\n",
      "-------------------------------------------------\n",
      "Episode 13\n",
      "Episode ended after 93 steps\n",
      "Accumulated reward in this episode 93.0\n",
      "Mean reward so far 28.00\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 14\n",
      "Episode ended after 49 steps\n",
      "Accumulated reward in this episode 49.0\n",
      "Mean reward so far 29.40\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 15\n",
      "Episode ended after 33 steps\n",
      "Accumulated reward in this episode 33.0\n",
      "Mean reward so far 29.62\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 16\n",
      "Episode ended after 45 steps\n",
      "Accumulated reward in this episode 45.0\n",
      "Mean reward so far 30.53\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 17\n",
      "Episode ended after 42 steps\n",
      "Accumulated reward in this episode 42.0\n",
      "Mean reward so far 31.17\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 18\n",
      "Episode ended after 81 steps\n",
      "Accumulated reward in this episode 81.0\n",
      "Mean reward so far 33.79\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 19\n",
      "Episode ended after 48 steps\n",
      "Accumulated reward in this episode 48.0\n",
      "Mean reward so far 34.50\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 20\n",
      "Episode ended after 53 steps\n",
      "Accumulated reward in this episode 53.0\n",
      "Mean reward so far 35.38\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 21\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 37.0\n",
      "Mean reward so far 35.45\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 22\n",
      "Episode ended after 41 steps\n",
      "Accumulated reward in this episode 41.0\n",
      "Mean reward so far 35.70\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 23\n",
      "Episode ended after 74 steps\n",
      "Accumulated reward in this episode 74.0\n",
      "Mean reward so far 37.29\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 24\n",
      "Episode ended after 55 steps\n",
      "Accumulated reward in this episode 55.0\n",
      "Mean reward so far 38.00\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 25\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode 57.0\n",
      "Mean reward so far 38.73\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 26\n",
      "Episode ended after 68 steps\n",
      "Accumulated reward in this episode 68.0\n",
      "Mean reward so far 39.81\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 27\n",
      "Episode ended after 63 steps\n",
      "Accumulated reward in this episode 63.0\n",
      "Mean reward so far 40.64\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 28\n",
      "Episode ended after 34 steps\n",
      "Accumulated reward in this episode 34.0\n",
      "Mean reward so far 40.41\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 29\n",
      "Episode ended after 44 steps\n",
      "Accumulated reward in this episode 44.0\n",
      "Mean reward so far 40.53\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 30\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 29.0\n",
      "Mean reward so far 40.16\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 31\n",
      "Episode ended after 64 steps\n",
      "Accumulated reward in this episode 64.0\n",
      "Mean reward so far 40.91\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 32\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode 57.0\n",
      "Mean reward so far 41.39\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 33\n",
      "Episode ended after 30 steps\n",
      "Accumulated reward in this episode 30.0\n",
      "Mean reward so far 41.06\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 34\n",
      "Episode ended after 27 steps\n",
      "Accumulated reward in this episode 27.0\n",
      "Mean reward so far 40.66\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 35\n",
      "Episode ended after 27 steps\n",
      "Accumulated reward in this episode 27.0\n",
      "Mean reward so far 40.28\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 36\n",
      "Episode ended after 34 steps\n",
      "Accumulated reward in this episode 34.0\n",
      "Mean reward so far 40.11\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 37\n",
      "Episode ended after 61 steps\n",
      "Accumulated reward in this episode 61.0\n",
      "Mean reward so far 40.66\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 38\n",
      "Episode ended after 31 steps\n",
      "Accumulated reward in this episode 31.0\n",
      "Mean reward so far 40.41\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 39\n",
      "Episode ended after 49 steps\n",
      "Accumulated reward in this episode 49.0\n",
      "Mean reward so far 40.62\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 40\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 21.0\n",
      "Mean reward so far 40.15\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 41\n",
      "Episode ended after 66 steps\n",
      "Accumulated reward in this episode 66.0\n",
      "Mean reward so far 40.76\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 42\n",
      "Episode ended after 25 steps\n",
      "Accumulated reward in this episode 25.0\n",
      "Mean reward so far 40.40\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 43\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 37.0\n",
      "Mean reward so far 40.32\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 44\n",
      "Episode ended after 44 steps\n",
      "Accumulated reward in this episode 44.0\n",
      "Mean reward so far 40.40\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 45\n",
      "Episode ended after 55 steps\n",
      "Accumulated reward in this episode 55.0\n",
      "Mean reward so far 40.72\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 46\n",
      "Episode ended after 41 steps\n",
      "Accumulated reward in this episode 41.0\n",
      "Mean reward so far 40.72\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 47\n",
      "Episode ended after 62 steps\n",
      "Accumulated reward in this episode 62.0\n",
      "Mean reward so far 41.17\n",
      "Maximal reward so far 93.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 48\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode 57.0\n",
      "Mean reward so far 41.49\n",
      "Maximal reward so far 93.0\n",
      "-------------------------------------------------\n",
      "Episode 49\n",
      "Episode ended after 125 steps\n",
      "Accumulated reward in this episode 125.0\n",
      "Mean reward so far 43.16\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 50\n",
      "Episode ended after 52 steps\n",
      "Accumulated reward in this episode 52.0\n",
      "Mean reward so far 43.33\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 51\n",
      "Episode ended after 55 steps\n",
      "Accumulated reward in this episode 55.0\n",
      "Mean reward so far 43.56\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 52\n",
      "Episode ended after 54 steps\n",
      "Accumulated reward in this episode 54.0\n",
      "Mean reward so far 43.75\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 53\n",
      "Episode ended after 91 steps\n",
      "Accumulated reward in this episode 91.0\n",
      "Mean reward so far 44.63\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 54\n",
      "Episode ended after 65 steps\n",
      "Accumulated reward in this episode 65.0\n",
      "Mean reward so far 45.00\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 55\n",
      "Episode ended after 58 steps\n",
      "Accumulated reward in this episode 58.0\n",
      "Mean reward so far 45.23\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 56\n",
      "Episode ended after 198 steps\n",
      "Accumulated reward in this episode 198.0\n",
      "Mean reward so far 47.91\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 57\n",
      "Episode ended after 62 steps\n",
      "Accumulated reward in this episode 62.0\n",
      "Mean reward so far 48.16\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 58\n",
      "Episode ended after 65 steps\n",
      "Accumulated reward in this episode 65.0\n",
      "Mean reward so far 48.44\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 59\n",
      "Episode ended after 80 steps\n",
      "Accumulated reward in this episode 80.0\n",
      "Mean reward so far 48.97\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 60\n",
      "Episode ended after 65 steps\n",
      "Accumulated reward in this episode 65.0\n",
      "Mean reward so far 49.23\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 61\n",
      "Episode ended after 55 steps\n",
      "Accumulated reward in this episode 55.0\n",
      "Mean reward so far 49.32\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 62\n",
      "Episode ended after 81 steps\n",
      "Accumulated reward in this episode 81.0\n",
      "Mean reward so far 49.83\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 63\n",
      "Episode ended after 135 steps\n",
      "Accumulated reward in this episode 135.0\n",
      "Mean reward so far 51.16\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 64\n",
      "Episode ended after 50 steps\n",
      "Accumulated reward in this episode 50.0\n",
      "Mean reward so far 51.14\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 65\n",
      "Episode ended after 71 steps\n",
      "Accumulated reward in this episode 71.0\n",
      "Mean reward so far 51.44\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 66\n",
      "Episode ended after 129 steps\n",
      "Accumulated reward in this episode 129.0\n",
      "Mean reward so far 52.60\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 67\n",
      "Episode ended after 123 steps\n",
      "Accumulated reward in this episode 123.0\n",
      "Mean reward so far 53.63\n",
      "Maximal reward so far 198.0\n",
      "-------------------------------------------------\n",
      "Episode 68\n",
      "Episode ended after 412 steps\n",
      "Accumulated reward in this episode 412.0\n",
      "Mean reward so far 58.83\n",
      "Maximal reward so far 412.0\n",
      "-------------------------------------------------\n",
      "Episode 69\n",
      "Episode ended after 181 steps\n",
      "Accumulated reward in this episode 181.0\n",
      "Mean reward so far 60.57\n",
      "Maximal reward so far 412.0\n",
      "-------------------------------------------------\n",
      "Episode 70\n",
      "Episode ended after 263 steps\n",
      "Accumulated reward in this episode 263.0\n",
      "Mean reward so far 63.42\n",
      "Maximal reward so far 412.0\n",
      "-------------------------------------------------\n",
      "Episode 71\n",
      "Episode ended after 448 steps\n",
      "Accumulated reward in this episode 448.0\n",
      "Mean reward so far 68.76\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 72\n",
      "Episode ended after 277 steps\n",
      "Accumulated reward in this episode 277.0\n",
      "Mean reward so far 71.62\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 73\n",
      "Episode ended after 127 steps\n",
      "Accumulated reward in this episode 127.0\n",
      "Mean reward so far 72.36\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 74\n",
      "Episode ended after 369 steps\n",
      "Accumulated reward in this episode 369.0\n",
      "Mean reward so far 76.32\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 75\n",
      "Episode ended after 151 steps\n",
      "Accumulated reward in this episode 151.0\n",
      "Mean reward so far 77.30\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 76\n",
      "Episode ended after 111 steps\n",
      "Accumulated reward in this episode 111.0\n",
      "Mean reward so far 77.74\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 77\n",
      "Episode ended after 73 steps\n",
      "Accumulated reward in this episode 73.0\n",
      "Mean reward so far 77.68\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 78\n",
      "Episode ended after 422 steps\n",
      "Accumulated reward in this episode 422.0\n",
      "Mean reward so far 82.04\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 79\n",
      "Episode ended after 200 steps\n",
      "Accumulated reward in this episode 200.0\n",
      "Mean reward so far 83.51\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 80\n",
      "Episode ended after 173 steps\n",
      "Accumulated reward in this episode 173.0\n",
      "Mean reward so far 84.62\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 81\n",
      "Episode ended after 193 steps\n",
      "Accumulated reward in this episode 193.0\n",
      "Mean reward so far 85.94\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 82\n",
      "Episode ended after 324 steps\n",
      "Accumulated reward in this episode 324.0\n",
      "Mean reward so far 88.81\n",
      "Maximal reward so far 448.0\n",
      "-------------------------------------------------\n",
      "Episode 83\n",
      "Episode ended after 674 steps\n",
      "Accumulated reward in this episode 674.0\n",
      "Mean reward so far 95.77\n",
      "Maximal reward so far 674.0\n",
      "-------------------------------------------------\n",
      "Episode 84\n",
      "Episode ended after 227 steps\n",
      "Accumulated reward in this episode 227.0\n",
      "Mean reward so far 97.32\n",
      "Maximal reward so far 674.0\n",
      "-------------------------------------------------\n",
      "Episode 85\n",
      "Episode ended after 127 steps\n",
      "Accumulated reward in this episode 127.0\n",
      "Mean reward so far 97.66\n",
      "Maximal reward so far 674.0\n",
      "-------------------------------------------------\n",
      "Episode 86\n",
      "Episode ended after 1737 steps\n",
      "Accumulated reward in this episode 1737.0\n",
      "Mean reward so far 116.51\n",
      "Maximal reward so far 1737.0\n",
      "-------------------------------------------------\n",
      "Episode 87\n",
      "Episode ended after 836 steps\n",
      "Accumulated reward in this episode 836.0\n",
      "Mean reward so far 124.68\n",
      "Maximal reward so far 1737.0\n",
      "-------------------------------------------------\n",
      "Episode 88\n",
      "Episode ended after 818 steps\n",
      "Accumulated reward in this episode 818.0\n",
      "Mean reward so far 132.47\n",
      "Maximal reward so far 1737.0\n",
      "-------------------------------------------------\n",
      "Episode 89\n",
      "Episode ended after 824 steps\n",
      "Accumulated reward in this episode 824.0\n",
      "Mean reward so far 140.16\n",
      "Maximal reward so far 1737.0\n",
      "-------------------------------------------------\n",
      "Episode 90\n",
      "Episode ended after 1191 steps\n",
      "Accumulated reward in this episode 1191.0\n",
      "Mean reward so far 151.70\n",
      "Maximal reward so far 1737.0\n",
      "-------------------------------------------------\n",
      "Episode 91\n",
      "Episode ended after 3160 steps\n",
      "Accumulated reward in this episode 3160.0\n",
      "Mean reward so far 184.40\n",
      "Maximal reward so far 3160.0\n",
      "-------------------------------------------------\n",
      "Episode 92\n",
      "Episode ended after 4113 steps\n",
      "Accumulated reward in this episode 4113.0\n",
      "Mean reward so far 226.65\n",
      "Maximal reward so far 4113.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 93\n",
      "Episode ended after 3178 steps\n",
      "Accumulated reward in this episode 3178.0\n",
      "Mean reward so far 258.04\n",
      "Maximal reward so far 4113.0\n",
      "-------------------------------------------------\n",
      "Episode 94\n",
      "Episode ended after 5669 steps\n",
      "Accumulated reward in this episode 5669.0\n",
      "Mean reward so far 315.00\n",
      "Maximal reward so far 5669.0\n",
      "-------------------------------------------------\n",
      "Episode 95\n",
      "Episode ended after 2096 steps\n",
      "Accumulated reward in this episode 2096.0\n",
      "Mean reward so far 333.55\n",
      "Maximal reward so far 5669.0\n",
      "-------------------------------------------------\n",
      "Episode 96\n",
      "Episode ended after 2645 steps\n",
      "Accumulated reward in this episode 2645.0\n",
      "Mean reward so far 357.38\n",
      "Maximal reward so far 5669.0\n",
      "-------------------------------------------------\n",
      "Episode 97\n",
      "Episode ended after 4062 steps\n",
      "Accumulated reward in this episode 4062.0\n",
      "Mean reward so far 395.18\n",
      "Maximal reward so far 5669.0\n",
      "-------------------------------------------------\n",
      "Episode 98\n",
      "Episode ended after 9415 steps\n",
      "Accumulated reward in this episode 9415.0\n",
      "Mean reward so far 486.29\n",
      "Maximal reward so far 9415.0\n",
      "-------------------------------------------------\n",
      "Episode 99\n",
      "Episode ended after 2765 steps\n",
      "Accumulated reward in this episode 2765.0\n",
      "Mean reward so far 509.08\n",
      "Maximal reward so far 9415.0\n"
     ]
    }
   ],
   "source": [
    "ep_action, ep_obs, ep_reward = [], [], []  # Allocate space for episode actions, observations and rewards\n",
    "tot_ep_reward = [] # Total episode reward\n",
    "mean_reward = []\n",
    "\n",
    "''' Run TF session '''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ''' Run episodes '''\n",
    "    for ep in range(num_episodes): \n",
    "        obs = env.reset()  # Reset and save first observation\n",
    "        ep_obs.append(obs) # append observation\n",
    "\n",
    "        ''' Run steps '''\n",
    "        while True:\n",
    "            # Propagate forward to compute action probability distribution\n",
    "            apd = np.squeeze(sess.run(action_prob_dist, feed_dict = {input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "            action = np.random.choice(np.arange(num_actions), p = apd)   # Sample an action based on the pdf\n",
    "            obs, reward, done, info = env.step(action)  # Take action and save observation, reward and done boolean\n",
    "            \n",
    "            # Convert action to one hot\n",
    "            action_oh = np.zeros((1,num_actions))\n",
    "            action_oh[0,action] = 1\n",
    "            \n",
    "            ep_action.append(action_oh)  # append action\n",
    "            ep_obs.append(obs)           # append observation\n",
    "            ep_reward.append(reward)     # append reward\n",
    "\n",
    "            if done: \n",
    "                # Stack vertically episode parameters to one np.array\n",
    "                ep_action = np.vstack(ep_action)\n",
    "                ep_obs = np.vstack(ep_obs)\n",
    "                ep_reward = np.hstack(ep_reward)\n",
    "\n",
    "                # Discount rewards\n",
    "                dis_rewards_arr = discount_rewards(ep_reward)\n",
    "                # Compute loss and optimize\n",
    "                sess.run([loss, training_opt],\n",
    "                         feed_dict = {input_ : ep_obs[:-1], actions : ep_action, dis_rewards : dis_rewards_arr})\n",
    "                \n",
    "                tot_ep_reward.append(np.sum(ep_reward))  # Compute total reward for episode\n",
    "                mean_reward.append(np.mean(tot_ep_reward))\n",
    "                \n",
    "                 # print info\n",
    "                print(\"-------------------------------------------------\")\n",
    "                print(\"Episode {}\".format(ep))\n",
    "                print(\"Episode ended after {} steps\".format(ep_action.shape[0]))\n",
    "                print(\"Accumulated reward in this episode {}\".format(tot_ep_reward[ep]))\n",
    "                print(\"Mean reward so far {:0.2f}\".format(np.mean(tot_ep_reward)))\n",
    "                print(\"Maximal reward so far {}\".format(np.max(tot_ep_reward)))\n",
    "                \n",
    "                ep_action, ep_obs, ep_reward = [], [], []  # Clear episode values for next episode\n",
    "                      \n",
    "                break\n",
    "\n",
    "    saver.save(sess, \"models/model.ckpt\") # save model for later\n",
    "#     writer.add_graph(sess.graph) # Save graph for displaying with TensorBoard\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of how good our model and to see if our agent improves in the training process, I like to plot the mean rewards gained in one episode over the episode number. This value would grow as our agent becomes more skilled and gains more rewards in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mean_reward)\n",
    "plt.xlabel(\"episode\", fontsize=14)\n",
    "plt.ylabel(\"mean reward\", fontsize = 14)\n",
    "plt.title(\"Mean episode rewards over episode number\", fontsize = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the agent play an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model.ckpt\n",
      "Game ended after 3759 steps\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/model.ckpt\") # load model\n",
    "    obs = env.reset() # Reset env and save observation\n",
    "    t = 0\n",
    "    while True:\n",
    "        env.render() # Render game\n",
    "        # Use our model to create a probability distribution of actions based on observation\n",
    "        apd = np.squeeze(sess.run(action_prob_dist, feed_dict={input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "        # Choose an action out of the PDF and take action\n",
    "        action = np.random.choice(np.arange(num_actions), p = apd)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        t = t+1\n",
    "        if done:\n",
    "            print(\"Game ended after {} steps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
