{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Cart Pole game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define all the hyperparameters for the model, this will allow as to easily iterate and find the values that give best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100      # Number of episodes for training\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99           # Discount factor for reward\n",
    "num_Hidden = 200       # number of nodes in the hidden layer\n",
    "\n",
    "env = gym.make('CartPole-v0')    # Choose a game and create an environment\n",
    "env = env.unwrapped              # The wrapper limits the number of steps in an episode to 200, let's get rid of it\n",
    "obs_dim = env.reset().shape      # obervation dimension\n",
    "num_actions = env.action_space.n # number of actions (this works only for descrete action space, which is the case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discount reward function**: We we'll train our agent based on the reward gained for his actions. For each action we'll define the episode reward as the total reward gained in all the next steps of the current episode. Since a reward gained further away in the future has less correlation to the present action, we will give it less weight by discounting future rewards.\n",
    "\n",
    "The formula for the discounted rewards is given by:\n",
    "\n",
    "$$ R_t = \\sum_k \\gamma^k r_{t+k} $$\n",
    "\n",
    "Where $r_t$ is the reward gained in the step $t$ and $\\gamma \\in [0,1]$ is a hyperparameter called the discount factor.\n",
    "Here we define a function that takes a vector of rewards in consequent steps and returns the discounted reward vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    '''Takes a 1D rewards (one episode) and discounts it and also standardize\n",
    "    the rewards to be unit normal (helps control the gradient estimator variance)'''\n",
    "    \n",
    "    # Discounting\n",
    "    dis_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_sum = gamma*running_sum + r[t]\n",
    "        dis_r[t] = running_sum\n",
    "    \n",
    "    # Normailzing\n",
    "    dis_r = (dis_r - np.mean(dis_r))/np.std(dis_r)\n",
    "        \n",
    "    return dis_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the simplest model: Observation as an input, one hidden layer and two nodes in the output layer - corresponding to the action of moving right (1) or left (0).\n",
    "\n",
    "To keep order in our model, we use name scopes which basically group the layers of our model in a simple to follow way. Eventually, when using [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) for visualizing the model, the graph is more readable and makes it easy to understand the model and find errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create placeholders for inputs'''\n",
    "# A place holder for input observations\n",
    "input_ = tf.placeholder(tf.float32, shape = (None, obs_dim[0]), name = \"input\")\n",
    "# A place holder for actions in a full episode\n",
    "actions = tf.placeholder(tf.float32, shape = (None, num_actions), name = \"actions\")\n",
    "# A place holder for discounted rewards in a full episode\n",
    "dis_rewards = tf.placeholder(tf.float32, shape = (None, ), name = \"dis_rewards\")\n",
    "\n",
    "'''Fully connected layers'''\n",
    "with tf.name_scope(\"FC1\"):\n",
    "    fc1 = tf.layers.dense(inputs = input_, units = num_Hidden ,activation = tf.nn.relu, name = \"fc1\" )\n",
    "\n",
    "with tf.name_scope(\"FC2\"):\n",
    "    fc2 = tf.layers.dense(inputs = fc1, units = num_actions ,activation = None, name = \"fc2\" )\n",
    "\n",
    "with tf.name_scope(\"Action_PDF\"):\n",
    "    # Operate with softmax on fc2 outputs to get an action probability distribution\n",
    "    action_prob_dist = tf.nn.softmax(logits = fc2, name = \"softamx\")\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    '''Define loss'''\n",
    "    # Fist define reular softmax cross entropy loss\n",
    "    CE_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = actions, logits = fc2, name = \"CE_loss\")\n",
    "    # Modulate the loss based on our discounted reward - this is where reinforcment learning happens,\n",
    "    # we favor actions that produced high reward\n",
    "    loss = tf.reduce_mean(CE_loss * dis_rewards)\n",
    "\n",
    "with tf.name_scope(\"Training\"):\n",
    "    '''Define optimizer'''\n",
    "    training_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "'''Define saver for saving and restoring model'''\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a writer for saving summaries to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.summary.FileWriter(\"./tensorboard/1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a graph visualization:\n",
    "\n",
    "<img src=\"./img/model_graph.png\" width=\"500\">\n",
    "\n",
    "Each block in the graph is expandable and let you see the content inside, for example see an [image](./img/model_graph_loss.png) with expanded loss block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model ready, we can start training it.\n",
    "\n",
    "Our goal is to achieve a model that uses the current observation to create the best probability distribution function (PDF) for the next action to be taken. This is **not a deterministic model** - the agent has a fine probability to take any action after receiving an observation, with larger probability to take favorable actions.\n",
    "\n",
    "To achieve that, in each step, we use our model to generate a PDF of actions, draw an action out of it and take the next step.\n",
    "When we reach the end of the episode, we compute the episode loss by feeding the observation, action and discounted rewards vectors (which we kept track of). Our optimizer minimizes the loss which makes favorable (unfavorable) actions more probable (less probable) due to the multiplicative factor of the discounted reward. By repeating these steps to each episode, our agent gradually improves its PDF and becomes a better player!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 0\n",
      "Episode ended after 85 steps\n",
      "Accumulated reward in this episode 85.0\n",
      "Mean reward so far 85.00\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 1\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode 57.0\n",
      "Mean reward so far 71.00\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 2\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 20.0\n",
      "Mean reward so far 54.00\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 3\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 37.0\n",
      "Mean reward so far 49.75\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 4\n",
      "Episode ended after 73 steps\n",
      "Accumulated reward in this episode 73.0\n",
      "Mean reward so far 54.40\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 5\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 17.0\n",
      "Mean reward so far 48.17\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 6\n",
      "Episode ended after 52 steps\n",
      "Accumulated reward in this episode 52.0\n",
      "Mean reward so far 48.71\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 7\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 14.0\n",
      "Mean reward so far 44.38\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 8\n",
      "Episode ended after 33 steps\n",
      "Accumulated reward in this episode 33.0\n",
      "Mean reward so far 43.11\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 9\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode 57.0\n",
      "Mean reward so far 44.50\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 10\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 29.0\n",
      "Mean reward so far 43.09\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 11\n",
      "Episode ended after 16 steps\n",
      "Accumulated reward in this episode 16.0\n",
      "Mean reward so far 40.83\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 12\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 28.0\n",
      "Mean reward so far 39.85\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 13\n",
      "Episode ended after 49 steps\n",
      "Accumulated reward in this episode 49.0\n",
      "Mean reward so far 40.50\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 14\n",
      "Episode ended after 49 steps\n",
      "Accumulated reward in this episode 49.0\n",
      "Mean reward so far 41.07\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 15\n",
      "Episode ended after 42 steps\n",
      "Accumulated reward in this episode 42.0\n",
      "Mean reward so far 41.12\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 16\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 24.0\n",
      "Mean reward so far 40.12\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 17\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 37.0\n",
      "Mean reward so far 39.94\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 18\n",
      "Episode ended after 44 steps\n",
      "Accumulated reward in this episode 44.0\n",
      "Mean reward so far 40.16\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 19\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 20.0\n",
      "Mean reward so far 39.15\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 20\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 22.0\n",
      "Mean reward so far 38.33\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 21\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 21.0\n",
      "Mean reward so far 37.55\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 22\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 22.0\n",
      "Mean reward so far 36.87\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 23\n",
      "Episode ended after 71 steps\n",
      "Accumulated reward in this episode 71.0\n",
      "Mean reward so far 38.29\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 24\n",
      "Episode ended after 58 steps\n",
      "Accumulated reward in this episode 58.0\n",
      "Mean reward so far 39.08\n",
      "Maximal reward so far 85.0\n",
      "-------------------------------------------------\n",
      "Episode 25\n",
      "Episode ended after 149 steps\n",
      "Accumulated reward in this episode 149.0\n",
      "Mean reward so far 43.31\n",
      "Maximal reward so far 149.0\n",
      "-------------------------------------------------\n",
      "Episode 26\n",
      "Episode ended after 145 steps\n",
      "Accumulated reward in this episode 145.0\n",
      "Mean reward so far 47.07\n",
      "Maximal reward so far 149.0\n",
      "-------------------------------------------------\n",
      "Episode 27\n",
      "Episode ended after 182 steps\n",
      "Accumulated reward in this episode 182.0\n",
      "Mean reward so far 51.89\n",
      "Maximal reward so far 182.0\n",
      "-------------------------------------------------\n",
      "Episode 28\n",
      "Episode ended after 60 steps\n",
      "Accumulated reward in this episode 60.0\n",
      "Mean reward so far 52.17\n",
      "Maximal reward so far 182.0\n",
      "-------------------------------------------------\n",
      "Episode 29\n",
      "Episode ended after 125 steps\n",
      "Accumulated reward in this episode 125.0\n",
      "Mean reward so far 54.60\n",
      "Maximal reward so far 182.0\n",
      "-------------------------------------------------\n",
      "Episode 30\n",
      "Episode ended after 118 steps\n",
      "Accumulated reward in this episode 118.0\n",
      "Mean reward so far 56.65\n",
      "Maximal reward so far 182.0\n",
      "-------------------------------------------------\n",
      "Episode 31\n",
      "Episode ended after 150 steps\n",
      "Accumulated reward in this episode 150.0\n",
      "Mean reward so far 59.56\n",
      "Maximal reward so far 182.0\n",
      "-------------------------------------------------\n",
      "Episode 32\n",
      "Episode ended after 135 steps\n",
      "Accumulated reward in this episode 135.0\n",
      "Mean reward so far 61.85\n",
      "Maximal reward so far 182.0\n",
      "-------------------------------------------------\n",
      "Episode 33\n",
      "Episode ended after 217 steps\n",
      "Accumulated reward in this episode 217.0\n",
      "Mean reward so far 66.41\n",
      "Maximal reward so far 217.0\n",
      "-------------------------------------------------\n",
      "Episode 34\n",
      "Episode ended after 138 steps\n",
      "Accumulated reward in this episode 138.0\n",
      "Mean reward so far 68.46\n",
      "Maximal reward so far 217.0\n",
      "-------------------------------------------------\n",
      "Episode 35\n",
      "Episode ended after 138 steps\n",
      "Accumulated reward in this episode 138.0\n",
      "Mean reward so far 70.39\n",
      "Maximal reward so far 217.0\n",
      "-------------------------------------------------\n",
      "Episode 36\n",
      "Episode ended after 232 steps\n",
      "Accumulated reward in this episode 232.0\n",
      "Mean reward so far 74.76\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 37\n",
      "Episode ended after 127 steps\n",
      "Accumulated reward in this episode 127.0\n",
      "Mean reward so far 76.13\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 38\n",
      "Episode ended after 162 steps\n",
      "Accumulated reward in this episode 162.0\n",
      "Mean reward so far 78.33\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 39\n",
      "Episode ended after 101 steps\n",
      "Accumulated reward in this episode 101.0\n",
      "Mean reward so far 78.90\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 40\n",
      "Episode ended after 114 steps\n",
      "Accumulated reward in this episode 114.0\n",
      "Mean reward so far 79.76\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 41\n",
      "Episode ended after 118 steps\n",
      "Accumulated reward in this episode 118.0\n",
      "Mean reward so far 80.67\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 42\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode 57.0\n",
      "Mean reward so far 80.12\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 43\n",
      "Episode ended after 59 steps\n",
      "Accumulated reward in this episode 59.0\n",
      "Mean reward so far 79.64\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 44\n",
      "Episode ended after 23 steps\n",
      "Accumulated reward in this episode 23.0\n",
      "Mean reward so far 78.38\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 45\n",
      "Episode ended after 40 steps\n",
      "Accumulated reward in this episode 40.0\n",
      "Mean reward so far 77.54\n",
      "Maximal reward so far 232.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 46\n",
      "Episode ended after 89 steps\n",
      "Accumulated reward in this episode 89.0\n",
      "Mean reward so far 77.79\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 47\n",
      "Episode ended after 75 steps\n",
      "Accumulated reward in this episode 75.0\n",
      "Mean reward so far 77.73\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 48\n",
      "Episode ended after 73 steps\n",
      "Accumulated reward in this episode 73.0\n",
      "Mean reward so far 77.63\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 49\n",
      "Episode ended after 46 steps\n",
      "Accumulated reward in this episode 46.0\n",
      "Mean reward so far 77.00\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 50\n",
      "Episode ended after 73 steps\n",
      "Accumulated reward in this episode 73.0\n",
      "Mean reward so far 76.92\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 51\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 37.0\n",
      "Mean reward so far 76.15\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 52\n",
      "Episode ended after 90 steps\n",
      "Accumulated reward in this episode 90.0\n",
      "Mean reward so far 76.42\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 53\n",
      "Episode ended after 65 steps\n",
      "Accumulated reward in this episode 65.0\n",
      "Mean reward so far 76.20\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 54\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 37.0\n",
      "Mean reward so far 75.49\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 55\n",
      "Episode ended after 70 steps\n",
      "Accumulated reward in this episode 70.0\n",
      "Mean reward so far 75.39\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 56\n",
      "Episode ended after 109 steps\n",
      "Accumulated reward in this episode 109.0\n",
      "Mean reward so far 75.98\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 57\n",
      "Episode ended after 110 steps\n",
      "Accumulated reward in this episode 110.0\n",
      "Mean reward so far 76.57\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 58\n",
      "Episode ended after 123 steps\n",
      "Accumulated reward in this episode 123.0\n",
      "Mean reward so far 77.36\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 59\n",
      "Episode ended after 154 steps\n",
      "Accumulated reward in this episode 154.0\n",
      "Mean reward so far 78.63\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 60\n",
      "Episode ended after 227 steps\n",
      "Accumulated reward in this episode 227.0\n",
      "Mean reward so far 81.07\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 61\n",
      "Episode ended after 130 steps\n",
      "Accumulated reward in this episode 130.0\n",
      "Mean reward so far 81.85\n",
      "Maximal reward so far 232.0\n",
      "-------------------------------------------------\n",
      "Episode 62\n",
      "Episode ended after 314 steps\n",
      "Accumulated reward in this episode 314.0\n",
      "Mean reward so far 85.54\n",
      "Maximal reward so far 314.0\n",
      "-------------------------------------------------\n",
      "Episode 63\n",
      "Episode ended after 324 steps\n",
      "Accumulated reward in this episode 324.0\n",
      "Mean reward so far 89.27\n",
      "Maximal reward so far 324.0\n",
      "-------------------------------------------------\n",
      "Episode 64\n",
      "Episode ended after 277 steps\n",
      "Accumulated reward in this episode 277.0\n",
      "Mean reward so far 92.15\n",
      "Maximal reward so far 324.0\n",
      "-------------------------------------------------\n",
      "Episode 65\n",
      "Episode ended after 1122 steps\n",
      "Accumulated reward in this episode 1122.0\n",
      "Mean reward so far 107.76\n",
      "Maximal reward so far 1122.0\n",
      "-------------------------------------------------\n",
      "Episode 66\n",
      "Episode ended after 585 steps\n",
      "Accumulated reward in this episode 585.0\n",
      "Mean reward so far 114.88\n",
      "Maximal reward so far 1122.0\n",
      "-------------------------------------------------\n",
      "Episode 67\n",
      "Episode ended after 805 steps\n",
      "Accumulated reward in this episode 805.0\n",
      "Mean reward so far 125.03\n",
      "Maximal reward so far 1122.0\n",
      "-------------------------------------------------\n",
      "Episode 68\n",
      "Episode ended after 667 steps\n",
      "Accumulated reward in this episode 667.0\n",
      "Mean reward so far 132.88\n",
      "Maximal reward so far 1122.0\n",
      "-------------------------------------------------\n",
      "Episode 69\n",
      "Episode ended after 1728 steps\n",
      "Accumulated reward in this episode 1728.0\n",
      "Mean reward so far 155.67\n",
      "Maximal reward so far 1728.0\n",
      "-------------------------------------------------\n",
      "Episode 70\n",
      "Episode ended after 7204 steps\n",
      "Accumulated reward in this episode 7204.0\n",
      "Mean reward so far 254.94\n",
      "Maximal reward so far 7204.0\n",
      "-------------------------------------------------\n",
      "Episode 71\n",
      "Episode ended after 3603 steps\n",
      "Accumulated reward in this episode 3603.0\n",
      "Mean reward so far 301.44\n",
      "Maximal reward so far 7204.0\n",
      "-------------------------------------------------\n",
      "Episode 72\n",
      "Episode ended after 984 steps\n",
      "Accumulated reward in this episode 984.0\n",
      "Mean reward so far 310.79\n",
      "Maximal reward so far 7204.0\n",
      "-------------------------------------------------\n",
      "Episode 73\n",
      "Episode ended after 1088 steps\n",
      "Accumulated reward in this episode 1088.0\n",
      "Mean reward so far 321.30\n",
      "Maximal reward so far 7204.0\n",
      "-------------------------------------------------\n",
      "Episode 74\n",
      "Episode ended after 6302 steps\n",
      "Accumulated reward in this episode 6302.0\n",
      "Mean reward so far 401.04\n",
      "Maximal reward so far 7204.0\n",
      "-------------------------------------------------\n",
      "Episode 75\n",
      "Episode ended after 1620 steps\n",
      "Accumulated reward in this episode 1620.0\n",
      "Mean reward so far 417.08\n",
      "Maximal reward so far 7204.0\n",
      "-------------------------------------------------\n",
      "Episode 76\n",
      "Episode ended after 21009 steps\n",
      "Accumulated reward in this episode 21009.0\n",
      "Mean reward so far 684.51\n",
      "Maximal reward so far 21009.0\n",
      "-------------------------------------------------\n",
      "Episode 77\n",
      "Episode ended after 2566 steps\n",
      "Accumulated reward in this episode 2566.0\n",
      "Mean reward so far 708.63\n",
      "Maximal reward so far 21009.0\n",
      "-------------------------------------------------\n",
      "Episode 78\n",
      "Episode ended after 1101 steps\n",
      "Accumulated reward in this episode 1101.0\n",
      "Mean reward so far 713.59\n",
      "Maximal reward so far 21009.0\n",
      "-------------------------------------------------\n",
      "Episode 79\n",
      "Episode ended after 5657 steps\n",
      "Accumulated reward in this episode 5657.0\n",
      "Mean reward so far 775.39\n",
      "Maximal reward so far 21009.0\n",
      "-------------------------------------------------\n",
      "Episode 80\n",
      "Episode ended after 906 steps\n",
      "Accumulated reward in this episode 906.0\n",
      "Mean reward so far 777.00\n",
      "Maximal reward so far 21009.0\n",
      "-------------------------------------------------\n",
      "Episode 81\n",
      "Episode ended after 15645 steps\n",
      "Accumulated reward in this episode 15645.0\n",
      "Mean reward so far 958.32\n",
      "Maximal reward so far 21009.0\n",
      "-------------------------------------------------\n",
      "Episode 82\n",
      "Episode ended after 1934 steps\n",
      "Accumulated reward in this episode 1934.0\n",
      "Mean reward so far 970.07\n",
      "Maximal reward so far 21009.0\n",
      "-------------------------------------------------\n",
      "Episode 83\n",
      "Episode ended after 13403 steps\n",
      "Accumulated reward in this episode 13403.0\n",
      "Mean reward so far 1118.08\n",
      "Maximal reward so far 21009.0\n",
      "-------------------------------------------------\n",
      "Episode 84\n",
      "Episode ended after 34603 steps\n",
      "Accumulated reward in this episode 34603.0\n",
      "Mean reward so far 1512.02\n",
      "Maximal reward so far 34603.0\n",
      "-------------------------------------------------\n",
      "Episode 85\n",
      "Episode ended after 211507 steps\n",
      "Accumulated reward in this episode 211507.0\n",
      "Mean reward so far 3953.83\n",
      "Maximal reward so far 211507.0\n",
      "-------------------------------------------------\n",
      "Episode 86\n",
      "Episode ended after 81565 steps\n",
      "Accumulated reward in this episode 81565.0\n",
      "Mean reward so far 4845.91\n",
      "Maximal reward so far 211507.0\n",
      "-------------------------------------------------\n",
      "Episode 87\n",
      "Episode ended after 43351 steps\n",
      "Accumulated reward in this episode 43351.0\n",
      "Mean reward so far 5283.47\n",
      "Maximal reward so far 211507.0\n",
      "-------------------------------------------------\n",
      "Episode 88\n",
      "Episode ended after 6292 steps\n",
      "Accumulated reward in this episode 6292.0\n",
      "Mean reward so far 5294.80\n",
      "Maximal reward so far 211507.0\n",
      "-------------------------------------------------\n",
      "Episode 89\n",
      "Episode ended after 34735 steps\n",
      "Accumulated reward in this episode 34735.0\n",
      "Mean reward so far 5621.91\n",
      "Maximal reward so far 211507.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 90\n",
      "Episode ended after 290577 steps\n",
      "Accumulated reward in this episode 290577.0\n",
      "Mean reward so far 8753.29\n",
      "Maximal reward so far 290577.0\n",
      "-------------------------------------------------\n",
      "Episode 91\n",
      "Episode ended after 582503 steps\n",
      "Accumulated reward in this episode 582503.0\n",
      "Mean reward so far 14989.70\n",
      "Maximal reward so far 582503.0\n",
      "-------------------------------------------------\n",
      "Episode 92\n",
      "Episode ended after 768659 steps\n",
      "Accumulated reward in this episode 768659.0\n",
      "Mean reward so far 23093.67\n",
      "Maximal reward so far 768659.0\n",
      "-------------------------------------------------\n",
      "Episode 93\n",
      "Episode ended after 356940 steps\n",
      "Accumulated reward in this episode 356940.0\n",
      "Mean reward so far 26645.22\n",
      "Maximal reward so far 768659.0\n",
      "-------------------------------------------------\n",
      "Episode 94\n",
      "Episode ended after 424289 steps\n",
      "Accumulated reward in this episode 424289.0\n",
      "Mean reward so far 30830.95\n",
      "Maximal reward so far 768659.0\n",
      "-------------------------------------------------\n",
      "Episode 95\n",
      "Episode ended after 1994553 steps\n",
      "Accumulated reward in this episode 1994553.0\n",
      "Mean reward so far 51286.39\n",
      "Maximal reward so far 1994553.0\n",
      "-------------------------------------------------\n",
      "Episode 96\n",
      "Episode ended after 993969 steps\n",
      "Accumulated reward in this episode 993969.0\n",
      "Mean reward so far 61004.76\n",
      "Maximal reward so far 1994553.0\n",
      "-------------------------------------------------\n",
      "Episode 97\n",
      "Episode ended after 797929 steps\n",
      "Accumulated reward in this episode 797929.0\n",
      "Mean reward so far 68524.40\n",
      "Maximal reward so far 1994553.0\n",
      "-------------------------------------------------\n",
      "Episode 98\n",
      "Episode ended after 899534 steps\n",
      "Accumulated reward in this episode 899534.0\n",
      "Mean reward so far 76918.43\n",
      "Maximal reward so far 1994553.0\n",
      "-------------------------------------------------\n",
      "Episode 99\n",
      "Episode ended after 1196315 steps\n",
      "Accumulated reward in this episode 1196315.0\n",
      "Mean reward so far 88112.40\n",
      "Maximal reward so far 1994553.0\n"
     ]
    }
   ],
   "source": [
    "ep_action, ep_obs, ep_reward = [], [], []  # Allocate space for episode actions, observations and rewards\n",
    "tot_ep_reward = [] # Total episode reward\n",
    "mean_reward = []\n",
    "\n",
    "''' Run TF session '''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ''' Run episodes '''\n",
    "    for ep in range(num_episodes): \n",
    "        obs = env.reset()  # Reset and save first observation\n",
    "        ep_obs.append(obs) # append observation\n",
    "\n",
    "        ''' Run steps '''\n",
    "        while True:\n",
    "            # Propagate forward to compute action probability distribution\n",
    "            apd = np.squeeze(sess.run(action_prob_dist, feed_dict = {input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "            action = np.random.choice(np.arange(num_actions), p = apd)   # Sample an action based on the pdf\n",
    "            obs, reward, done, info = env.step(action)  # Take action and save observation, reward and done boolean\n",
    "            \n",
    "            # Convert action to one hot\n",
    "            action_oh = np.zeros((1,num_actions))\n",
    "            action_oh[0,action] = 1\n",
    "            \n",
    "            ep_action.append(action_oh)  # append action\n",
    "            ep_obs.append(obs)           # append observation\n",
    "            ep_reward.append(reward)     # append reward\n",
    "\n",
    "            if done: \n",
    "                # Stack vertically episode parameters to one np.array\n",
    "                ep_action = np.vstack(ep_action)\n",
    "                ep_obs = np.vstack(ep_obs)\n",
    "                ep_reward = np.hstack(ep_reward)\n",
    "\n",
    "                # Discount rewards\n",
    "                dis_rewards_arr = discount_rewards(ep_reward)\n",
    "                # Compute loss and optimize\n",
    "                sess.run([loss, training_opt],\n",
    "                         feed_dict = {input_ : ep_obs[:-1], actions : ep_action, dis_rewards : dis_rewards_arr})\n",
    "                \n",
    "                tot_ep_reward.append(np.sum(ep_reward))  # Compute total reward for episode\n",
    "                mean_reward.append(np.mean(tot_ep_reward))\n",
    "                \n",
    "                 # print info\n",
    "                print(\"-------------------------------------------------\")\n",
    "                print(\"Episode {}\".format(ep))\n",
    "                print(\"Episode ended after {} steps\".format(ep_action.shape[0]))\n",
    "                print(\"Accumulated reward in this episode {}\".format(tot_ep_reward[ep]))\n",
    "                print(\"Mean reward so far {:0.2f}\".format(np.mean(tot_ep_reward)))\n",
    "                print(\"Maximal reward so far {}\".format(np.max(tot_ep_reward)))\n",
    "                \n",
    "                ep_action, ep_obs, ep_reward = [], [], []  # Clear episode values for next episode\n",
    "                      \n",
    "                break\n",
    "                \n",
    "    saver.save(sess, \"models/CartPole/model.ckpt\") # save model for later\n",
    "#     writer.add_graph(sess.graph) # Save graph for displaying with TensorBoard\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of how good our model is, and to see if our agent improves in the training process, we can plot the mean rewards gained in one episode over the episode number. This value would grow as our agent becomes more skilled and gains more rewards in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mean_reward)\n",
    "plt.xlabel(\"episode\", fontsize=14)\n",
    "plt.ylabel(\"mean reward\", fontsize = 14)\n",
    "plt.title(\"Mean episode rewards over episode number\", fontsize = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch the agent play an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fun part!\n",
    "Now we get to see how good our agent really is by watching it play an episode. I also record the episode using the Monitor wrapper and limit the number of steps for this episode to 500, because this skilled agent can play forever..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/CartPole/model.ckpt\n",
      "Game ended after 501 steps\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 500\n",
    "# env = gym.wrappers.Monitor(env, \"recording/CartPole\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/CartPole/model.ckpt\") # load model\n",
    "    obs = env.reset() # Reset env and save observation\n",
    "    t = 0\n",
    "    while True:\n",
    "        env.render() # Render game\n",
    "        # Use our model to create a probability distribution of actions based on observation\n",
    "        apd = np.squeeze(sess.run(action_prob_dist, feed_dict={input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "        # Choose an action out of the PDF and take action\n",
    "        action = np.random.choice(np.arange(num_actions), p = apd)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        t = t+1\n",
    "        if done:\n",
    "            print(\"Game ended after {} steps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recorded video is available [here](./recording/CartPole/openaigym.video.0.15312.video000000.mp4), and also as a cool gif right below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/CartPole_Agent.gif\" width=\"500\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
