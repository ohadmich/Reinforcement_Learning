{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Cart Pole game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10**7      # Maxiaml number of steps in episode\n",
    "num_episodes = 200      # Number of episodes to play\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99           # Discount factor for reward\n",
    "num_Hidden = 200       # number of nodes in the hidden layer\n",
    "\n",
    "env = gym.make('CartPole-v0')    # Choose a game and create an environment\n",
    "obs_dim = env.reset().shape      # obervation dimension\n",
    "num_actions = env.action_space.n # number of actions (this works only for descrete action space, which is the case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    '''Takes a 1D rewards (one episode) and discounts it and also standardize\n",
    "    the rewards to be unit normal (helps control the gradient estimator variance)'''\n",
    "    \n",
    "    # Discounting\n",
    "    dis_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_sum = running_sum + gamma*r[t]\n",
    "        dis_r[t] = running_sum\n",
    "    \n",
    "    # Normailzing\n",
    "    dis_r = (dis_r - np.mean(dis_r))/np.std(dis_r)\n",
    "        \n",
    "    return dis_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the simplest model: Observation as an input, one hidden layer and one node in the output layer - corresponding to the action of moving right (1) or left (0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create placeholders for inputs'''\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    # A place holder for input observations\n",
    "    input_ = tf.placeholder(tf.float32, shape = (None, obs_dim[0]), name = \"input\")\n",
    "    # A place holder for actions in a full episode\n",
    "    actions = tf.placeholder(tf.float32, shape = (None, num_actions), name = \"actions\")\n",
    "    # A place holder for discounted rewards in a full episode\n",
    "    dis_rewards = tf.placeholder(tf.float32, shape = (None, 1), name = \"dis_rewards\")\n",
    "\n",
    "'''Fully connected layers'''\n",
    "with tf.name_scope(\"FC\"):\n",
    "    fc1 = tf.layers.dense(inputs = input_, units = num_Hidden ,activation = tf.nn.relu, name = \"fc1\" )\n",
    "    fc2 = tf.layers.dense(inputs = fc1, units = num_actions ,activation = None, name = \"fc2\" )\n",
    "    \n",
    "# Operate with softmax on fc2 outputs to get a probability distribution\n",
    "action_prob_dist = tf.nn.softmax(logits = fc2, name = \"softamx\")\n",
    "\n",
    "'''Define loss'''\n",
    "# Fist define reular softmax cross entropy loss\n",
    "CE_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = actions, logits = fc2, name = \"CE_loss\")\n",
    "# Modulate the loss based on our discounted reward - this is where reinforcment learning happens,\n",
    "# we favor actions that produced high reward\n",
    "loss = tf.reduce_mean(CE_loss * dis_rewards)\n",
    "\n",
    "'''Define optimizer'''\n",
    "training_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 19.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 17.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 17.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 38 steps\n",
      "Accumulated reward in this episode 37.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 42 steps\n",
      "Accumulated reward in this episode 41.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 20.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 44 steps\n",
      "Accumulated reward in this episode 43.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 41 steps\n",
      "Accumulated reward in this episode 40.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 23 steps\n",
      "Accumulated reward in this episode 22.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 18.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 36 steps\n",
      "Accumulated reward in this episode 35.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 21.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 25 steps\n",
      "Accumulated reward in this episode 24.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 17.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 15 steps\n",
      "Accumulated reward in this episode 14.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode 56.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 18.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 75 steps\n",
      "Accumulated reward in this episode 74.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 63 steps\n",
      "Accumulated reward in this episode 62.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 12.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 38 steps\n",
      "Accumulated reward in this episode 37.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 36 steps\n",
      "Accumulated reward in this episode 35.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 27 steps\n",
      "Accumulated reward in this episode 26.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 21.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 19.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 20.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 27.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode 56.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 35 steps\n",
      "Accumulated reward in this episode 34.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 21.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 46 steps\n",
      "Accumulated reward in this episode 45.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 11.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 47 steps\n",
      "Accumulated reward in this episode 46.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 21.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 36.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 17.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 11.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 19.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 31 steps\n",
      "Accumulated reward in this episode 30.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 25 steps\n",
      "Accumulated reward in this episode 24.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 28.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 23.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 11.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 18.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 26 steps\n",
      "Accumulated reward in this episode 25.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 36 steps\n",
      "Accumulated reward in this episode 35.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 28.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 36.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 25 steps\n",
      "Accumulated reward in this episode 24.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 35 steps\n",
      "Accumulated reward in this episode 34.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 17.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 23 steps\n",
      "Accumulated reward in this episode 22.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 23.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 20.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 34 steps\n",
      "Accumulated reward in this episode 33.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 12.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 18.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 70 steps\n",
      "Accumulated reward in this episode 69.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 18.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 64 steps\n",
      "Accumulated reward in this episode 63.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 23 steps\n",
      "Accumulated reward in this episode 22.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 19.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 19.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 27.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 18.0 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Episode ended after 93 steps\n",
      "Accumulated reward in this episode 92.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 11.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 16 steps\n",
      "Accumulated reward in this episode 15.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 18.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 27 steps\n",
      "Accumulated reward in this episode 26.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 53 steps\n",
      "Accumulated reward in this episode 52.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 27.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 48 steps\n",
      "Accumulated reward in this episode 47.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 60 steps\n",
      "Accumulated reward in this episode 59.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 21.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 26 steps\n",
      "Accumulated reward in this episode 25.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 23 steps\n",
      "Accumulated reward in this episode 22.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 12.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 44 steps\n",
      "Accumulated reward in this episode 43.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 28.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 11.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 26 steps\n",
      "Accumulated reward in this episode 25.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 27.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 15 steps\n",
      "Accumulated reward in this episode 14.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 16 steps\n",
      "Accumulated reward in this episode 15.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 32 steps\n",
      "Accumulated reward in this episode 31.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 27 steps\n",
      "Accumulated reward in this episode 26.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 33 steps\n",
      "Accumulated reward in this episode 32.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 27 steps\n",
      "Accumulated reward in this episode 26.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 25 steps\n",
      "Accumulated reward in this episode 24.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 25 steps\n",
      "Accumulated reward in this episode 24.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 12.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 16 steps\n",
      "Accumulated reward in this episode 15.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 27.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 12.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 30 steps\n",
      "Accumulated reward in this episode 29.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 32 steps\n",
      "Accumulated reward in this episode 31.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 36 steps\n",
      "Accumulated reward in this episode 35.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 11.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 23 steps\n",
      "Accumulated reward in this episode 22.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 23.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 34 steps\n",
      "Accumulated reward in this episode 33.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 21.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 30 steps\n",
      "Accumulated reward in this episode 29.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 19.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 10 steps\n",
      "Accumulated reward in this episode 9.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 18.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 20.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 18.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 39 steps\n",
      "Accumulated reward in this episode 38.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 23.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 30 steps\n",
      "Accumulated reward in this episode 29.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 18.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 19.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 10 steps\n",
      "Accumulated reward in this episode 9.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 38 steps\n",
      "Accumulated reward in this episode 37.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 11 steps\n",
      "Accumulated reward in this episode 10.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 26 steps\n",
      "Accumulated reward in this episode 25.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 19.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 17.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 15 steps\n",
      "Accumulated reward in this episode 14.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 31 steps\n",
      "Accumulated reward in this episode 30.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 23.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 12.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 35 steps\n",
      "Accumulated reward in this episode 34.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 48 steps\n",
      "Accumulated reward in this episode 47.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 23.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 38 steps\n",
      "Accumulated reward in this episode 37.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 48 steps\n",
      "Accumulated reward in this episode 47.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 11 steps\n",
      "Accumulated reward in this episode 10.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 25 steps\n",
      "Accumulated reward in this episode 24.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 51 steps\n",
      "Accumulated reward in this episode 50.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 21.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 20.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 11.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 23.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 49 steps\n",
      "Accumulated reward in this episode 48.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 28.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 81 steps\n",
      "Accumulated reward in this episode 80.0 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Episode ended after 25 steps\n",
      "Accumulated reward in this episode 24.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 11.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 39 steps\n",
      "Accumulated reward in this episode 38.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 21.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 53 steps\n",
      "Accumulated reward in this episode 52.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 17.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 20.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 41 steps\n",
      "Accumulated reward in this episode 40.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 56 steps\n",
      "Accumulated reward in this episode 55.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 20.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 30 steps\n",
      "Accumulated reward in this episode 29.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 38 steps\n",
      "Accumulated reward in this episode 37.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 26 steps\n",
      "Accumulated reward in this episode 25.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 11.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 43 steps\n",
      "Accumulated reward in this episode 42.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 27 steps\n",
      "Accumulated reward in this episode 26.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 53 steps\n",
      "Accumulated reward in this episode 52.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 41 steps\n",
      "Accumulated reward in this episode 40.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 17.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 16.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 17.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 27.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 35 steps\n",
      "Accumulated reward in this episode 34.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 23.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 74 steps\n",
      "Accumulated reward in this episode 73.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 45 steps\n",
      "Accumulated reward in this episode 44.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 20 steps\n",
      "Accumulated reward in this episode 19.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 34 steps\n",
      "Accumulated reward in this episode 33.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 30 steps\n",
      "Accumulated reward in this episode 29.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 12.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 28 steps\n",
      "Accumulated reward in this episode 27.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 26 steps\n",
      "Accumulated reward in this episode 25.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 28.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 10 steps\n",
      "Accumulated reward in this episode 9.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 34 steps\n",
      "Accumulated reward in this episode 33.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 33 steps\n",
      "Accumulated reward in this episode 32.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 13.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 47 steps\n",
      "Accumulated reward in this episode 46.0 steps\n",
      "----------------------------------------\n",
      "Episode ended after 44 steps\n",
      "Accumulated reward in this episode 43.0 steps\n"
     ]
    }
   ],
   "source": [
    "ep_action, ep_obs, ep_reward = [], [], []  # Allocate space for episode actions, observations and rewards\n",
    "tot_ep_reward = [] # Total episode reward\n",
    "\n",
    "''' Run TF session '''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ''' Run episodes '''\n",
    "    for ep in range(num_episodes): \n",
    "        obs = env.reset()  # Reset and save first observation\n",
    "        ep_obs.append(obs) # append observation\n",
    "\n",
    "        ''' Run steps '''\n",
    "        for t in range(1,num_steps):\n",
    "            # Propagate forward to compute action probability distribution\n",
    "            apd = np.squeeze(sess.run(action_prob_dist, feed_dict = {input_ : obs.reshape((1,4))}))\n",
    "            action = np.random.choice(np.arange(num_actions), p = apd)   # Sample an action based on the pdf\n",
    "            obs, reward, done, info = env.step(action)  # Take action and save observation, reward and done boolean\n",
    "            \n",
    "            # Convert action to one hot\n",
    "            action_oh = np.zeros((1,num_actions))\n",
    "            action_oh[0,action] = 1\n",
    "            \n",
    "            ep_action.append(action_oh)  # append action\n",
    "            ep_obs.append(obs)           # append observation\n",
    "            ep_reward.append(reward)     # append reward\n",
    "\n",
    "            if done: \n",
    "                # Stack vertically episode parameters to one np.array\n",
    "                ep_action = np.vstack(ep_action)\n",
    "                ep_obs = np.vstack(ep_obs)\n",
    "                ep_reward = np.vstack(ep_reward)\n",
    "\n",
    "                # Discount rewards\n",
    "                dis_rewards_arr = discount_rewards(ep_reward)\n",
    "                # Compute loss and optimize\n",
    "                sess.run([loss, training_opt], feed_dict = {input_ : ep_obs[:-1], actions : ep_action, dis_rewards : dis_rewards_arr})\n",
    "                \n",
    "                tot_ep_reward.append(np.sum(ep_reward))  # Compute total reward for episode\n",
    "                ep_action, ep_obs, ep_reward = [], [], []  # Clear episode values for next episode\n",
    "                # print info\n",
    "                print(\"----------------------------------------\")\n",
    "                print(\"Episode ended after {} steps\".format(t+1))\n",
    "                print(\"Accumulated reward in this episode {} steps\".format(tot_ep_reward[ep]))\n",
    "                break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
