{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Cart Pole game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100      # Number of episodes for training\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99           # Discount factor for reward\n",
    "num_Hidden = 200       # number of nodes in the hidden layer\n",
    "\n",
    "env = gym.make('CartPole-v0')    # Choose a game and create an environment\n",
    "env = env.unwrapped              # The wrapper limits the number of steps in an episode to 200, let's get rid of it\n",
    "obs_dim = env.reset().shape      # obervation dimension\n",
    "num_actions = env.action_space.n # number of actions (this works only for descrete action space, which is the case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    '''Takes a 1D rewards (one episode) and discounts it and also standardize\n",
    "    the rewards to be unit normal (helps control the gradient estimator variance)'''\n",
    "    \n",
    "    # Discounting\n",
    "    dis_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_sum = gamma*running_sum + r[t]\n",
    "        dis_r[t] = running_sum\n",
    "    \n",
    "    # Normailzing\n",
    "    dis_r = (dis_r - np.mean(dis_r))/np.std(dis_r)\n",
    "        \n",
    "    return dis_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the simplest model: Observation as an input, one hidden layer and two nodes in the output layer - corresponding to the action of moving right (1) or left (0).\n",
    "\n",
    "To keep order in our model, we use name scopes which basically group the layers of our model in a simple to follow way. Eventually, when using [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) for visualizing the model, the graph is more readable and makes it easy to understand the model and find errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create placeholders for inputs'''\n",
    "# A place holder for input observations\n",
    "input_ = tf.placeholder(tf.float32, shape = (None, obs_dim[0]), name = \"input\")\n",
    "# A place holder for actions in a full episode\n",
    "actions = tf.placeholder(tf.float32, shape = (None, num_actions), name = \"actions\")\n",
    "# A place holder for discounted rewards in a full episode\n",
    "dis_rewards = tf.placeholder(tf.float32, shape = (None, ), name = \"dis_rewards\")\n",
    "\n",
    "'''Fully connected layers'''\n",
    "with tf.name_scope(\"FC1\"):\n",
    "    fc1 = tf.layers.dense(inputs = input_, units = num_Hidden ,activation = tf.nn.relu, name = \"fc1\" )\n",
    "\n",
    "with tf.name_scope(\"FC2\"):\n",
    "    fc2 = tf.layers.dense(inputs = fc1, units = num_actions ,activation = None, name = \"fc2\" )\n",
    "\n",
    "with tf.name_scope(\"Action_PDF\"):\n",
    "    # Operate with softmax on fc2 outputs to get an action probability distribution\n",
    "    action_prob_dist = tf.nn.softmax(logits = fc2, name = \"softamx\")\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    '''Define loss'''\n",
    "    # Fist define reular softmax cross entropy loss\n",
    "    CE_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = actions, logits = fc2, name = \"CE_loss\")\n",
    "    # Modulate the loss based on our discounted reward - this is where reinforcment learning happens,\n",
    "    # we favor actions that produced high reward\n",
    "    loss = tf.reduce_mean(CE_loss * dis_rewards)\n",
    "\n",
    "with tf.name_scope(\"Training\"):\n",
    "    '''Define optimizer'''\n",
    "    training_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "'''Define saver for saving and restoring model'''\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a writer for saving summaries to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.summary.FileWriter(\"./tensorboard/1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a graph visualization:\n",
    "\n",
    "<img src=\"./img/model_graph.png\" width=\"500\">\n",
    "\n",
    "Each block in the graph is expandable and let you see the content inside, for example see an [image](./img/model_graph_loss.png) with expanded loss block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model ready, we can start training it.\n",
    "\n",
    "Our goal is to achieve a model that uses the current observation to create the best probability distribution function (PDF) for the next action to be taken. This is **not a deterministic model** - the agent has a fine probability to take any action after receiving an observation, with larger probability to take favorable actions.\n",
    "\n",
    "To achieve that, in each step, we use our model to generate a PDF of actions, draw an action out of it and take the next step.\n",
    "When we reach the end of the episode, we compute the episode loss by feeding the observation, action and discounted rewards vectors (which we kept track of). Our optimizer minimizes the loss which makes favorable (unfavorable) actions more probable (less probable) due to the multiplicative factor of the discounted reward. By repeating these steps to each episode, our agent gradually improves its PDF and becomes a better player!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 0\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 13.0\n",
      "Mean reward so far 13.00\n",
      "Maximal reward so far 13.0\n",
      "-------------------------------------------------\n",
      "Episode 1\n",
      "Episode ended after 31 steps\n",
      "Accumulated reward in this episode 31.0\n",
      "Mean reward so far 22.00\n",
      "Maximal reward so far 31.0\n",
      "-------------------------------------------------\n",
      "Episode 2\n",
      "Episode ended after 38 steps\n",
      "Accumulated reward in this episode 38.0\n",
      "Mean reward so far 27.33\n",
      "Maximal reward so far 38.0\n",
      "-------------------------------------------------\n",
      "Episode 3\n",
      "Episode ended after 39 steps\n",
      "Accumulated reward in this episode 39.0\n",
      "Mean reward so far 30.25\n",
      "Maximal reward so far 39.0\n",
      "-------------------------------------------------\n",
      "Episode 4\n",
      "Episode ended after 15 steps\n",
      "Accumulated reward in this episode 15.0\n",
      "Mean reward so far 27.20\n",
      "Maximal reward so far 39.0\n",
      "-------------------------------------------------\n",
      "Episode 5\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 17.0\n",
      "Mean reward so far 25.50\n",
      "Maximal reward so far 39.0\n",
      "-------------------------------------------------\n",
      "Episode 6\n",
      "Episode ended after 34 steps\n",
      "Accumulated reward in this episode 34.0\n",
      "Mean reward so far 26.71\n",
      "Maximal reward so far 39.0\n",
      "-------------------------------------------------\n",
      "Episode 7\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 18.0\n",
      "Mean reward so far 25.62\n",
      "Maximal reward so far 39.0\n",
      "-------------------------------------------------\n",
      "Episode 8\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 29.0\n",
      "Mean reward so far 26.00\n",
      "Maximal reward so far 39.0\n",
      "-------------------------------------------------\n",
      "Episode 9\n",
      "Episode ended after 23 steps\n",
      "Accumulated reward in this episode 23.0\n",
      "Mean reward so far 25.70\n",
      "Maximal reward so far 39.0\n",
      "-------------------------------------------------\n",
      "Episode 10\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 14.0\n",
      "Mean reward so far 24.64\n",
      "Maximal reward so far 39.0\n",
      "-------------------------------------------------\n",
      "Episode 11\n",
      "Episode ended after 22 steps\n",
      "Accumulated reward in this episode 22.0\n",
      "Mean reward so far 24.42\n",
      "Maximal reward so far 39.0\n",
      "-------------------------------------------------\n",
      "Episode 12\n",
      "Episode ended after 69 steps\n",
      "Accumulated reward in this episode 69.0\n",
      "Mean reward so far 27.85\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 13\n",
      "Episode ended after 11 steps\n",
      "Accumulated reward in this episode 11.0\n",
      "Mean reward so far 26.64\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 14\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 13.0\n",
      "Mean reward so far 25.73\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 15\n",
      "Episode ended after 15 steps\n",
      "Accumulated reward in this episode 15.0\n",
      "Mean reward so far 25.06\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 16\n",
      "Episode ended after 40 steps\n",
      "Accumulated reward in this episode 40.0\n",
      "Mean reward so far 25.94\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 17\n",
      "Episode ended after 27 steps\n",
      "Accumulated reward in this episode 27.0\n",
      "Mean reward so far 26.00\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 18\n",
      "Episode ended after 35 steps\n",
      "Accumulated reward in this episode 35.0\n",
      "Mean reward so far 26.47\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 19\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 17.0\n",
      "Mean reward so far 26.00\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 20\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 13.0\n",
      "Mean reward so far 25.38\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 21\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 13.0\n",
      "Mean reward so far 24.82\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 22\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 29.0\n",
      "Mean reward so far 25.00\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 23\n",
      "Episode ended after 25 steps\n",
      "Accumulated reward in this episode 25.0\n",
      "Mean reward so far 25.00\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 24\n",
      "Episode ended after 15 steps\n",
      "Accumulated reward in this episode 15.0\n",
      "Mean reward so far 24.60\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 25\n",
      "Episode ended after 23 steps\n",
      "Accumulated reward in this episode 23.0\n",
      "Mean reward so far 24.54\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 26\n",
      "Episode ended after 15 steps\n",
      "Accumulated reward in this episode 15.0\n",
      "Mean reward so far 24.19\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 27\n",
      "Episode ended after 10 steps\n",
      "Accumulated reward in this episode 10.0\n",
      "Mean reward so far 23.68\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 28\n",
      "Episode ended after 61 steps\n",
      "Accumulated reward in this episode 61.0\n",
      "Mean reward so far 24.97\n",
      "Maximal reward so far 69.0\n",
      "-------------------------------------------------\n",
      "Episode 29\n",
      "Episode ended after 76 steps\n",
      "Accumulated reward in this episode 76.0\n",
      "Mean reward so far 26.67\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 30\n",
      "Episode ended after 18 steps\n",
      "Accumulated reward in this episode 18.0\n",
      "Mean reward so far 26.39\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 31\n",
      "Episode ended after 17 steps\n",
      "Accumulated reward in this episode 17.0\n",
      "Mean reward so far 26.09\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 32\n",
      "Episode ended after 35 steps\n",
      "Accumulated reward in this episode 35.0\n",
      "Mean reward so far 26.36\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 33\n",
      "Episode ended after 29 steps\n",
      "Accumulated reward in this episode 29.0\n",
      "Mean reward so far 26.44\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 34\n",
      "Episode ended after 10 steps\n",
      "Accumulated reward in this episode 10.0\n",
      "Mean reward so far 25.97\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 35\n",
      "Episode ended after 47 steps\n",
      "Accumulated reward in this episode 47.0\n",
      "Mean reward so far 26.56\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 36\n",
      "Episode ended after 44 steps\n",
      "Accumulated reward in this episode 44.0\n",
      "Mean reward so far 27.03\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 37\n",
      "Episode ended after 14 steps\n",
      "Accumulated reward in this episode 14.0\n",
      "Mean reward so far 26.68\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 38\n",
      "Episode ended after 42 steps\n",
      "Accumulated reward in this episode 42.0\n",
      "Mean reward so far 27.08\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 39\n",
      "Episode ended after 21 steps\n",
      "Accumulated reward in this episode 21.0\n",
      "Mean reward so far 26.93\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 40\n",
      "Episode ended after 12 steps\n",
      "Accumulated reward in this episode 12.0\n",
      "Mean reward so far 26.56\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 41\n",
      "Episode ended after 32 steps\n",
      "Accumulated reward in this episode 32.0\n",
      "Mean reward so far 26.69\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 42\n",
      "Episode ended after 24 steps\n",
      "Accumulated reward in this episode 24.0\n",
      "Mean reward so far 26.63\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 43\n",
      "Episode ended after 63 steps\n",
      "Accumulated reward in this episode 63.0\n",
      "Mean reward so far 27.45\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 44\n",
      "Episode ended after 11 steps\n",
      "Accumulated reward in this episode 11.0\n",
      "Mean reward so far 27.09\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 45\n",
      "Episode ended after 46 steps\n",
      "Accumulated reward in this episode 46.0\n",
      "Mean reward so far 27.50\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 46\n",
      "Episode ended after 10 steps\n",
      "Accumulated reward in this episode 10.0\n",
      "Mean reward so far 27.13\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 47\n",
      "Episode ended after 42 steps\n",
      "Accumulated reward in this episode 42.0\n",
      "Mean reward so far 27.44\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 48\n",
      "Episode ended after 40 steps\n",
      "Accumulated reward in this episode 40.0\n",
      "Mean reward so far 27.69\n",
      "Maximal reward so far 76.0\n",
      "-------------------------------------------------\n",
      "Episode 49\n",
      "Episode ended after 84 steps\n",
      "Accumulated reward in this episode 84.0\n",
      "Mean reward so far 28.82\n",
      "Maximal reward so far 84.0\n",
      "-------------------------------------------------\n",
      "Episode 50\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode 57.0\n",
      "Mean reward so far 29.37\n",
      "Maximal reward so far 84.0\n",
      "-------------------------------------------------\n",
      "Episode 51\n",
      "Episode ended after 13 steps\n",
      "Accumulated reward in this episode 13.0\n",
      "Mean reward so far 29.06\n",
      "Maximal reward so far 84.0\n",
      "-------------------------------------------------\n",
      "Episode 52\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 37.0\n",
      "Mean reward so far 29.21\n",
      "Maximal reward so far 84.0\n",
      "-------------------------------------------------\n",
      "Episode 53\n",
      "Episode ended after 32 steps\n",
      "Accumulated reward in this episode 32.0\n",
      "Mean reward so far 29.26\n",
      "Maximal reward so far 84.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 54\n",
      "Episode ended after 76 steps\n",
      "Accumulated reward in this episode 76.0\n",
      "Mean reward so far 30.11\n",
      "Maximal reward so far 84.0\n",
      "-------------------------------------------------\n",
      "Episode 55\n",
      "Episode ended after 52 steps\n",
      "Accumulated reward in this episode 52.0\n",
      "Mean reward so far 30.50\n",
      "Maximal reward so far 84.0\n",
      "-------------------------------------------------\n",
      "Episode 56\n",
      "Episode ended after 66 steps\n",
      "Accumulated reward in this episode 66.0\n",
      "Mean reward so far 31.12\n",
      "Maximal reward so far 84.0\n",
      "-------------------------------------------------\n",
      "Episode 57\n",
      "Episode ended after 50 steps\n",
      "Accumulated reward in this episode 50.0\n",
      "Mean reward so far 31.45\n",
      "Maximal reward so far 84.0\n",
      "-------------------------------------------------\n",
      "Episode 58\n",
      "Episode ended after 110 steps\n",
      "Accumulated reward in this episode 110.0\n",
      "Mean reward so far 32.78\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 59\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode 57.0\n",
      "Mean reward so far 33.18\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 60\n",
      "Episode ended after 52 steps\n",
      "Accumulated reward in this episode 52.0\n",
      "Mean reward so far 33.49\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 61\n",
      "Episode ended after 67 steps\n",
      "Accumulated reward in this episode 67.0\n",
      "Mean reward so far 34.03\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 62\n",
      "Episode ended after 36 steps\n",
      "Accumulated reward in this episode 36.0\n",
      "Mean reward so far 34.06\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 63\n",
      "Episode ended after 61 steps\n",
      "Accumulated reward in this episode 61.0\n",
      "Mean reward so far 34.48\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 64\n",
      "Episode ended after 58 steps\n",
      "Accumulated reward in this episode 58.0\n",
      "Mean reward so far 34.85\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 65\n",
      "Episode ended after 83 steps\n",
      "Accumulated reward in this episode 83.0\n",
      "Mean reward so far 35.58\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 66\n",
      "Episode ended after 19 steps\n",
      "Accumulated reward in this episode 19.0\n",
      "Mean reward so far 35.33\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 67\n",
      "Episode ended after 47 steps\n",
      "Accumulated reward in this episode 47.0\n",
      "Mean reward so far 35.50\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 68\n",
      "Episode ended after 27 steps\n",
      "Accumulated reward in this episode 27.0\n",
      "Mean reward so far 35.38\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 69\n",
      "Episode ended after 48 steps\n",
      "Accumulated reward in this episode 48.0\n",
      "Mean reward so far 35.56\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 70\n",
      "Episode ended after 76 steps\n",
      "Accumulated reward in this episode 76.0\n",
      "Mean reward so far 36.13\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 71\n",
      "Episode ended after 75 steps\n",
      "Accumulated reward in this episode 75.0\n",
      "Mean reward so far 36.67\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 72\n",
      "Episode ended after 37 steps\n",
      "Accumulated reward in this episode 37.0\n",
      "Mean reward so far 36.67\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 73\n",
      "Episode ended after 59 steps\n",
      "Accumulated reward in this episode 59.0\n",
      "Mean reward so far 36.97\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 74\n",
      "Episode ended after 52 steps\n",
      "Accumulated reward in this episode 52.0\n",
      "Mean reward so far 37.17\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 75\n",
      "Episode ended after 81 steps\n",
      "Accumulated reward in this episode 81.0\n",
      "Mean reward so far 37.75\n",
      "Maximal reward so far 110.0\n",
      "-------------------------------------------------\n",
      "Episode 76\n",
      "Episode ended after 125 steps\n",
      "Accumulated reward in this episode 125.0\n",
      "Mean reward so far 38.88\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 77\n",
      "Episode ended after 104 steps\n",
      "Accumulated reward in this episode 104.0\n",
      "Mean reward so far 39.72\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 78\n",
      "Episode ended after 100 steps\n",
      "Accumulated reward in this episode 100.0\n",
      "Mean reward so far 40.48\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 79\n",
      "Episode ended after 91 steps\n",
      "Accumulated reward in this episode 91.0\n",
      "Mean reward so far 41.11\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 80\n",
      "Episode ended after 36 steps\n",
      "Accumulated reward in this episode 36.0\n",
      "Mean reward so far 41.05\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 81\n",
      "Episode ended after 115 steps\n",
      "Accumulated reward in this episode 115.0\n",
      "Mean reward so far 41.95\n",
      "Maximal reward so far 125.0\n",
      "-------------------------------------------------\n",
      "Episode 82\n",
      "Episode ended after 147 steps\n",
      "Accumulated reward in this episode 147.0\n",
      "Mean reward so far 43.22\n",
      "Maximal reward so far 147.0\n",
      "-------------------------------------------------\n",
      "Episode 83\n",
      "Episode ended after 141 steps\n",
      "Accumulated reward in this episode 141.0\n",
      "Mean reward so far 44.38\n",
      "Maximal reward so far 147.0\n",
      "-------------------------------------------------\n",
      "Episode 84\n",
      "Episode ended after 245 steps\n",
      "Accumulated reward in this episode 245.0\n",
      "Mean reward so far 46.74\n",
      "Maximal reward so far 245.0\n",
      "-------------------------------------------------\n",
      "Episode 85\n",
      "Episode ended after 145 steps\n",
      "Accumulated reward in this episode 145.0\n",
      "Mean reward so far 47.88\n",
      "Maximal reward so far 245.0\n",
      "-------------------------------------------------\n",
      "Episode 86\n",
      "Episode ended after 238 steps\n",
      "Accumulated reward in this episode 238.0\n",
      "Mean reward so far 50.07\n",
      "Maximal reward so far 245.0\n",
      "-------------------------------------------------\n",
      "Episode 87\n",
      "Episode ended after 254 steps\n",
      "Accumulated reward in this episode 254.0\n",
      "Mean reward so far 52.39\n",
      "Maximal reward so far 254.0\n",
      "-------------------------------------------------\n",
      "Episode 88\n",
      "Episode ended after 326 steps\n",
      "Accumulated reward in this episode 326.0\n",
      "Mean reward so far 55.46\n",
      "Maximal reward so far 326.0\n",
      "-------------------------------------------------\n",
      "Episode 89\n",
      "Episode ended after 314 steps\n",
      "Accumulated reward in this episode 314.0\n",
      "Mean reward so far 58.33\n",
      "Maximal reward so far 326.0\n",
      "-------------------------------------------------\n",
      "Episode 90\n",
      "Episode ended after 482 steps\n",
      "Accumulated reward in this episode 482.0\n",
      "Mean reward so far 62.99\n",
      "Maximal reward so far 482.0\n",
      "-------------------------------------------------\n",
      "Episode 91\n",
      "Episode ended after 1719 steps\n",
      "Accumulated reward in this episode 1719.0\n",
      "Mean reward so far 80.99\n",
      "Maximal reward so far 1719.0\n",
      "-------------------------------------------------\n",
      "Episode 92\n",
      "Episode ended after 5662 steps\n",
      "Accumulated reward in this episode 5662.0\n",
      "Mean reward so far 141.00\n",
      "Maximal reward so far 5662.0\n",
      "-------------------------------------------------\n",
      "Episode 93\n",
      "Episode ended after 453 steps\n",
      "Accumulated reward in this episode 453.0\n",
      "Mean reward so far 144.32\n",
      "Maximal reward so far 5662.0\n",
      "-------------------------------------------------\n",
      "Episode 94\n",
      "Episode ended after 401 steps\n",
      "Accumulated reward in this episode 401.0\n",
      "Mean reward so far 147.02\n",
      "Maximal reward so far 5662.0\n",
      "-------------------------------------------------\n",
      "Episode 95\n",
      "Episode ended after 3643 steps\n",
      "Accumulated reward in this episode 3643.0\n",
      "Mean reward so far 183.44\n",
      "Maximal reward so far 5662.0\n",
      "-------------------------------------------------\n",
      "Episode 96\n",
      "Episode ended after 663 steps\n",
      "Accumulated reward in this episode 663.0\n",
      "Mean reward so far 188.38\n",
      "Maximal reward so far 5662.0\n",
      "-------------------------------------------------\n",
      "Episode 97\n",
      "Episode ended after 1806 steps\n",
      "Accumulated reward in this episode 1806.0\n",
      "Mean reward so far 204.89\n",
      "Maximal reward so far 5662.0\n",
      "-------------------------------------------------\n",
      "Episode 98\n",
      "Episode ended after 3646 steps\n",
      "Accumulated reward in this episode 3646.0\n",
      "Mean reward so far 239.65\n",
      "Maximal reward so far 5662.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 99\n",
      "Episode ended after 9976 steps\n",
      "Accumulated reward in this episode 9976.0\n",
      "Mean reward so far 337.01\n",
      "Maximal reward so far 9976.0\n"
     ]
    }
   ],
   "source": [
    "ep_action, ep_obs, ep_reward = [], [], []  # Allocate space for episode actions, observations and rewards\n",
    "tot_ep_reward = [] # Total episode reward\n",
    "mean_reward = []\n",
    "\n",
    "''' Run TF session '''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ''' Run episodes '''\n",
    "    for ep in range(num_episodes): \n",
    "        obs = env.reset()  # Reset and save first observation\n",
    "        ep_obs.append(obs) # append observation\n",
    "\n",
    "        ''' Run steps '''\n",
    "        while True:\n",
    "            # Propagate forward to compute action probability distribution\n",
    "            apd = np.squeeze(sess.run(action_prob_dist, feed_dict = {input_ : obs.reshape((1,4))}))\n",
    "            action = np.random.choice(np.arange(num_actions), p = apd)   # Sample an action based on the pdf\n",
    "            obs, reward, done, info = env.step(action)  # Take action and save observation, reward and done boolean\n",
    "            \n",
    "            # Convert action to one hot\n",
    "            action_oh = np.zeros((1,num_actions))\n",
    "            action_oh[0,action] = 1\n",
    "            \n",
    "            ep_action.append(action_oh)  # append action\n",
    "            ep_obs.append(obs)           # append observation\n",
    "            ep_reward.append(reward)     # append reward\n",
    "\n",
    "            if done: \n",
    "                # Stack vertically episode parameters to one np.array\n",
    "                ep_action = np.vstack(ep_action)\n",
    "                ep_obs = np.vstack(ep_obs)\n",
    "                ep_reward = np.hstack(ep_reward)\n",
    "\n",
    "                # Discount rewards\n",
    "                dis_rewards_arr = discount_rewards(ep_reward)\n",
    "                # Compute loss and optimize\n",
    "                sess.run([loss, training_opt],\n",
    "                         feed_dict = {input_ : ep_obs[:-1], actions : ep_action, dis_rewards : dis_rewards_arr})\n",
    "                \n",
    "                tot_ep_reward.append(np.sum(ep_reward))  # Compute total reward for episode\n",
    "                mean_reward.append(np.mean(tot_ep_reward))\n",
    "                \n",
    "                 # print info\n",
    "                print(\"-------------------------------------------------\")\n",
    "                print(\"Episode {}\".format(ep))\n",
    "                print(\"Episode ended after {} steps\".format(ep_action.shape[0]))\n",
    "                print(\"Accumulated reward in this episode {}\".format(tot_ep_reward[ep]))\n",
    "                print(\"Mean reward so far {:0.2f}\".format(np.mean(tot_ep_reward)))\n",
    "                print(\"Maximal reward so far {}\".format(np.max(tot_ep_reward)))\n",
    "                \n",
    "                ep_action, ep_obs, ep_reward = [], [], []  # Clear episode values for next episode\n",
    "                      \n",
    "                break\n",
    "\n",
    "    saver.save(sess, \"models/model.ckpt\") # save model for later\n",
    "#     writer.add_graph(sess.graph) # Save graph for displaying with TensorBoard\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of how good our model and to see if our agent improves in the training process, I like to plot the mean rewards gained in one episode over the episode number. This value would grow as our agent becomes more skilled and gains more rewards in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mean_reward)\n",
    "plt.xlabel(\"episode\", fontsize=14)\n",
    "plt.ylabel(\"mean reward\", fontsize = 14)\n",
    "plt.title(\"Mean episode rewards over episode number\", fontsize = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the agent play an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model.ckpt\n",
      "Game ended after 8900 steps\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/model.ckpt\") # load model\n",
    "    obs = env.reset() # Reset env and save observation\n",
    "    t = 0\n",
    "    while True:\n",
    "        env.render() # Render game\n",
    "        # Use our model to create a probability distribution of actions based on observation\n",
    "        apd = np.squeeze(sess.run(action_prob_dist, feed_dict={input_ : obs.reshape((1,4))}))\n",
    "        # Choose an action out of the PDF and take action\n",
    "        action = np.random.choice(np.arange(num_actions), p = apd)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        t = t+1\n",
    "        if done:\n",
    "            print(\"Game ended after {} steps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
