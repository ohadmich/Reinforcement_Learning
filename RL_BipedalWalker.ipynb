{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Bipedal Walker game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define all the hyperparameters for the model, this will allow as to easily iterate and find the values that give best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100      # Number of episodes for training\n",
    "learning_rate = 0.0002\n",
    "gamma = 0.99           # Discount factor for reward\n",
    "num_Hidden = 200       # number of nodes in the hidden layer\n",
    "\n",
    "env = gym.make('BipedalWalker-v2')    # Choose a game and create an environment\n",
    "env._max_episode_steps = 5000\n",
    "# env = env.unwrapped              # The wrapper limits the number of steps in an episode, let's get rid of it\n",
    "obs_dim = env.reset().shape      # obervation dimension\n",
    "num_actions = len(env.action_space.high) # number of actions (this works only for continuous action space, which is the case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discount reward function**: We we'll train our agent based on the reward gained for his actions. For each action we'll define the episode reward as the total reward gained in all the next steps of the current episode. Since a reward gained further away in the future has less correlation to the present action, we will give it less weight by discounting future rewards.\n",
    "\n",
    "The formula for the discounted rewards is given by:\n",
    "\n",
    "$$ R_t = \\sum_k \\gamma^k r_{t+k} $$\n",
    "\n",
    "Where $r_t$ is the reward gained in the step $t$ and $\\gamma \\in [0,1]$ is a hyperparameter called the discount factor.\n",
    "Here we define a function that takes a vector of rewards in consequent steps and returns the discounted reward vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    '''Takes a 1D rewards (one episode) and discounts it and also standardize\n",
    "    the rewards to be unit normal (helps control the gradient estimator variance)'''\n",
    "    \n",
    "    # Discounting\n",
    "    dis_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_sum = gamma*running_sum + r[t]\n",
    "        dis_r[t] = running_sum\n",
    "    \n",
    "    # Normailzing\n",
    "    dis_r = (dis_r - np.mean(dis_r))/np.std(dis_r)\n",
    "        \n",
    "    return dis_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This game takes a continuous range of values for each action. In the Gym library language, the action space is a Box instead of Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "[1. 1. 1. 1.]\n",
      "[-1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.high)\n",
    "print(env.action_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, each action is a vector of length 4 where every element value is in the [-1,1] domain. We will use Gaussian policy to train this model. Our NN would output the 4 mean values and 4 std values for each element of the action vector, and then use them to generate actions in each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mean values must be contained in the region [-1,1], we would like to use some activation function that reflects that. We'll use a custom activation which is basically a sigmoid scaled to our desired domain: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m1t1_sigmoid(x):\n",
    "    return (tf.nn.sigmoid(x) - 0.5)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep order in our model, we use name scopes which basically group the layers of our model in a simple to follow way. Eventually, when using [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) for visualizing the model, the graph is more readable and makes it easy to understand the model and find errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create placeholders for inputs'''\n",
    "# A place holder for input observations\n",
    "input_ = tf.placeholder(tf.float32, shape = (None, obs_dim[0]), name = \"input\")\n",
    "# A place holder for actions in a full episode\n",
    "actions = tf.placeholder(tf.float32, shape = (None, num_actions), name = \"actions\")\n",
    "# A place holder for discounted rewards in a full episode\n",
    "dis_rewards = tf.placeholder(tf.float32, shape = (None, ), name = \"dis_rewards\")\n",
    "\n",
    "'''Fully connected layers'''\n",
    "# A network channel for predicting the mean values of the action PDF\n",
    "with tf.name_scope(\"Mean_channel\"):\n",
    "    Mfc1 = tf.layers.dense(inputs = input_, units = num_Hidden ,activation = tf.nn.relu, name = \"Mfc1\" )\n",
    "    Mfc2 = tf.layers.dense(inputs = Mfc1, units = num_actions ,activation = m1t1_sigmoid, name = \"Mfc2\")\n",
    "\n",
    "    # A network channel for predicting the std values of the action PDF\n",
    "with tf.name_scope(\"Std_channel\"):\n",
    "    Sfc1 = tf.layers.dense(inputs = input_, units = num_Hidden ,activation = tf.nn.relu, name = \"Sfc1\" )\n",
    "    # We use a softplus activation for the last layer to ensure positive std value\n",
    "    Sfc2 = tf.layers.dense(inputs = input_, units = num_actions ,activation = tf.nn.sigmoid, \n",
    "                           name = \"Sfc2\" )\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"Action_PDF\"):\n",
    "    # Use the predicted mean and std for creating a probabilty distribution for actions\n",
    "    action_prob_dist = tf.contrib.distributions.Normal(loc = Mfc2, scale = Sfc2 + 1e-5, \n",
    "                                                       name = \"Normal_PDF\")\n",
    "    sample_action = action_prob_dist.sample([1], name = \"sample_action\")\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    '''Define loss'''\n",
    "    # compute negative log probability using our PDF and sum over the actions in every time step\n",
    "    negative_logprob = - tf.reduce_sum(action_prob_dist.log_prob(actions), axis = 1)\n",
    "    # Modulate the loss based on our discounted reward - this is where reinforcment learning happens,\n",
    "    # we favor actions that produced high reward\n",
    "    loss = tf.reduce_mean(negative_logprob * dis_rewards, name = \"loss\")\n",
    "\n",
    "with tf.name_scope(\"Training\"):\n",
    "    '''Define optimizer'''\n",
    "    training_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "'''Define saver for saving and restoring model'''\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a writer for saving summaries to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = tf.summary.FileWriter(\"./tensorboard/1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model ready, we can start training it.\n",
    "\n",
    "Our goal is to achieve a model that uses the current observation to create the best probability distribution function (PDF) for the next action to be taken. This is **not a deterministic model** - the agent has a fine probability to take any action after receiving an observation, with larger probability to take favorable actions.\n",
    "\n",
    "To achieve that, in each step, we use our model to generate a PDF of actions, draw an action out of it and take the next step.\n",
    "When we reach the end of the episode, we compute the episode loss by feeding the observation, action and discounted rewards vectors (which we kept track of). Our optimizer minimizes the loss which makes favorable (unfavorable) actions more probable (less probable) due to the multiplicative factor of the discounted reward. By repeating these steps to each episode, our agent gradually improves its PDF and becomes a better player!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 0\n",
      "Episode ended after 117 steps\n",
      "Accumulated reward in this episode -117.69759306088028\n",
      "Mean reward so far -117.70\n",
      "Maximal reward so far -117.69759306088028\n",
      "-------------------------------------------------\n",
      "Episode 1\n",
      "Episode ended after 72 steps\n",
      "Accumulated reward in this episode -109.65105192225239\n",
      "Mean reward so far -113.67\n",
      "Maximal reward so far -109.65105192225239\n",
      "-------------------------------------------------\n",
      "Episode 2\n",
      "Episode ended after 76 steps\n",
      "Accumulated reward in this episode -100.87074878252484\n",
      "Mean reward so far -109.41\n",
      "Maximal reward so far -100.87074878252484\n",
      "-------------------------------------------------\n",
      "Episode 3\n",
      "Episode ended after 70 steps\n",
      "Accumulated reward in this episode -98.94868520465121\n",
      "Mean reward so far -106.79\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 4\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -207.46723295550947\n",
      "Mean reward so far -126.93\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 5\n",
      "Episode ended after 71 steps\n",
      "Accumulated reward in this episode -110.88026993810013\n",
      "Mean reward so far -124.25\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 6\n",
      "Episode ended after 65 steps\n",
      "Accumulated reward in this episode -113.26141808558131\n",
      "Mean reward so far -122.68\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 7\n",
      "Episode ended after 77 steps\n",
      "Accumulated reward in this episode -105.01141350417895\n",
      "Mean reward so far -120.47\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 8\n",
      "Episode ended after 57 steps\n",
      "Accumulated reward in this episode -110.3357884781845\n",
      "Mean reward so far -119.35\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 9\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -192.35245075416753\n",
      "Mean reward so far -126.65\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 10\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -211.4895847900342\n",
      "Mean reward so far -134.36\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 11\n",
      "Episode ended after 42 steps\n",
      "Accumulated reward in this episode -108.84471744254542\n",
      "Mean reward so far -132.23\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 12\n",
      "Episode ended after 62 steps\n",
      "Accumulated reward in this episode -103.30247780413491\n",
      "Mean reward so far -130.01\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 13\n",
      "Episode ended after 79 steps\n",
      "Accumulated reward in this episode -112.31270621377851\n",
      "Mean reward so far -128.74\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 14\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -206.11582466615863\n",
      "Mean reward so far -133.90\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 15\n",
      "Episode ended after 63 steps\n",
      "Accumulated reward in this episode -109.60073567543986\n",
      "Mean reward so far -132.38\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 16\n",
      "Episode ended after 70 steps\n",
      "Accumulated reward in this episode -114.51063709684027\n",
      "Mean reward so far -131.33\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 17\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -206.59537637631132\n",
      "Mean reward so far -135.51\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 18\n",
      "Episode ended after 65 steps\n",
      "Accumulated reward in this episode -111.87855992914798\n",
      "Mean reward so far -134.27\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 19\n",
      "Episode ended after 87 steps\n",
      "Accumulated reward in this episode -112.1218098124247\n",
      "Mean reward so far -133.16\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 20\n",
      "Episode ended after 173 steps\n",
      "Accumulated reward in this episode -113.60126340503618\n",
      "Mean reward so far -132.23\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 21\n",
      "Episode ended after 99 steps\n",
      "Accumulated reward in this episode -118.5146149002829\n",
      "Mean reward so far -131.61\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 22\n",
      "Episode ended after 54 steps\n",
      "Accumulated reward in this episode -108.05025793264186\n",
      "Mean reward so far -130.58\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 23\n",
      "Episode ended after 68 steps\n",
      "Accumulated reward in this episode -112.59473932001119\n",
      "Mean reward so far -129.83\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 24\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -199.71553712733672\n",
      "Mean reward so far -132.63\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 25\n",
      "Episode ended after 62 steps\n",
      "Accumulated reward in this episode -111.2491768856862\n",
      "Mean reward so far -131.81\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 26\n",
      "Episode ended after 52 steps\n",
      "Accumulated reward in this episode -108.79392458972335\n",
      "Mean reward so far -130.95\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 27\n",
      "Episode ended after 75 steps\n",
      "Accumulated reward in this episode -103.644774279433\n",
      "Mean reward so far -129.98\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 28\n",
      "Episode ended after 69 steps\n",
      "Accumulated reward in this episode -112.98909128114396\n",
      "Mean reward so far -129.39\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 29\n",
      "Episode ended after 143 steps\n",
      "Accumulated reward in this episode -124.57350815046578\n",
      "Mean reward so far -129.23\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 30\n",
      "Episode ended after 78 steps\n",
      "Accumulated reward in this episode -99.30378427196635\n",
      "Mean reward so far -128.27\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 31\n",
      "Episode ended after 79 steps\n",
      "Accumulated reward in this episode -118.28507569567424\n",
      "Mean reward so far -127.96\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 32\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -209.2683083611115\n",
      "Mean reward so far -130.42\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 33\n",
      "Episode ended after 92 steps\n",
      "Accumulated reward in this episode -119.16238156387085\n",
      "Mean reward so far -130.09\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 34\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -209.67960915425513\n",
      "Mean reward so far -132.36\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 35\n",
      "Episode ended after 99 steps\n",
      "Accumulated reward in this episode -107.50410315982438\n",
      "Mean reward so far -131.67\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 36\n",
      "Episode ended after 68 steps\n",
      "Accumulated reward in this episode -104.70340034566944\n",
      "Mean reward so far -130.94\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 37\n",
      "Episode ended after 92 steps\n",
      "Accumulated reward in this episode -118.62473061643416\n",
      "Mean reward so far -130.62\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 38\n",
      "Episode ended after 93 steps\n",
      "Accumulated reward in this episode -109.67069969848761\n",
      "Mean reward so far -130.08\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 39\n",
      "Episode ended after 68 steps\n",
      "Accumulated reward in this episode -103.94335042369241\n",
      "Mean reward so far -129.43\n",
      "Maximal reward so far -98.94868520465121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 40\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -207.21262056730657\n",
      "Mean reward so far -131.33\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 41\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -207.49045063667728\n",
      "Mean reward so far -133.14\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 42\n",
      "Episode ended after 77 steps\n",
      "Accumulated reward in this episode -117.81085921240971\n",
      "Mean reward so far -132.78\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 43\n",
      "Episode ended after 73 steps\n",
      "Accumulated reward in this episode -112.50275549034029\n",
      "Mean reward so far -132.32\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 44\n",
      "Episode ended after 95 steps\n",
      "Accumulated reward in this episode -107.79477788919522\n",
      "Mean reward so far -131.78\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 45\n",
      "Episode ended after 73 steps\n",
      "Accumulated reward in this episode -103.30777803982856\n",
      "Mean reward so far -131.16\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 46\n",
      "Episode ended after 108 steps\n",
      "Accumulated reward in this episode -117.8651248346412\n",
      "Mean reward so far -130.87\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 47\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -210.82841993236417\n",
      "Mean reward so far -132.54\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 48\n",
      "Episode ended after 184 steps\n",
      "Accumulated reward in this episode -109.15492192290723\n",
      "Mean reward so far -132.06\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 49\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -192.53496145676823\n",
      "Mean reward so far -133.27\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 50\n",
      "Episode ended after 87 steps\n",
      "Accumulated reward in this episode -104.32343991678829\n",
      "Mean reward so far -132.70\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 51\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -210.75560835171387\n",
      "Mean reward so far -134.21\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 52\n",
      "Episode ended after 53 steps\n",
      "Accumulated reward in this episode -105.23097095023716\n",
      "Mean reward so far -133.66\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 53\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -191.7182342278349\n",
      "Mean reward so far -134.73\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 54\n",
      "Episode ended after 85 steps\n",
      "Accumulated reward in this episode -114.83251453970745\n",
      "Mean reward so far -134.37\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 55\n",
      "Episode ended after 59 steps\n",
      "Accumulated reward in this episode -113.03337455945524\n",
      "Mean reward so far -133.99\n",
      "Maximal reward so far -98.94868520465121\n",
      "-------------------------------------------------\n",
      "Episode 56\n",
      "Episode ended after 107 steps\n",
      "Accumulated reward in this episode -95.2897280581786\n",
      "Mean reward so far -133.31\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 57\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -196.34137881834357\n",
      "Mean reward so far -134.40\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 58\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -199.49330516701502\n",
      "Mean reward so far -135.50\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 59\n",
      "Episode ended after 101 steps\n",
      "Accumulated reward in this episode -114.37161230860961\n",
      "Mean reward so far -135.15\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 60\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -197.58181439163968\n",
      "Mean reward so far -136.17\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 61\n",
      "Episode ended after 68 steps\n",
      "Accumulated reward in this episode -115.34299017094075\n",
      "Mean reward so far -135.84\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 62\n",
      "Episode ended after 65 steps\n",
      "Accumulated reward in this episode -115.3093268509023\n",
      "Mean reward so far -135.51\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 63\n",
      "Episode ended after 194 steps\n",
      "Accumulated reward in this episode -136.85529580874802\n",
      "Mean reward so far -135.53\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 64\n",
      "Episode ended after 193 steps\n",
      "Accumulated reward in this episode -127.81870399826673\n",
      "Mean reward so far -135.41\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 65\n",
      "Episode ended after 76 steps\n",
      "Accumulated reward in this episode -111.86609784749027\n",
      "Mean reward so far -135.06\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 66\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -200.72531565390588\n",
      "Mean reward so far -136.04\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 67\n",
      "Episode ended after 171 steps\n",
      "Accumulated reward in this episode -128.10149516762297\n",
      "Mean reward so far -135.92\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 68\n",
      "Episode ended after 67 steps\n",
      "Accumulated reward in this episode -112.38024453415349\n",
      "Mean reward so far -135.58\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 69\n",
      "Episode ended after 60 steps\n",
      "Accumulated reward in this episode -114.26422498665812\n",
      "Mean reward so far -135.28\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 70\n",
      "Episode ended after 74 steps\n",
      "Accumulated reward in this episode -116.17291005586026\n",
      "Mean reward so far -135.01\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 71\n",
      "Episode ended after 96 steps\n",
      "Accumulated reward in this episode -119.94566330330943\n",
      "Mean reward so far -134.80\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 72\n",
      "Episode ended after 71 steps\n",
      "Accumulated reward in this episode -112.5573113737305\n",
      "Mean reward so far -134.49\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 73\n",
      "Episode ended after 84 steps\n",
      "Accumulated reward in this episode -116.72873608661878\n",
      "Mean reward so far -134.25\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 74\n",
      "Episode ended after 84 steps\n",
      "Accumulated reward in this episode -114.14747974586052\n",
      "Mean reward so far -133.98\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 75\n",
      "Episode ended after 92 steps\n",
      "Accumulated reward in this episode -105.1253428252209\n",
      "Mean reward so far -133.60\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 76\n",
      "Episode ended after 55 steps\n",
      "Accumulated reward in this episode -107.32157156013884\n",
      "Mean reward so far -133.26\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 77\n",
      "Episode ended after 74 steps\n",
      "Accumulated reward in this episode -117.721903787413\n",
      "Mean reward so far -133.06\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 78\n",
      "Episode ended after 131 steps\n",
      "Accumulated reward in this episode -115.7314563846439\n",
      "Mean reward so far -132.84\n",
      "Maximal reward so far -95.2897280581786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Episode 79\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -196.83717242200845\n",
      "Mean reward so far -133.64\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 80\n",
      "Episode ended after 82 steps\n",
      "Accumulated reward in this episode -96.66506358878314\n",
      "Mean reward so far -133.19\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 81\n",
      "Episode ended after 49 steps\n",
      "Accumulated reward in this episode -111.17182144195773\n",
      "Mean reward so far -132.92\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 82\n",
      "Episode ended after 63 steps\n",
      "Accumulated reward in this episode -112.36232141517414\n",
      "Mean reward so far -132.67\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 83\n",
      "Episode ended after 135 steps\n",
      "Accumulated reward in this episode -122.18590368027178\n",
      "Mean reward so far -132.55\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 84\n",
      "Episode ended after 81 steps\n",
      "Accumulated reward in this episode -111.39111913416907\n",
      "Mean reward so far -132.30\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 85\n",
      "Episode ended after 60 steps\n",
      "Accumulated reward in this episode -110.46831970295744\n",
      "Mean reward so far -132.04\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 86\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -195.20144047668578\n",
      "Mean reward so far -132.77\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 87\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -200.4205834378181\n",
      "Mean reward so far -133.54\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 88\n",
      "Episode ended after 129 steps\n",
      "Accumulated reward in this episode -108.5873140000018\n",
      "Mean reward so far -133.26\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 89\n",
      "Episode ended after 98 steps\n",
      "Accumulated reward in this episode -104.52938850356297\n",
      "Mean reward so far -132.94\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 90\n",
      "Episode ended after 102 steps\n",
      "Accumulated reward in this episode -121.98534433738763\n",
      "Mean reward so far -132.82\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 91\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -193.1447214551697\n",
      "Mean reward so far -133.47\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 92\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -196.41502495036585\n",
      "Mean reward so far -134.15\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 93\n",
      "Episode ended after 63 steps\n",
      "Accumulated reward in this episode -99.05480793818955\n",
      "Mean reward so far -133.78\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 94\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -199.0960962623774\n",
      "Mean reward so far -134.47\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 95\n",
      "Episode ended after 69 steps\n",
      "Accumulated reward in this episode -115.0659951106608\n",
      "Mean reward so far -134.26\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 96\n",
      "Episode ended after 5000 steps\n",
      "Accumulated reward in this episode -198.86510728428885\n",
      "Mean reward so far -134.93\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 97\n",
      "Episode ended after 173 steps\n",
      "Accumulated reward in this episode -125.3889308253533\n",
      "Mean reward so far -134.83\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 98\n",
      "Episode ended after 106 steps\n",
      "Accumulated reward in this episode -120.0809587259659\n",
      "Mean reward so far -134.68\n",
      "Maximal reward so far -95.2897280581786\n",
      "-------------------------------------------------\n",
      "Episode 99\n",
      "Episode ended after 87 steps\n",
      "Accumulated reward in this episode -113.6036696066993\n",
      "Mean reward so far -134.47\n",
      "Maximal reward so far -95.2897280581786\n"
     ]
    }
   ],
   "source": [
    "ep_action, ep_obs, ep_reward = [], [], []  # Allocate space for episode actions, observations and rewards\n",
    "tot_ep_reward = [] # Total episode reward\n",
    "mean_reward = []\n",
    "\n",
    "''' Run TF session '''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ''' Run episodes '''\n",
    "    for ep in range(num_episodes): \n",
    "        obs = env.reset()  # Reset and save first observation\n",
    "        ep_obs.append(obs) # append observation\n",
    "\n",
    "        ''' Run steps '''\n",
    "        while True:\n",
    "            # Propagate forward to compute action probability distribution and sample an action out of it\n",
    "            action = np.squeeze(sess.run(sample_action, \n",
    "                                         feed_dict = {input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "            m = sess.run(Mfc2, feed_dict = {input_ : obs.reshape((1,obs_dim[0]))})\n",
    "            # clip action values to fit into the allowed box\n",
    "            action = np.clip(action, env.action_space.low[0], env.action_space.high[0])\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)  # Take action and save observation, reward and done boolean\n",
    "            \n",
    "            ep_action.append(action)     # append action\n",
    "            ep_obs.append(obs)           # append observation\n",
    "            ep_reward.append(reward)     # append reward\n",
    "\n",
    "            if done: \n",
    "                # Stack vertically episode parameters to one np.array\n",
    "                ep_action = np.vstack(ep_action)\n",
    "                ep_obs = np.vstack(ep_obs)\n",
    "                ep_reward = np.hstack(ep_reward)\n",
    "\n",
    "                # Discount rewards\n",
    "                dis_rewards_arr = discount_rewards(ep_reward)\n",
    "                # Compute loss and optimize\n",
    "                sess.run([loss, training_opt],\n",
    "                         feed_dict = {input_ : ep_obs[:-1], actions : ep_action,\n",
    "                                      dis_rewards : dis_rewards_arr})\n",
    "                \n",
    "                tot_ep_reward.append(np.sum(ep_reward))  # Compute total reward for episode\n",
    "                mean_reward.append(np.mean(tot_ep_reward))\n",
    "                \n",
    "                 # print info\n",
    "                print(\"-------------------------------------------------\")\n",
    "                print(\"Episode {}\".format(ep))\n",
    "                print(\"Episode ended after {} steps\".format(ep_action.shape[0]))\n",
    "                print(\"Accumulated reward in this episode {}\".format(tot_ep_reward[ep]))\n",
    "                print(\"Mean reward so far {:0.2f}\".format(np.mean(tot_ep_reward)))\n",
    "                print(\"Maximal reward so far {}\".format(np.max(tot_ep_reward)))\n",
    "                \n",
    "                ep_action, ep_obs, ep_reward = [], [], []  # Clear episode values for next episode\n",
    "                      \n",
    "                break\n",
    "                \n",
    "    saver.save(sess, \"models/BipedalWalker/model.ckpt\") # save model for later\n",
    "#     writer.add_graph(sess.graph) # Save graph for displaying with TensorBoard\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a sense of how good our model is, and to see if our agent improves in the training process, we can plot the mean rewards gained in one episode over the episode number. This value would grow as our agent becomes more skilled and gains more rewards in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mean_reward)\n",
    "plt.xlabel(\"episode\", fontsize=14)\n",
    "plt.ylabel(\"mean reward\", fontsize = 14)\n",
    "plt.title(\"Mean episode rewards over episode number\", fontsize = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch the agent play an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fun part!\n",
    "Now we get to see how good our agent really is by watching it play an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from models/BipedalWalker/model.ckpt\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -59.26\n"
     ]
    }
   ],
   "source": [
    "ep_reward = []\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "# env._max_episode_steps = 1000\n",
    "# env = gym.wrappers.Monitor(env, \"recording/LunarLander\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/BipedalWalker/model.ckpt\") # load model\n",
    "    obs = env.reset() # Reset env and save observation\n",
    "    t = 0\n",
    "    while True:\n",
    "        env.render() # Render game\n",
    "        # Use our model to create a probability distribution of actions based on observation\n",
    "        action = np.squeeze(sess.run(sample_action, feed_dict = {input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "        # clip action values to fit into the allowed box\n",
    "        action = np.clip(action, env.action_space.low[0], env.action_space.high[0])\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        ep_reward.append(reward)\n",
    "        t = t+1\n",
    "        if done:\n",
    "            print(\"Game ended after {} steps\".format(t+1))\n",
    "            print(\"Total reward earned in this episode: {:0.2f}\".format(sum(ep_reward)))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from models/BipedalWalker/model.ckpt\n",
      "-------------------------------------------\n",
      "Game ended after 71 steps\n",
      "Total reward earned in this episode: -105.04\n",
      "-------------------------------------------\n",
      "Game ended after 72 steps\n",
      "Total reward earned in this episode: -111.85\n",
      "-------------------------------------------\n",
      "Game ended after 65 steps\n",
      "Total reward earned in this episode: -112.45\n",
      "-------------------------------------------\n",
      "Game ended after 161 steps\n",
      "Total reward earned in this episode: -125.43\n",
      "-------------------------------------------\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -60.65\n",
      "-------------------------------------------\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -58.01\n",
      "-------------------------------------------\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -60.46\n",
      "-------------------------------------------\n",
      "Game ended after 74 steps\n",
      "Total reward earned in this episode: -120.93\n",
      "-------------------------------------------\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -57.50\n",
      "-------------------------------------------\n",
      "Game ended after 61 steps\n",
      "Total reward earned in this episode: -110.86\n",
      "-------------------------------------------\n",
      "Game ended after 158 steps\n",
      "Total reward earned in this episode: -118.14\n",
      "-------------------------------------------\n",
      "Game ended after 48 steps\n",
      "Total reward earned in this episode: -112.52\n",
      "-------------------------------------------\n",
      "Game ended after 71 steps\n",
      "Total reward earned in this episode: -113.32\n",
      "-------------------------------------------\n",
      "Game ended after 81 steps\n",
      "Total reward earned in this episode: -105.79\n",
      "-------------------------------------------\n",
      "Game ended after 175 steps\n",
      "Total reward earned in this episode: -108.67\n",
      "-------------------------------------------\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -58.58\n",
      "-------------------------------------------\n",
      "Game ended after 56 steps\n",
      "Total reward earned in this episode: -110.83\n",
      "-------------------------------------------\n",
      "Game ended after 47 steps\n",
      "Total reward earned in this episode: -110.28\n",
      "-------------------------------------------\n",
      "Game ended after 53 steps\n",
      "Total reward earned in this episode: -112.68\n",
      "-------------------------------------------\n",
      "Game ended after 78 steps\n",
      "Total reward earned in this episode: -101.07\n",
      "-------------------------------------------\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -59.07\n",
      "-------------------------------------------\n",
      "Game ended after 73 steps\n",
      "Total reward earned in this episode: -113.36\n",
      "-------------------------------------------\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -64.19\n",
      "-------------------------------------------\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -54.34\n",
      "-------------------------------------------\n",
      "Game ended after 73 steps\n",
      "Total reward earned in this episode: -118.64\n",
      "-------------------------------------------\n",
      "Game ended after 62 steps\n",
      "Total reward earned in this episode: -117.24\n",
      "-------------------------------------------\n",
      "Game ended after 59 steps\n",
      "Total reward earned in this episode: -102.81\n",
      "-------------------------------------------\n",
      "Game ended after 85 steps\n",
      "Total reward earned in this episode: -114.30\n",
      "-------------------------------------------\n",
      "Game ended after 81 steps\n",
      "Total reward earned in this episode: -115.57\n",
      "-------------------------------------------\n",
      "Game ended after 77 steps\n",
      "Total reward earned in this episode: -119.66\n",
      "-------------------------------------------\n",
      "Game ended after 62 steps\n",
      "Total reward earned in this episode: -108.57\n",
      "-------------------------------------------\n",
      "Game ended after 86 steps\n",
      "Total reward earned in this episode: -103.45\n",
      "-------------------------------------------\n",
      "Game ended after 74 steps\n",
      "Total reward earned in this episode: -104.36\n",
      "-------------------------------------------\n",
      "Game ended after 90 steps\n",
      "Total reward earned in this episode: -105.32\n",
      "-------------------------------------------\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -58.98\n",
      "-------------------------------------------\n",
      "Game ended after 104 steps\n",
      "Total reward earned in this episode: -103.68\n",
      "-------------------------------------------\n",
      "Game ended after 88 steps\n",
      "Total reward earned in this episode: -111.14\n",
      "-------------------------------------------\n",
      "Game ended after 70 steps\n",
      "Total reward earned in this episode: -114.77\n",
      "-------------------------------------------\n",
      "Game ended after 84 steps\n",
      "Total reward earned in this episode: -101.73\n",
      "-------------------------------------------\n",
      "Game ended after 93 steps\n",
      "Total reward earned in this episode: -114.64\n",
      "-------------------------------------------\n",
      "Game ended after 59 steps\n",
      "Total reward earned in this episode: -113.54\n",
      "-------------------------------------------\n",
      "Game ended after 70 steps\n",
      "Total reward earned in this episode: -107.56\n",
      "-------------------------------------------\n",
      "Game ended after 69 steps\n",
      "Total reward earned in this episode: -107.77\n",
      "-------------------------------------------\n",
      "Game ended after 85 steps\n",
      "Total reward earned in this episode: -113.05\n",
      "-------------------------------------------\n",
      "Game ended after 72 steps\n",
      "Total reward earned in this episode: -115.78\n",
      "-------------------------------------------\n",
      "Game ended after 1601 steps\n",
      "Total reward earned in this episode: -59.05\n",
      "-------------------------------------------\n",
      "Game ended after 56 steps\n",
      "Total reward earned in this episode: -110.90\n",
      "-------------------------------------------\n",
      "Game ended after 74 steps\n",
      "Total reward earned in this episode: -114.92\n",
      "-------------------------------------------\n",
      "Game ended after 135 steps\n",
      "Total reward earned in this episode: -118.77\n"
     ]
    }
   ],
   "source": [
    "num_ep = 100\n",
    "ep_reward = []\n",
    "batch_reward = []\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "# env._max_episode_steps = 1000\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/BipedalWalker/model.ckpt\") # load model\n",
    "    for ep in range(num_ep):\n",
    "        obs = env.reset() # Reset env and save observation\n",
    "        t = 0\n",
    "        while True:\n",
    "            # Use our model to create a probability distribution of actions based on observation\n",
    "            action = np.squeeze(sess.run(sample_action, feed_dict = {input_ : obs.reshape((1,obs_dim[0]))}))\n",
    "            # clip action values to fit into the allowed box\n",
    "            action = np.clip(action, env.action_space.low[0], env.action_space.high[0])\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            ep_reward.append(reward)\n",
    "            t = t+1\n",
    "            if done:\n",
    "                print(\"-------------------------------------------\")\n",
    "                print(\"Game ended after {} steps\".format(t+1))\n",
    "                print(\"Total reward earned in this episode: {:0.2f}\".format(sum(ep_reward)))\n",
    "                batch_reward.append(sum(ep_reward))\n",
    "                ep_reward = []\n",
    "                break\n",
    "print(\"===================================================\")\n",
    "print(\"Average reward in batch is {:0.2f}\".format(np.mean(batch_reward)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
